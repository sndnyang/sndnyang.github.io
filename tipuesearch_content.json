{"pages":[{"url":"/classical-music-week1.html","text":"4. 音乐传播过程-声波和耳朵 长波低音高， 短波高音高 空气（外耳） - 内耳 - 电化学信号 - basilar membrane（耳蜗基底膜） - 纤毛感应特定声波 - 大脑 - primary auditory nerve - auditory cortex(大脑皮层) in temporal lobe 其他相关： 1. prefrontal cortex(lobe) -- where am i in this piece 2. motor cortex parietal lobe -- movement 3. hippocampus - memory 4. limbic system, amygdala -- emotion 5. ... 6. why we like what we like. 对音乐模板、模式(template)的预测方式 nurture -- 文化培养 nature 7. 西方音乐的语法syntax leading tone -- pull toward the home pitch or tonic large leap -- reversed by following pitch harmony -- must fit with the melody chord progression -- harmonies usually return home to tonic 8. 音乐的本质 nature overtone series 乐器震动出声时， 其实不只一个音， 只是听觉系统让我们只听到最低音， 因为它声音最大 这个最低音振幅最大， 定为 fundamental tone , 其他音 overtones 后面几个音和前面做和音","tags":"音乐","title":"古典音乐导论-第一周"},{"url":"/multi_layer_cd_suver.html","text":"摘要 Community Detection in Multi-Layer Graphs: A Survey 本文介绍多层网络下聚落检测问题， 并对相应算法做综述 导论 - 过 背景知识 群落/社区 community densely-connected components/subgraph relative/similar 比如： 同校、 同系、 同班是一个 community 同一个社团、 公司 同一领域的研究 多层网络模型 关系的不同方面就可以表达成多个独立图组成的多层图， 里面的每个、每层图就代表了一个方面 比如： 同学关系 微信好友 微博好友 单层图定义 a weighted graph (V,w) V is a set of vertices w is a set of edge weights: (V × V ) → [0,1]. 点映射(图层之间) node mapping is a function from a graph layer L1 = (V1 ,w1 ) to another graph layer L2 = (V2 ,w2 ) V1 × V2 → [0,1]. For each u ∈ V 1 , the set C(u) = {v ∈ V2 |f(u,v) > 0} is the set of V2 vertices corresponding to u. 对于一个facebook上的账号（个人）： twitter 上没有账号 twitter 上只有一个号--pillar(柱型) multi-layer graph twitter 上多个号 多层图定义 a tuple MLN = (L1 ,...,Ll ,IM) where Li = (Vi ,wi ),i ∈ 1,...,l are graph layers IM (Identity Mapping) is an l ×l matrix of node mappings, with $ IM_{i,j} : V_i ×V_j → [0,1] $ 例： 症状 疾病名 细菌、病毒或基因 信息网络定义： 是个有向图， 存在\"点\"到\"点类别\" 的函数映射 及 \"边\"到\"关系(边)类别\"的函数映射 异构信息网络 Heterogeneous Information Networks 点的类别 或 关系的类别个数大于1的信息网络。 异构信息网络与多层网络模型 等价 但强调不太相同 heterogeneous information networks emphasize heterogeneous types of entities connected by different relationships 主要方法 分类 聚类扩展 cluster expansion 矩阵分解 matrix factorization 统一距离？ unified distance 基于概率模型 model based 模式挖掘 pattern mining 图合并 graph merging 特点 多数只支持两层图 一层是图的原始拓扑结构信息 其他层一般是利用点的属性信息来计算相似度 聚类扩展 Cluster Expansion 论文： Scalable community discovery on textual data with relations 基于关系（文章引用）与文本属性 针对的问题 大型文档语料 -- large cocument corpus 没有同时考虑 textual attribute 和 relations(文献里的引用？) 大数据集的可扩展性scalability 多数算法基于一堆要（人工）设定的参数 思路 非监督方法 快速地找到初始的核， 作为群落的种子 核进行扩展（或合并merge)， 扩展成群落，（提高scalability) cores dictate the formation and topics of communities 核 用来表示 社区的构造和主题 步骤 4 steps: core probing core merging, 根据主题相似度进行合并 affiliation, 利用关系信息，将core扩展成初始社区 classification， 主题不相关的成员从社区中移除 第一步 Core Probing 基本思想 co-occurrence analysis: multiple objects are linked simultaneously by others, they are more likely to be able to define a coherent topic scope prob 步骤 生成每个点的outgoing relations 用关联规则来计算频繁项集(Apriori) 与 Apriori 的不同点 不使用固定的过滤阈值， 根据项集的长度决定阈值 项集存在包含关系，如果项集 S1 和 S2存在 $ S1 \\in S2 $, 不保留S1 core merging 保证了合并后核的高度一致性， 不受过滤阈值的影响 证明过程略 步骤 输入参数: core probing 返回的核 迭代： 对S中任意一对核Ki, Kj， 如果重叠， 转2 计算p-min, p-max, p- 如果 Ki, Kj的交集不为空，且 pi- 或 pj- 属于 该交集， 转4 从S 中移除Ki, 和 Kj, 加入Ki,Kj的并集。 如果遍历完， S 没有变化， 则退出 计算 p-min, p-max, p- p-min, pmax: 在特征空间内， 为 core生成了边界框 p- : 中心 图示： Affiliation Propagation 完成cores probe后，剩余的点作为 affiliated members 初始化社区C = 找到的核K ， 迭代处理： 对K中的每个点d，把所有的、其他的、能连到d的点u 加到C中 设定迭代次数， 避免关系环 或迭代中 没有新的点加入 相关概念 好像没什么用 两个社区的公共成员则为 interdisciplinary member 点和社区间的相近度(closeness)用迭代时的次数代表 Intra-Community Classification 只根据relation找到的社区 很可能误判(false hits) 要根据属性分析， 将当前的C 划分成两个集合， C' 和 C- 步骤 核K 视作是 positive example正例， 即肯定属于这个社区 选择社区C的核K（正例） 和 其他社区的核（negative example) 将所有点转换成 特征向量（feature vector）来代表它们的topical position 使用 LDA（Latent Dirichlet Allocation）来降维 使用某种分类器（SVM），将负标签的点都移除 图示 主要贡献 用关联规则、频繁项集来初始化 统一距离 Unified Distance structural and attribute similarities using a unified distance measure SA-Cluster 步骤 建立统一距离度量， 新的图 用新的图 进行聚类， 类k-means unified distance measure 基于属性增广图(attribute-argmented graph), 使用Random Walk with Restart (RWR) 邻点随机游走距离 Neighborhood Random Walk Distance l as the length that a random walk can go c ∈ (0, 1) as the restart probability attribute-argmented graph 添加属性点（attribute vertices），代表属性的值。 原始的点连接到对应的属性点上 两点上共同的属性点越多， 两点相似度直觉上就越高。 聚类算法 利用unified distance measure， 进行 k-medoids clustering（类似 k-means） 选择每个聚簇(cluster)最中心的点 其余点分配给最近的中心点。 迭代， 调整边的权重 聚类中心初始化 思想： 从vi走 l 步能到的点越多， vi越可能是中心 计算点的密度函数： 降序排列， 选择前k点作为聚类中心 聚类过程 分配点到最近的中心，即有最大random walk distance的中心点 对每个cluster ,用随机游走距离 计算\"平均点\" 寻找新的中心点，距\"平均点\"最近 不停迭代， 直到 聚类目标函数 收敛 聚类目标函数 目标是最大化 问题转化 有以上的目标函数后， 可转化成三个子问题 聚类分配 中心更新 权重调整 权重自我调整 在每次迭代时， 进行权重调整 属性 ai 权重在第t+1次迭代的计算公式为： 投票机制 majority voting mechanism counts the number of vertices within clusters that share the same attribute values with the centroids on ai 主要贡献 一个统一的距离评估方式， 将结构和属性相似度结合 带权重的自调整方法， 调节结构属性相似度的重要度 基于模型方法 Model-Based Method model-based community detection approach based on both structural and attribute aspects of a graph 步骤关键点 概率模型的构建， 结合结构和属性信息， 不使用人工定义的距离 变分法(variational approach)解决模型 构建概率模型 聚类属性图定义： X: n x n 的邻接矩阵 Y: n x t 的属性矩阵 Z: n x 1 的聚类向量， 即每个点所属的聚类 目标： 求最优化： 其中 联合概率分布 alpha - 每个聚类的点分布（vertex distribution) theta - 属性分布(attribute distribution) phi - 类间 边出现概率(edge occurrence prob) 两大问题 Z 的N个变量最大化 计算量过大， 全局最优基本不可能 计算Z的后验概率分布时， 不存在 p(Z|X,Y)的closed-form expression 变分法 variational algorithm 使用variational distribution q(α, θ, φ, Z) 来逼近原分布 并且对 variational distribution 作限制 全局最优就转成求局部最优 两个新问题 如何定义the family of variational distributions 如何从中找出最优分布， 最接近p(α, θ, φ, Z|X, Y) Parametric Family Optimizing Variational Parameters measure the distance between a variational distribution q(α, θ, φ, Z) and the true posterior p(α, θ, φ, Z|X, Y) 等价于 最大化 关系式： 图合并 Graph Merging combine structural and attribute information using the graph merging process CODICIL 步骤 创建内容边 create content edges 边组合 combining edges 边采样 sampling edges with bias 聚类 clustering creating content edges 对每个点vi, 用cosine相似度， 计算k 内容最近邻 在vi 和 k近邻间 建立content edges combining edges 将新创建的content edges 和 初始的拓扑边集进行简单的联合(unified) sampling edges with bias 对每个点 vi， 从邻点选择要保留的边， 通过 cosine 相似度或Jaccard 相似度 clustering 因为图合并部分独立于community detection， 所以任意 community detection 都可以， 这块不是本文的重点 主要贡献 通过 用内容信息消除连接结构里的噪音， 来强化社区信号 矩阵分解 Matrix Factorization 论文： Community Detection with Edge Content in Social Media Networks Edge-Induced Matrix Factorization 主要idea 通过从多层图中抽取相同因子(common factors) 把不同信息进行结合 使用通用的聚类方法处理 方法 使用 低秩矩阵因子分解(low-rank matrix factorization) 来逼近目标矩阵O， P: n x n 的特征矩阵 lambda(大写的？): n x n 特征值矩阵 目标 对于多个目标矩阵O&#94;i, i = 1,-,l 要算出一个common factor matrix 求最小化： P: n x n的所有层 公因子矩阵 Λ&#94;i: n x n 矩阵， 第i层的特征 || ·|| is the Frobenius norm α: regularization 参数 全局转局部最优 迭代处理： 固定P , 优化 Λ&#94;i 固定Λ&#94;i , 优化 P 直到 收敛 模式挖掘 Pattern Mining Coherent Closed Quasi-Clique Discovery from Large Dense Graph Databases Cocain 方法 子图挖掘算法， 搜索多层图中频率高于某给定阈值的 quasi-cliques 基础定义 gamma(γ)-Quasi-clique cross-graph quasi-clique: a set of vertices belonging to a quasi-clique appears on all layers must be the maximal set Edge Cut, Edge Connectivity edge cut is a set of edges Ec such that G'=(V ,E-Ec) is disconnected A minimum cut is the smallest set among all edge cuts. The edge connectivity of G, denoted by κ(G), is the size of the minimum cut coherent subgraph: a subgraph that satisfies a minimum cut bound gamma(γ)-Isomorphism 同构 若两个图G1, G2是 gamma同构， 当且仅当： 都是 γgamma-quasi-cliques 点个数相同 存在 biject f:V1->V2, 对V1中的每个点v, 满足F1(v) = F2(f(v)) multiset 点的标签的集合(a bag of vertex labels) 忽略顺序 突出多样性 定义为 M(G)， G的multiset string of a graph Given a k-graph g, any sequence of all elements in M(g) 给定 k-graph g, M(g)的任意一种序列 canonical form of a graph the minimum string among all its strings and denoted by CF(G) 图的最小 string, 记作 CF(G) 有引理： 两个γ-quasi-cliques Q1 Q2 是γ同构， 当且仅当 CF(Q1) = CF(Q2) 步骤 将子图转成 canonical forms 枚举γ-quasi-cliques可行解(feasible candidate for γ-quasi-cliques), 用DFS策略进行剪枝 基于 闭包检查规划(closure-checking scheme)， 选择出闭包的 γ-quasi-cliques 枚举策略 枚举树 满足： 子代必须能归入祖先 关键： 对每个 quasi-clique Q, 处理完它的子代后， 进行闭包检查 主要贡献 find cross-graph quasi-cliques in a multi-layer graph that are frequent, coherent, and closed 另一篇模式挖掘 论文 论文： Mining Coherent Subgraphs in Multi-Layer Graphs with Edge Labels 本文贡献 提出了带边标签的多层图聚类的新范式 提出了MLCS, 避免了结果集的冗余 提出了最好优先搜索算法MiMAG 来求MLCS聚类的近似解 multi-layer coherent subgraph (MLCS) model clusters of vertices that are densely connected by edges with similar labels in a subset of the graph layers 找聚类， 满足条件：在某层的图中，不仅边的密度高，且具有相似的标签。 the edge labels represent characteristics of the relations quasi-clique One-dimensional MLCS cluster One-dimensional MLCS cluster 某一图（层）的点集满足以下条件： 形成一个 0.5-quasi-clique 点集的每条边的两个顶点： dist(l_i(x), l_i(y)) <= w, edge label 为连续值时需要w, 不然置为0. 多层 MLCS cluster 冗余关系 redundancey relation MiMAG 算法 Mining Multi-layered, Attributed Graphs 计算出最大化、 无冗余、 高质量（不是最优质量）的聚类 基于寻找quasi-cliques的快速算法。 总结 Apriori 频繁项集的， 特例或通用 随机游走 概率模型 矩阵分解 clique community 没有严格定义 未来研究方向 通用多层图的适应性 General multi-layer graph applicability 当前算法一般仅研究了 pillar(柱形)多层图 现实世界不保证不同层之间正好一一对应 所以现有算法的泛化， 对通用多层图的适应性非常有意义 多层图的不确定性 Uncertainty in multi-layer graphs 现有的研究都假定 图数据已经清理完毕， 缺少噪音、歧义的研究 constructing multi-layer graphs with entity resolution and/or trustworthy analysis certainly enhances the quality of the community detection process 可扩展性问题 所以可能要考虑并行及分布式之类的方法 或是对多层图的特征向量矩阵（feature-vector matrices）进行采样 Temporal analysis 图是随时间变化的。 目前存在一些对单层图的时间变化的研究， 但基本不可用于多层图 谢谢！","tags":"研究","title":"多层网络聚落检测综述"},{"url":"/GaoCD.html","text":"导论 算法思路 通过 划分密度(partition density) 这个目标函数的最优化来寻找 连接群落link communities 通过 novel genotype representation method, 将 连接群落映射回 点群落。 群落数自动发现 算法描述 框架 目标函数 partition density D only considers the link density within the community, different from the common community definition that a community should be densely intra-connected and sparsely connected with the rest communities.# 划分密度D $$ D(c) = \\frac{m_c - (n_c - 1)}{\\frac{n_c(n_c-1)}{2} - (n_c-1)} $$ partition density D is the average of Dc over all communities $$ D = \\frac{2}{M}\\sum_c m_c\\frac{m_c - (n_c - 1)}{(n_c-2)(n_c-1)} $$ 3.3 基因表达 编码 基于连接的表示方法， 群体中的个体g 有 m 个基因， 下标i 代表边的序号 m 是边数——吓死人了。 gj 从连接的点中选一个。 当无向图中两边共点时， 两边相连 解码 把基因型转化成 分割（由连接群落组成）， gi 作为基因型， 值 j 可看作是边 i 和 边 j 有一个共同点， 并应该归入同一群落中。 桥接边： 连接两个 聚落的边。 Fine tuning: 调整 单一映射方法得到的点群落 的点附属关系 寻找有多附属关系的点 membership 计算这种点 对各群落是否有贡献——如果加入的话。 贡献计算方法： $$ AD(c) = 2 * \\frac{|E(c)|}/{|c|} $$ 添加点后， EC 上升， 则OK","tags":"研究","title":"GaoCD-总结"},{"url":"/ema_comminities_dynamic_networks.html","text":"摘要： 目标： 最大化当前数据的聚类准确性 最小化阶段过渡时的聚类漂移 clustering drift 新概念： temporal smoothness 短时平滑性 snapshot quality , temporal quality 快照质量和短时质量 优点： provides a solution representing the best trade-off between the accuracy of the clustering obtained, and the deviation from one time step to the successive. 为聚类的准确性及阶段过渡时的变动提出了一个最优折衷的方案 ** 问题是这几个东西都不是这篇文章提的概念， 只是函数可能有变化 导言 进化聚类方法(evolutionary clustering) 利用 temporal smoothness 框架。 核心假设： abrupt changes of clustering in a short time period are not desirable （译： 短时间内聚类突变是不值得要的？不合适的？） it smooths each community over time 平滑性的实现 折衷： snapshot quality: 在当前阶段所拥有数据下， 聚类要尽可能精确。 temporal cost: 每个聚类在阶段过渡时， 不能发生剧烈变化。 本文方法 名字： DYNMOGA (DYNamic MultiObjective Genetic Algorithms) 目标 最大化 snapshot quality, 表明当前聚类效果（准确性）， 为此调整了 modularity 的概念 最小化 temporal cost， 表明两阶段间聚类差别， 为此去计算 归一化互信息(normalized mutual information) 优势 利用这两个方法的优势 选择性搜索解空间， 不需要提前知道 聚类个数。 本文主要贡献 将动态网络中群落结构的检测问题 建模成 多目标优化问题--以前肯定有人弄过了，也算贡献？ 本方法可以考虑成 通用框架，应用于进化聚类。 仅仅需要修改目标函数，测试不同的质量函数--别人的算法也可以，这篇就是利用别人的框架。 本方法不需要参数， 不需要为快照和短时成本设置权重， 也不用设定聚类个数--不知道他人工作情况。 相关工作 主要工作 Evolutionary Clustering by Chakrabarti et al. in [13] 认为changes of connections in short time periods could be caused by noise. 提出了 temporal smoothness 和 snapshot cost temporal cost 问题是： not allow that the number of communities varies over time FacetNet by Lin et al[5] particle-and-density based clustering method by Kim and Han [3] 这些方法的主要问题 聚类个数 不知道。 相对于要选择 参数 alpha 去应用于 temporal smoothness。 DYNMOGA算法 DYNMOGA has been adapted with a customized population type that suitably represents a partitioning of a network and endowed with two complementary objectives 他们使用了 matlab 实现的 NSGA-II 算法框架, DYNMOGA支持 定制的、可表示网络分割情况的群体类型, 并具有两种互补的目标（然而并没有说是哪两种）。 目标函数 定义 $$$ CR&#94;t = { C&#94;t_1, ... C&#94;t_k } 是图在 t 阶段的聚类结果 一个聚类中有 n_S 个结点 m_S 条边。 m_S(u) = {v | v \\in C_t } 是结点u 在聚类C&#94;t 的邻点个数 c_S = { (u, v) | u \\in C&#94;t, v \\notin C&#94;t} 是聚类C&#94;t边界的边数。 l_S 是 只连接 模块 C&#94;t_S 内部结点 的边总数。 d_S 是 C&#94;t_S 中点的度数之和 $$$ 多种分值定义 Q: the first term of each summand is the fraction of edges inside a community, while the second one is the expected value of the fraction of edges that would be in the network if edges fall at random without regard to the community structure. Values approaching 1 indicate strong community structure modularity 颗粒度 ： $$ Q = \\sum&#94;k_{s=1}[\\frac{l_s}{m} - (\\frac{d_s}{2m})&#94;2] $$ conductance 导率, the fraction of edges pointing outside the clustering： $$ CO = \\sum&#94;k_{S=1}\\frac{c_S}{2m_S+c_S} $$ Normalized Cut 归一化分割 the fraction of total edge connections to all the nodes in the graph: $$ NC = \\sum&#94;k_{S=1}\\frac{c_S}{2m_S+c_S} + \\frac{c_S}{2(m-m_S)+c_S} $$ Community Score 群落分值, measure the fraction of internal edges of each cluster per nodes： $$ CS = \\sum&#94;k_{s=1}(\\sum_{u \\in C&#94;t}(\\frac{m_S(v)}{n_S})&#94;2) * \\frac{2m_S}{n_S} $$ 基因表达 locus-based adjacency representation [34] 每个个体包含 n 个基因， n 指代 结点的个数 每个基因 取值范围 1-n， 即第i个基因与第j个基因之间有连接，该划分到同一群落 ** 注： 这种表达肯定不能用于 群落重叠问题——然而 现实是， 主流用法 就是这样，大同小异 好处： 由个体组成部分的个数，在解码步骤中自动得到 decoding step 解码 使用并查集 建立并查集 makeset 对每条边去查找, findset 查到后的进行合并 初始化 一个有若干个体的群体， 对每个点i, 在邻接点中随机选择一个作为值， 表示 存在边 (i,j) uniform crossover 均匀交叉 给定两个父辈个体， 创建一个随机二元mask, 进行选择， 当 mask 为0时， 取第一个父辈个体的基因（值）， 为1时， 取第二个父辈个体。 如此组成子代的基因 突变 与初始化类似， 对结点i 随机变更值成其他邻点。","tags":"研究","title":"进化多目标方法在动态网络聚落检测中的应用-总结"},{"url":"/ea_based_communities_survey.html","text":"网络相关背景知识 定义 群落检测没有标准定义， 目前一般视为： 一组顶点，之间可能有着共同的属性或在图中起到相似的作用 分类 common model 共同？普通？模型 directed model 有向图模型 signed model 正负向模型 overlapping model 重叠模型 dynamic model 动态模型 进化算法与多目标优化 进化算法 相同性质： 省 框架： Algorithm 1 General framework of EAs 输入： 参数与问题实例 输出： 最优方案 Begin: population initialisation store optimal solutions for i = 1 to max_iteration do for each individual in the population do generate a new individual through stochastic components evaluate the fitness of the new individual endfor update optimal solutions endfor End 多目标优化 定义： 略 Pareto optimal solution: 待看书， 此处略 适应函数 单目标优化 4.1.1 基于颗粒度的模型 因为目标是多目标优化，先略， 如果有用再回来看 4.1.2 multi-resolution model 多分辨率？模型 同上 4.2 多目标优化 其他model 略 重叠模型 操作子 设计 个体表示 个体重现 个体局部搜索 结论 主要思想： 将群落检测建模成 单目标或多目标优化问题 设计元启发方法来解决 问题一： 根据 no free lunch 理论， 没有通用方法能解决全部类型的网络 不同网络的时-空特性不同 问题二： 由于数据集过大， 基于元启发方法的群落检测是LSGO问题（大规模全局优化）， 包含大量的决策变量，对现存优化技术是个挑战。 如何又快又好就值得思考 网络群落问题将超越纯粹结构分析，变成强调网络智能。","tags":"研究","title":"基于进化算法的群落检测问题综述总结"},{"url":"/overlap_communities_survey.html","text":"算法 1. Clique Percolation 中文名？派系过滤 假设 群落由 完全连通子图的重叠集合组成——a community consists of overlapping sets of fully connected subgraphs 思路 detects communities by searching for adjacent cliques 扩展内容 派系(Cliques)。在一个无向网络图中，\"派系\"指的是至少包含3个点的最大完备子图。这个概念包含3层含义：①一个派系至少包含三个点。②派系是完备的，根据完备图的定义，派系中任何两点之间都存在直接联系。③派系是\"最大\"的，即向这个子图中增加任何一点，将改变其\"完备\"的性质。 n-派系(n-Cliques)。对于一个总图来说，如果其中的一个子图满足如下条件，就称之为n-派系：在该子图中，任何两点之间在总图中的距离(即捷径的长度)最大不超过n。从形式化角度说，令d(i,j)代表两点和n在总图中的距离，那么一个n-派系的形式化定义就是一个满足如下条件的拥有点集的子图，即：d(i,J)\\le n，对于所有的，n_i,n_j\\in N,来说，在总图中不存在与子图中的任何点的距离不超过n的点。 n-宗派(n—Clan)。所谓n-宗派(n—Clan)是指满足以下条件的n-派系，即其中任何两点之间的捷径的距离都不超过n。可见，所有的n-宗派都是n-派系。 k-丛(k-Plex)。一个k-丛就是满足下列条件的一个凝聚子群，即在这样一个子群中，每个点都至少与除了k个点之外的其他点直接相连。也就是说，当这个凝聚子群的规模为n时，其中每个点至少都与该凝聚子群中n-k个点有直接联系，即每个点的度数都至少为n—k。 某个的步骤之一 begins by identifying all cliques of size k in a network, a new graph is constructed such that each vertex represents one of these k-cliques 结论 more like pattern matching rather than finding communities since they aim to find specific, localized structure in a network. 2. Line Graph and Link Partitioning 中文名？连接划分 思路 partitioning links A node in the original graph is called overlapping if links connected to it are put in more than one cluster. 3. Local Expansion and Optimization 中文名？局部增广和优化 思路 based on growing a natural community or a partial community rely on a local benefit function that characterizes the quality of a densely connected group of nodes 4. Fuzzy Detection 中文名？模糊检测 思路 quantify the strength of association between all pairs of nodes and communities 例子 Non-negative Matrix Factorization 5. Agent-Based and Dynamical Algorithms 中文名？基于啥的动态算法 思路 label propagation algorithm by allowing a node to have multiple labels 6. others 中文名——无法分类 :) 评估方法 1. Normalized Mutual Information 中文名？标准互信息 2. Omega Index 结论","tags":"研究","title":"群落覆盖问题总结"},{"url":"/findbugs_summary.html","text":"原文: Finding Bugs is Easy by David Hovemeyer, William Pugh 摘要 旧方法基于 formal methods 和 复杂程序分析， 难用， 无作用 bug patterns - detectors 简单的自动技术在遇到常规错误和难解特性时都有用 导论 conclusion 不存在这种bug, 过于明显，以至于在实际代码中没有找到例子——发现的bug中，有些十分明显， 让我们吃惊， 即使是在生产应用和库里 现代OO语言的高度复杂性， 对语言特性及API的滥用是屡见不鲜的。 自动bug检测可以在 程序正确性 上起到巨大作用","tags":"研究","title":"FindBugs总结"},{"url":"/human_kernel_summary.html","text":"bayesian nonparametric models such as Gaussian processes function extrapolation problems kernel learning framework ** reverse the human-like and inductive biases of human across a set of behavioral experiments to gain psychological insights and to extrapolate in human model ability determined by its support(which solutions are a priori possible ) inductive biases (which solutions are a priori likely) controlled by a covariance kernel","tags":"研究","title":"human kernel 总结"},{"url":"/d3js_learn_1.html","text":"流程 一、添加 svg 画布， 得到一个 选择器selection var svg = d3.select(\"body\") //选择文档中的某一元素，根据CSS规范，如 \"body\" .append(\"svg\") //添加一个svg元素, 符合html操作 .attr(\"width\", width) //设定宽度 .attr(\"height\", height); //设定高度 二、布局（转换数据） 2.1 种类 布局种类 2.2 定义布局 var mylayout = d3.layout.tree() // 例： Tree布局 2.3 设置布局的基本属性（接上一项） .size ( [ width , height ] ) // tree 只想到这个属性 2.4 用布局转换数据 var nodes = mylayout.nodes(data)[.reverse()] // reverse 逆序， data格式有讲究， json且属性名字限定 var links = mylayout.links(nodes) // 树图要生成点和边 或者 var piedata = pie(dataset); // 饼状图就不需要边了 三、数据绑定到元素（默认新建元素） 3.1 从画布选择器中，再选择后代元素 svg.select(name) // 选择第一个匹配的元素 或者 svg.selectAll(name) // 选择全部匹配的元素 3.2 加载数据（接上一项） .data ( dataset ) // 元素和数据集一对一加载 , 此时得到的叫 update 部分 或者 .datum ( oneData ) // 一个数据绑定给全部元素 3.3 对应数据来创建元素（接上一项），如果元素是现成的不需要建 .enter () // 只能接上一项 数据加载 , 此时得到的叫 enter 部分 .append ( type ) // 创建需要类型的元素 3.4 数据属性设置（例） 根据上一项的 type 和 数据， 设置需要的属性 . attr ( \"x\" , 20 ) //屏幕上的起始横坐标 . attr ( \"y\" , function ( d , i ){ //屏幕上的起始纵坐标 return i * rectHeight ; }) . attr ( \"width\" , function ( d ){ // 元素宽度（有这个属性的话） return d ; }) . attr ( \"height\" , rectHeight - 2 ) // 元素高度（有这个属性的话） . attr ( \"fill\" , \"steelblue\" ); // 颜色填充 3.5 事件设置（也是属性） . on ( \"mouseover\" , function ( d , i ){}) . on ( \"mouseout\" , function ( d , i ){}) . on ( \"click\" , function ( d , i ){}) 在该元素下继续添加其他子元素 前面3.1-3.5步骤结果保存在某变量中， 即可使用该变量继续append,设置属性和事件 四、动画效果 4.1 方法 transition() // 启动过渡效果, 其前后是图形变化前后的状态（形状、位置、颜色等等） duration() // 指定过渡的持续时间，单位为毫秒 ease() 指定过渡的方式，常用的有： linear：普通的线性变化 circle：慢慢地到达变换的最终状态 elastic：带有弹跳的到达最终状态 bounce：在最终状态处弹跳几次 delay() 指定延迟的时间，可用匿名函数function(d,i) 指定各个的延迟 4.2 创建动态效果 var transition = selection.transition() .duration(time) .attr()... // 定义变量是可能用于4.3 4.3 对子元素进行处理 如果元素中有子元素，需要一并处理。 transition.select(name).attr()","tags":"工具","title":"D3.js 学习心得一"},{"url":"/memory-movies-week1.html","text":"记忆概念 分类 working memory episodic memory semantic memory procedural memory working memory workbench(工作台) -- 维持、操作思维和意识 特点：短暂","tags":"心理学","title":"记忆与电影-第一周"},{"url":"/word2vec-2-mindmap.html","text":"译自： 原文链接 (没有找过作者， 随手就翻译了) 思维导图这一工具因其长于组织大量任务、材料信息，在头脑风暴、 计划和问题解决等领域得到广泛使用、一致好评。 对思路的可视整理有助于整个思考的过程， 并且模拟了我们人类思考时获取脑中知识的方式。 现今有很多工具可以帮助我们画出思维导图， 但还没有一个能自行生成的， \"生成\"是指从文本（语音）内容中提成。 为了做到这一点， 我花了最长的时间（至今快8个月了）， 研究如何结合文本挖掘和图论做成一个框架来生成思维导图（给定一段文本）。 当然， 第一个问题就是， 任意一段文字都不会只有那么一种可行的思维导图。 只是， 如果你要构建自己的思维导图， 有这么一个自动工具， 可能会给你更多的思路和洞见， 特别是头脑风暴时， 或帮你查缺补漏。 那我们先来看看一个思维导图的样式—— 两个关键点： 思维导图并不简单地是一棵树， 不只是递归地将主题划分成子主题。 它本质上更像图， 连接项在语义上是相关的。 正如‘夜晚'可能会让你想到‘白天'， 思维导图中， 意义相反的两个概念之间也很可能存在连接。 还有诸如使用图片强化概念等其他点。但这些并不是本文的主旨（我的设计师风格创造力糟透了）。 有备无患 ， Heres 这篇文章能帮助你熟悉构建和使用思维导图的过程。 在我上一篇博文 链接 中， 我描述了一种从文本生成Word2Vec模型的方法（使用维基的文章作为示例）。 在这里， 我将描述我使用的从 Word2Vec模型生成基本思维导图的方法。 第一步： 从文章中找出前n项 （就像我上一篇博文所说， 我只使用stemmed unigrams（一个词干的n-gram), 你可以自行采用更高阶的ngrams, 想来会更棘手（准确来说， 是当你生成n-gram的算法有效时） 这里的 n 是指思维导图中的节点数， 在我多次尝试之后， 50是个比较好的数字， 太小则信息少， 太大则噪音多。 欢迎尝试其他数字。 我使用了本文 链接 中写的 co-occurrence 方法， 列出文本的前 n 项词。 代码如下： def _get_param_matrices(vocabulary, sentence_terms): \"\"\" Returns ======= 1. Top 300(or lesser, if vocab is short) most frequent terms(list) 2. co-occurence matrix wrt the most frequent terms(dict) 3. Dict containing Pg of most-frequent terms(dict) 4. nw(no of terms affected) of each term(dict) \"\"\" #Figure out top n terms with respect to mere occurences n = min(300, len(vocabulary)) topterms = list(vocabulary.keys()) topterms.sort(key = lambda x: vocabulary[x], reverse = True) topterms = topterms[:n] #nw maps term to the number of terms it 'affects' #(sum of number of terms in all sentences it #appears in) nw = {} #Co-occurence values are wrt top terms only co_occur = {} #Initially, co-occurence matrix is empty for x in vocabulary: co_occur[x] = [0 for i in range(len(topterms))] #Iterate over list of all sentences' vocabulary dictionaries #Build the co-occurence matrix for sentence in sentence_terms: total_terms = sum(list(sentence.values())) #This list contains the indices of all terms from topterms, #that are present in this sentence top_indices = [] #Populate top_indices top_indices = [topterms.index(x) for x in sentence if x in topterms] #Update nw dict, and co-occurence matrix for term in sentence: nw[term] = nw.get(term, 0) + total_terms for index in top_indices: co_occur[term][index] += (sentence[term] * sentence[topterms[index]]) #Pg is just nw[term]/total vocabulary of text Pg = {} N = sum(list(vocabulary.values())) for x in topterms: Pg[x] = float(nw[x])/N return topterms, co_occur, Pg, nw def get_top_n_terms(vocabulary, sentence_terms, n=50): \"\"\" Returns the top 'n' terms from a block of text, in the form of a list, from most important to least. 'vocabulary' should be a dict mapping each term to the number of its occurences in the entire text. 'sentence_terms' should be an iterable of dicts, each denoting the vocabulary of the corresponding sentence. \"\"\" #First compute the matrices topterms, co_occur, Pg, nw = _get_param_matrices(vocabulary, sentence_terms) #This dict will map each term to its weightage with respect to the #document result = {} N = sum(list(vocabulary.values())) #Iterates over all terms in vocabulary for term in co_occur: term = str(term) org_term = str(term) for x in Pg: #expected_cooccur is the expected cooccurence of term with this #term, based on nw value of this and Pg value of the other expected_cooccur = nw[term] * Pg[x] #Result measures the difference(in no of terms) of expected #cooccurence and actual cooccurence result[org_term] = ((co_occur[term][topterms.index(x)] - expected_cooccur)**2/ float(expected_cooccur)) terms = list(result.keys()) terms.sort(key=lambda x: result[x], reverse=True) return terms[:n] get_top_n_terms 函数实现了这个功能， 我希望我写的 docstring 和 注释很好地解释了整个过程（结合起那篇论文）。 如果你有时间， 足够耐心， 你可以看到你Word2Vec模型里的整个词库（entire vocabulary）， 并找到你想加入到你的思维导图里的那些项。 这样做大概能得到最好的结果（就是太辛苦）。 第二步： 选定根节点 根结点是最能表达思维导图中心思想的。 相比起整个词库entire vocabulary， 选中的结点个数小上许多， 所以， 也许最好就是 手工选定根结点的项。 或者， 使用出现频率最高的（has the highest occurrence）。这一步也需要很多尝试（但数学科学能起什么作用吗） 第三步： 生成导图 这是至关重要的一步， 也是我花了最多时间的。 首先， 我需要定义一个项（term）的 情境向量（contextual vector） 假设， 本导图的根是‘电脑'， 连到另一个项‘硬件'， ‘硬件'再连‘键盘'， 那么， ‘键盘'的Word2Vec向量以 model[keyboard]的方式在Python/Gensim中获得。 定义这个向量为 $ v_{keyboard} $ 现在考虑构建过程。 因为你目前已经有了一些东西， 你再想到'键盘' 时， 其实已经处于'电脑'和 '硬件' 的情境（上下文）中。 所以你很难把 '键盘‘ 跟 '音乐‘ 联系起来（最起码不直接相关）。 可见， '键盘' 的contextual vector情境向量（定义为 $ v&#94;{'}_{keyboard} $ ) 一定会将方向偏向到 $ v 和 v_{hardware} $ (be biased in its direction towards). ---- 我们要计算 Word2Vec 模型的 cosine 相似度， 当然只跟方向有关。 从直觉上说， $ v_{hardware} 和 v&#94;{'}_{keyboard} $ 的影响应该大于的影响应该大于 v ， 也就是距离越远， 父节点的影响会越小。 为了考虑这个因素， 我再加入了一个 参数 情境递减因子 αα 。 数学表达如下： $$ v&#94;{'}_{computer} = v v&#94;{'} {hardware} = (1-\\alpha)v + \\alpha v&#94;{'} v&#94;{'} = (1-\\alpha)v_{keyboard} + \\alpha v&#94;{'}_{hardware} $$ 最后， 可以生成实际的导图了， 以下是我使用的算法（我希望行内注释能帮你理解我的工作） from scipy.spatial.distance import cosine from networkx import Graph def build_mind_map ( model , stemmer , root , nodes , alpha = 0.2 ): \"\"\" Returns the Mind-Map in the form of a NetworkX Graph instance. 'model' should be an instance of gensim.models.Word2Vec 'nodes' should be a list of terms, included in the vocabulary of 'model'. 'root' should be the node that is to be used as the root of the Mind Map graph. 'stemmer' should be an instance of StemmingHelper. \"\"\" #This will be the Mind-Map g = Graph () #Ensure that the every node is in the vocabulary of the Word2Vec #model, and that the root itself is included in the given nodes for node in nodes : if node not in model . vocab : raise ValueError ( node + \" not in model's vocabulary\" ) if root not in nodes : raise ValueError ( \"root not in nodes\" ) ##Containers for algorithm run #Initially, all nodes are unvisited unvisited_nodes = set ( nodes ) #Initially, no nodes are visited visited_nodes = set ([]) #The following will map visited node to its contextual vector visited_node_vectors = {} #Thw following will map unvisited nodes to (closest_distance, parent) #parent will obviously be a visited node node_distances = {} #Initialization with respect to root current_node = root visited_node_vectors [ root ] = model [ root ] unvisited_nodes . remove ( root ) visited_nodes . add ( root ) #Build the Mind-Map in n-1 iterations for i in range ( 1 , len ( nodes )): #For every unvisited node 'x' for x in unvisited_nodes : #Compute contextual distance between current node and x dist_from_current = cosine ( visited_node_vectors [ current_node ], model [ x ]) #Get the least contextual distance to x found until now distance = node_distances . get ( x , ( 100 , '' )) #If current node provides a shorter path to x, update x's #distance and parent information if distance [ 0 ] > dist_from_current : node_distances [ x ] = ( dist_from_current , current_node ) #Choose next 'current' as that unvisited node, which has the #lowest contextual distance from any of the visited nodes next_node = min ( unvisited_nodes , key = lambda x : node_distances [ x ][ 0 ]) ##Update all containers parent = node_distances [ next_node ][ 1 ] del node_distances [ next_node ] next_node_vect = (( 1 - alpha ) * model [ next_node ] + alpha * visited_node_vectors [ parent ]) visited_node_vectors [ next_node ] = next_node_vect unvisited_nodes . remove ( next_node ) visited_nodes . add ( next_node ) #Add the link between newly selected node and its parent(from the #visited nodes) to the NetworkX Graph instance g . add_edge ( stemmer . original_form ( parent ) . capitalize (), stemmer . original_form ( next_node ) . capitalize ()) #The new node becomes the current node for the next iteration current_node = next_node return g 备注： 我使用了 NetworkX 的简易图构建架构来完成了思维导图生成的核心任务（使之更易用于可视化）。 要计算 余弦距离， 我使用了 SciPy. 另外注意74和75行， 我使用了上篇博文所写的 StemmingHelper 类， 所以在思维导图中显示的是词干原始形式， 而不是词干。可以将StemmingHelper类直接当做参数 stemmer 传入。 所以， 如果你不需要词干处理， 那就把第4,74,75三行的代码干掉吧。 如果你仔细看过代码， 你会发现， 这看着很像 迪杰斯特拉的单点最短路径， 只是情境不同。 示例输出 原文链接 自己看。 看着不错， 和我人工画的也挺像的。 其他 还有一些可尝试的东西。 比如加入 bi-grams 和 trigrams。 我相信能让 Word2Vec 模型更强大， 能对文本做出更好的释义。 导图中仍存在多余项， 但它给出了文本的（最？）短长度（相对其他文本挖掘任务来说）， 这种关键词提取算法（我在上文提到过的论文）似乎相当不错。 这段翻译有点不确认（There are some unnecessary terms in the Mind Maps, but given the short length of the texts (compared to most text mining tasks), the Keyword extraction algorithm in the paper I mentioned before, seems really good.） 此方法可用于头脑风暴， 从你选择的一点出发， 这代码框架会给出建议的可连项， 你再做出选择， 然后又可以得到新的推荐——就有点像思维导图助手。 无论怎样， 这都是篇长博文了， 谢谢你坚持着读完全文！（翻译也一样感谢您的阅读）。","tags":"翻译","title":"基于Word2Vec生成基本的思维导图"}]}