{"pages":[{"url":"/ea_based_communities_survey.html","text":"网络相关背景知识 定义 群落检测没有标准定义， 目前一般视为： 一组顶点，之间可能有着共同的属性或在图中起到相似的作用 分类 common model 共同？普通？模型 directed model 有向图模型 signed model 正负向模型 overlapping model 重叠模型 dynamic model 动态模型 进化算法与多目标优化 进化算法 相同性质： 省 框架： Algorithm 1 General framework of EAs 输入： 参数与问题实例 输出： 最优方案 Begin: population initialisation store optimal solutions for i = 1 to max_iteration do for each individual in the population do generate a new individual through stochastic components evaluate the fitness of the new individual endfor update optimal solutions endfor End 多目标优化 定义： 略 Pareto optimal solution: 待看书， 此处略 适应函数 单目标优化 4.1.1 基于颗粒度的模型 因为目标是多目标优化，先略， 如果有用再回来看 4.1.2 multi-resolution model 多分辨率？模型 同上 4.2 多目标优化 其他model 略 重叠模型 操作子 设计 个体表示 个体重现 个体局部搜索 结论 主要思想： 将群落检测建模成 单目标或多目标优化问题 设计元启发方法来解决 问题一： 根据 no free lunch 理论， 没有通用方法能解决全部类型的网络 不同网络的时-空特性不同 问题二： 由于数据集过大， 基于元启发方法的群落检测是LSGO问题（大规模全局优化）， 包含大量的决策变量，对现存优化技术是个挑战。 如何又快又好就值得思考 网络群落问题将超越纯粹结构分析，变成强调网络智能。","tags":"研究","title":"基于进化算法的群落检测问题综述总结"},{"url":"/overlap_communities_survey.html","text":"算法 1. Clique Percolation 中文名？派系过滤 假设 群落由 完全连通子图的重叠集合组成——a community consists of overlapping sets of fully connected subgraphs 思路 detects communities by searching for adjacent cliques 扩展内容 派系(Cliques)。在一个无向网络图中，\"派系\"指的是至少包含3个点的最大完备子图。这个概念包含3层含义：①一个派系至少包含三个点。②派系是完备的，根据完备图的定义，派系中任何两点之间都存在直接联系。③派系是\"最大\"的，即向这个子图中增加任何一点，将改变其\"完备\"的性质。 n-派系(n-Cliques)。对于一个总图来说，如果其中的一个子图满足如下条件，就称之为n-派系：在该子图中，任何两点之间在总图中的距离(即捷径的长度)最大不超过n。从形式化角度说，令d(i,j)代表两点和n在总图中的距离，那么一个n-派系的形式化定义就是一个满足如下条件的拥有点集的子图，即：d(i,J)\\le n，对于所有的，n_i,n_j\\in N,来说，在总图中不存在与子图中的任何点的距离不超过n的点。 n-宗派(n—Clan)。所谓n-宗派(n—Clan)是指满足以下条件的n-派系，即其中任何两点之间的捷径的距离都不超过n。可见，所有的n-宗派都是n-派系。 k-丛(k-Plex)。一个k-丛就是满足下列条件的一个凝聚子群，即在这样一个子群中，每个点都至少与除了k个点之外的其他点直接相连。也就是说，当这个凝聚子群的规模为n时，其中每个点至少都与该凝聚子群中n-k个点有直接联系，即每个点的度数都至少为n—k。 某个的步骤之一 begins by identifying all cliques of size k in a network, a new graph is constructed such that each vertex represents one of these k-cliques 结论 more like pattern matching rather than finding communities since they aim to find specific, localized structure in a network. 2. Line Graph and Link Partitioning 中文名？连接划分 思路 partitioning links A node in the original graph is called overlapping if links connected to it are put in more than one cluster. 3. Local Expansion and Optimization 中文名？局部增广和优化 思路 based on growing a natural community or a partial community rely on a local benefit function that characterizes the quality of a densely connected group of nodes 4. Fuzzy Detection 中文名？模糊检测 思路 quantify the strength of association between all pairs of nodes and communities 例子 Non-negative Matrix Factorization 5. Agent-Based and Dynamical Algorithms 中文名？基于啥的动态算法 思路 label propagation algorithm by allowing a node to have multiple labels 6. others 中文名——无法分类 :) 评估方法 1. Normalized Mutual Information 中文名？标准互信息 2. Omega Index 结论","tags":"研究","title":"群落覆盖问题总结"},{"url":"/findbugs_summary.html","text":"原文: Finding Bugs is Easy by David Hovemeyer, William Pugh 摘要 旧方法基于 formal methods 和 复杂程序分析， 难用， 无作用 bug patterns - detectors 简单的自动技术在遇到常规错误和难解特性时都有用 导论 conclusion 不存在这种bug, 过于明显，以至于在实际代码中没有找到例子——发现的bug中，有些十分明显， 让我们吃惊， 即使是在生产应用和库里 现代OO语言的高度复杂性， 对语言特性及API的滥用是屡见不鲜的。 自动bug检测可以在 程序正确性 上起到巨大作用","tags":"研究","title":"FindBugs总结"},{"url":"/human_kernel_summary.html","text":"bayesian nonparametric models such as Gaussian processes function extrapolation problems kernel learning framework ** reverse the human-like and inductive biases of human across a set of behavioral experiments to gain psychological insights and to extrapolate in human model ability determined by its support(which solutions are a priori possible ) inductive biases (which solutions are a priori likely) controlled by a covariance kernel","tags":"研究","title":"human kernel 总结"},{"url":"/FOCS_summary.html","text":"基本定义 图： G(V, E) 目标—— 找到一簇子图(全部并正确)， 每个子图都是一个团（community) 即 $ S = {S_i | S_i \\subset V } $ 团（community)： 子图中任意点在该子图中的连通性 高于 非团的子图 定义 包含点 $ v_j $ 的团的集合为： $$ S(v_j) = {S_i | v_j \\in S_i \\land S_i \\in S } $$ disjoint cluster: $$ |S(v_j)| \\le 1 $$ overlapped cluster: > 1 定义 $ N(v_j) $ 为 $v_j$ 的邻接点 定义 $ N_i(v_j) $ 为 $v_j$ 在团 $S_i$ 的邻接点， 即 $$ N_i(v_j) ={v_k | (v_j, v_k) \\in E \\land v_k \\in S_i } $$ 新概念定义 团连通性 community connectedness, 即点 $v_j$在团$S_i$邻点超阈值个数 除以 该团点数, 代表 点对应团的归属性。 $$ \\zeta&#94;i_j = \\frac{|N_i(v_j)|-K+1}{|S_i| - K} if |N_i(v_j)| > K, else, 0 $$ 邻接连通性 neighborhood connectedness， 即 点 $v_j$在团$S_i$邻点数 除以 $v_j$的总邻点数， 代表 点加入新团的可能性 $$ \\xi&#94;i_j = \\frac{|N_i(v_j)|}{|N(v_j)|} $$ 外围结点 peripheral node: $v_i$的团邻接点 $$ Added_i = {v_k|v_k \\in N(v_i) \\land v_k \\in S_i }, \\forall S_i \\in S&#94;l $$ 步骤 初始化 初始化全部有K个以上邻接点的点， 由它及其邻接点组成团 $S_i$ 定义该阶段的外围结点 脱离阶段 对前面所有团里的点 $ V_j$, 计算相应的 团连通性 $ \\zeta&#94;i_j $ 邻接连通性 $ \\xi&#94;i_j $ 将[0, 1] 区间划分为 $max(20, N(v_j))$ 块， 每块初始化为0. 根据团连通性分数， 统计各区间 点的个数。 标记 最右的非0元， 并开始向左遍历， 直到： 遍历完毕 或 遇到某区间，<=标记值(最右非零元)， 且<=左边区间值， 这个值选为 留存阈值 stay cut-of of \\zeta 外围结点 $v_k \\in Added_i $ 排除出 $S_i$ , 当团连通性分数 $ \\zeta&#94;i_k $ 比留存阈值低。 Removal of only peripheral nodes ensures that nodes that form the core of a community are never eliminated 扩充阶段 对上一阶段处理后的团中所有点$v_j$， 当以下条件满足： 1. 该点未入 $ S_i$团 2. 该点选入 $S_i$团的可能性高（即 邻接连通性 $\\xi&#94;i_j$ 高于选中阈值 join cut-off ） join cut-off 选中阈值的计算方式与 stay 相同 去重阶段 代码描述 参考链接 focs-code /*! * * IPython notebook * */.ansibold{font-weight:700}.ansiblack{}.ansired{color:#8b0000}.ansigreen{6400}.ansiyellow{color:#c4a000}.ansiblue{8b}.ansipurple{color:#9400d3}.ansicyan{color:#4682b4}.ansigray{color:gray}.ansibgblack{background-}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:#ff0}.ansibgblue{background-f}.ansibgpurple{background-color:#ff00ff}.ansibgcyan{background-ff}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;display:-moz-box;-moz-box-orient:vertical;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:0}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;display:-moz-box;-moz-box-orient:vertical;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;display:-moz-box;-moz-box-orient:horizontal;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:700;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a,div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){.prompt{text-align:left}div.unrecognized_cell>div.prompt{display:none}}div.code_cell{}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;display:-moz-box;-moz-box-orient:horizontal;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{-webkit-box-orient:vertical;-moz-box-orient:vertical;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:0 0}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base,.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#BA2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88F}.highlight-keyword{color:green;font-weight:700}.highlight-builtin{color:green}.highlight-error{color:red}.highlight-operator{color:#A2F;font-weight:700}.highlight-meta{color:#A2F}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{f}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{color:green;font-weight:700}.cm-s-ipython span.cm-atom{color:#88F}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#A2F;font-weight:700}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#BA2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#A2F}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{color:green}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{f}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:red}.cm-s-ipython span.cm-tab{background:url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=')right no-repeat}div.output_wrapper{display:-webkit-box;-webkit-box-align:stretch;display:-moz-box;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;z-index:1}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,.8);box-shadow:inset 0 2px 8px rgba(0,0,0,.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;display:-moz-box;-moz-box-orient:vertical;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,.5)}div.output_prompt{color:#8b0000}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;display:-moz-box;-moz-box-orient:horizontal;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left!important}div.output_area div.output_area img,div.output_area svg{max-width:100%;height:auto}div.output_area img.unconfined,div.output_area svg.unconfined{max-width:none}.output{display:-webkit-box;-webkit-box-orient:vertical;display:-moz-box;-moz-box-orient:vertical;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{-webkit-box-orient:vertical;-moz-box-orient:vertical;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;background-color:transparent;border-radius:0}div.output_subarea{overflow-x:auto;padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1;max-width:calc(100% - 14ex)}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:#8b0000}div.raw_input_container{font-family:monospace;padding-top:5px}span.raw_input_prompt{}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:700;color:red}div.output_unrecognized a,div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link,.rendered_html :visited,.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child,.rendered_html h5:first-child,.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ol,.rendered_html *+ul{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;display:-moz-box;-moz-box-orient:horizontal;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:0;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.rendered .text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:700;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5,.cm-header-6{font-size:100%;font-style:italic} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ In [3]: % matplotlib inline import matplotlib.pyplot as plt import numpy as np import networkx as nx In [146]: import random from itertools import combinations edges = [] for ( u , v ) in combinations ( range ( 10 ), 2 ): r = random . random () if r < 0.5 : edges . append (( u , v )) for ( u , v ) in combinations ( range ( 8 ), 2 ): r = random . random () if r < 0.5 : edges . append (( u + 10 , v + 10 )) for ( u , v ) in combinations ( range ( 9 ), 2 ): r = random . random () if r < 0.6 : edges . append (( u + 18 , v + 18 )) for ( u , v ) in combinations ( range ( 5 ), 2 ): r = random . random () if r < 0.4 : edges . append (( u + 27 , v + 27 )) for ( u , v ) in combinations ( range ( 6 ), 2 ): r = random . random () if r < 0.1 : edges . append (( u , v + 10 )) for ( u , v ) in combinations ( range ( 7 ), 2 ): r = random . random () if r < 0.3 : edges . append (( u , v + 18 )) for ( u , v ) in combinations ( range ( 6 ), 2 ): r = random . random () if r < 0.2 : edges . append (( u , v + 27 )) print len ( edges ) test = nx . Graph () test . add_edges_from ( edges ) 76 [(0, 1), (0, 3), (0, 6), (0, 7), (0, 8), (0, 28), (1, 32), (1, 2), (1, 3), (1, 4), (1, 8), (1, 9), (1, 24), (2, 3), (2, 4), (2, 5), (2, 7), (2, 14), (2, 22), (2, 31), (3, 32), (3, 8), (3, 23), (3, 24), (4, 5), (4, 6), (4, 8), (4, 9), (4, 23), (5, 9), (7, 9), (8, 9), (10, 16), (10, 17), (10, 13), (10, 15), (11, 16), (11, 12), (11, 14), (11, 15), (12, 17), (12, 14), (14, 16), (15, 17), (16, 17), (18, 25), (18, 19), (18, 21), (18, 22), (19, 21), (19, 22), (19, 24), (19, 25), (19, 26), (20, 24), (20, 25), (20, 21), (20, 22), (20, 23), (21, 23), (21, 24), (21, 25), (21, 26), (22, 24), (22, 25), (23, 25), (24, 25), (24, 26), (25, 26), (27, 29), (27, 31), (28, 29), (28, 30), (29, 30), (29, 31), (30, 31)] In [148]: fp = file ( 'dataset' + str ( len ( test )) + '.txt' , 'w' ) for ( u , v ) in edges : fp . write ( ' %d %d \\n ' % ( u , v )) fp . close () In [206]: nx . draw ( test , with_label = True ) In [231]: pos = nx . spring_layout ( test ) nx . draw_networkx ( test , pos = pos ) In [5]: added = {} In [65]: def initializeCommunities ( g , k , S ): for v in g . nodes (): neighbors = g . neighbors ( v ) if len ( neighbors ) >= k : S [ v ] = set ( neighbors ) S [ v ] . add ( v ) added [ v ] = set ( neighbors ) #S.add(v) In [118]: def zeta ( v , S , g , k ): neighbors = set ( g . neighbors ( v )) . intersection ( S ) return 1.0 * ( len ( neighbors ) - k + 1 ) / ( len ( S ) - k ) if len ( neighbors ) > k else 0 def xi ( v , S , g ): neighbors = set ( g . neighbors ( v )) return 1.0 * len ( neighbors . intersection ( S )) / len ( neighbors ) def psi ( C , Cdot ): return 1.0 * len ( C . intersection ( Cdot )) / min ( len ( C ), len ( Cdot )) def findStayOff ( temp ): anchor = 0 mark = 0 for e in range ( len ( temp ) - 1 , 0 , - 1 ): if not mark and temp [ e ]: mark = temp [ e ] if temp [ e - 1 ] > temp [ e ] and temp [ e ] < mark : anchor = e break return anchor In [119]: temp = [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 ] print findStayOff ( temp ) 7 In [8]: def computeScoreAndStayOff ( S , v , g , k ): count = max ( 20 , len ( g . neighbors ( v ))) ratio = 1.0 / count zeta_list = [ 0 ] * ( count + 1 ) data = {} score = zeta ( v , S [ s ], g , k ) if score > 0 : data [ s ] = score bucket = int ( score / ratio ) zeta_list [ bucket ] += 1 #if score > 0: # print score, len(set(g.neighbors(v)).intersection(S[s])), len(S[s]), bucket anchor = findStayOff ( zeta_list ) data [ 'cutoff' ] = ( anchor - 1 ) * ratio return data File \"<ipython-input-8-a2d52df34c17>\" , line 7 score = zeta(v, S[s], g, k) &#94; IndentationError : unexpected indent In [63]: def duplicationRemovel ( S , ovl ): delete_list = [] for ( s1 , s2 ) in combinations ( S . keys (), 2 ): if psi ( S [ s1 ], S [ s2 ]) > ovl and s1 not in delete_list and s2 not in delete_list : delete_list . append ( s2 ) print 'duplicate' , delete_list , len ( delete_list ) for e in delete_list : S . pop ( e ) return S In [124]: def selectBucket ( scores_list , count ): ratio = 1.0 / count bucket_list = [ 0 ] * ( count + 1 ) for e in scores_list : bucket = int ( e / ratio ) bucket_list [ bucket ] += 1 anchor = findStayOff ( bucket_list ) #print scores_list, bucket_list, anchor return anchor In [120]: print selectBucket ([ 0 , 1.0 ], 20 ) [0, 1.0] [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 1 1 In [127]: def computeZetaList ( S , g , k ): zeta_scores = {} for i in S : for vj in S [ i ]: score = zeta ( vj , S [ i ], g , k ) if vj not in zeta_scores : zeta_scores [ vj ] = {} zeta_scores [ vj ][ i ] = score # for vj in zeta_scores: # print vj, zeta_scores[vj], g.neighbors(vj) return zeta_scores In [149]: S = {} initializeCommunities ( test , 2 , S ) S = duplicationRemovel ( S , 0.6 ) #for n in S: # print n, S[n] duplicate [6, 8, 32, 3, 4, 5, 7, 9, 15, 17, 12, 14, 19, 21, 22, 24, 25, 23, 29, 30, 31] 21 In [150]: zeta_scores = computeZetaList ( S , test , 2 ) for v in zeta_scores : count = max ( 20 , len ( test . neighbors ( v ))) anchor = selectBucket ( zeta_scores [ v ] . values (), count ) print v , zeta_scores [ v ], anchor zeta_scores [ v ][ 'cutoff' ] = anchor * 1.0 / count 0 {0: 1.0, 1: 0.2857142857142857, 28: 0} 6 1 {0: 0.4, 1: 1.0, 2: 0.2857142857142857} 9 2 {1: 0.2857142857142857, 2: 1.0} 6 3 {0: 0.4, 1: 0.7142857142857143, 2: 0} 9 4 {1: 0.42857142857142855, 2: 0.2857142857142857} 6 5 {2: 0} 0 6 {0: 0} 0 7 {0: 0, 2: 0} 0 8 {0: 0.4, 1: 0.5714285714285714} 9 9 {1: 0.2857142857142857} 0 10 {16: 0, 10: 1.0} 1 11 {16: 0, 11: 1.0} 1 12 {11: 0} 0 13 {10: 0} 0 14 {16: 0, 2: 0, 11: 0.6666666666666666} 1 15 {10: 0, 11: 0} 0 16 {16: 1.0, 10: 0, 11: 0} 1 17 {16: 0, 10: 0.6666666666666666} 1 18 {18: 1.0} 0 19 {18: 1.0, 26: 1.0} 0 20 {20: 1.0} 0 21 {18: 0.6666666666666666, 20: 0.75, 26: 1.0} 16 22 {2: 0, 20: 0.5, 18: 0.6666666666666666} 11 23 {20: 0.5} 0 24 {1: 0, 26: 1.0, 20: 0.75} 16 25 {18: 1.0, 20: 1.0, 26: 1.0} 0 26 {26: 1.0} 0 27 {27: 0} 0 28 {0: 0, 28: 1.0} 1 29 {27: 0, 28: 0} 0 30 {28: 0} 0 31 {2: 0, 27: 0} 0 32 {1: 0} 0 In [193]: def leaveCommunities ( S , g , k , ovl ): S = duplicationRemovel ( S , ovl ) zeta_scores = computeZetaList ( S , g , k ) for v in zeta_scores : count = max ( 20 , len ( g . neighbors ( v ))) anchor = selectBucket ( zeta_scores [ v ] . values (), count ) zeta_scores [ v ][ 'cutoff' ] = ( anchor - 1 ) * 1.0 / count delete_list = [] leave = 1 for s in S : for n in added [ s ]: if n not in S [ s ]: continue if n == s : continue if ( n not in zeta_scores or s not in zeta_scores [ n ]) and n in S [ s ]: S [ s ] . remove ( n ) if zeta_scores [ n ][ s ] < zeta_scores [ n ][ 'cutoff' ] or zeta_scores [ n ][ 'cutoff' ] == 0 : S [ s ] . remove ( n ) #print stay_cut_set[n][s] if len ( S [ s ]) < k : delete_list . append ( s ) leave = 0 for e in delete_list : S . pop ( e ) print 'after leave is: ' for e in S : print e , S [ e ] return leave In [197]: def expandCommunities ( S , g ): join_scores = {} for j in g . nodes (): if j not in join_scores : join_scores [ j ] = {} for i in S : score = xi ( j , S [ i ], g ) join_scores [ j ][ i ] = score for v in join_scores : count = max ( 20 , len ( g . neighbors ( v ))) anchor = selectBucket ( join_scores [ v ] . values (), count ) join_scores [ v ][ 'cutoff' ] = ( anchor - 1 ) * 1.0 / count #for e in join_scores: # print e, join_scores[e] #for e in added: # print e, added[e] nowadded = {} for i in S : nowadded [ i ] = set () for vj in added [ i ]: for uk in g . neighbors ( vj ): if uk not in join_scores or i not in join_scores [ uk ]: #print 'not in' , uk, i, uk not in join_scores, i not in join_scores[uk] continue if join_scores [ uk ][ i ] > join_scores [ uk ][ 'cutoff' ] and uk not in S [ i ]: S [ i ] . add ( uk ) nowadded [ i ] . add ( uk ) added [ i ] = nowadded [ i ] print 'after expand is: ' for e in S : print e , S [ e ] In [189]: t = {} #for e in added: # print e, added[e] initializeCommunities ( test , 2 , t ) leaveCommunities ( t , test , 2 , 0.6 ) #for e in added: # print e, added[e] S = { 0 : set ([ 0 , 1 , 3 , 6 , 7 , 8 ]), 1 : set ([ 0 , 32 , 2 , 3 , 4 , 1 , 8 , 9 ]), 2 : set ([ 2 , 4 , 5 , 7 , 31 ]), 10 : set ([ 10 , 13 , 15 ]), 11 : set ([ 11 , 12 , 15 ]), 18 : set ([ 25 , 18 , 19 , 22 ]), 20 : set ([ 20 , 21 , 22 , 23 , 24 , 25 ]), 26 : set ([ 24 , 25 , 26 , 19 , 21 ]), 27 : set ([ 27 , 29 , 31 ]), 28 : set ([ 28 , 29 , 30 ])} expandCommunities ( S , test ) for e in S : print e , S [ e ] duplicate [6, 8, 32, 3, 4, 5, 7, 9, 15, 17, 12, 14, 19, 21, 22, 24, 25, 23, 29, 30, 31] 21 after leave is: {0: set([0, 1, 3, 6, 7, 8]), 1: set([0, 32, 2, 3, 4, 1, 8, 9]), 2: set([2, 4, 5, 7, 31]), 10: set([10, 13, 15]), 11: set([11, 12, 15]), 18: set([25, 18, 19, 22]), 20: set([20, 21, 22, 23, 24, 25]), 26: set([24, 25, 26, 19, 21]), 27: set([27, 29, 31]), 28: set([28, 29, 30])} expand phrase 0 set([0, 1, 2, 3, 32, 6, 7, 8, 9, 4]) 1 set([0, 32, 2, 3, 4, 5, 6, 1, 8, 9, 7, 22, 28]) 2 set([2, 4, 5, 7, 9, 30, 31]) 27 set([27, 28, 29, 30, 31]) 20 set([18, 19, 20, 21, 22, 23, 24, 25]) 26 set([18, 19, 20, 21, 22, 24, 25, 26]) 10 set([16, 17, 10, 13, 15]) 11 set([11, 12, 14, 15, 16, 17]) 28 set([28, 29, 30, 31]) 18 set([18, 19, 21, 22, 24, 25]) In [203]: def preferredCommunities ( g , k , ovl ): S = {} initializeCommunities ( g , k , S ) print len ( S ) expand = 6 while expand : if expand <= 0 : break expand -= 1 leave = 1 leave = leaveCommunities ( S , g , k , ovl ) expandCommunities ( S , g ) print len ( S ) return S In [204]: z = preferredCommunities ( test , 2 , 0.6 ) print z . keys () 32 duplicate [6, 8, 32, 3, 4, 5, 7, 9, 15, 17, 12, 14, 19, 21, 22, 24, 25, 23, 29, 30, 31] 21 after leave is: 0 set([0, 1, 3, 6, 7, 8]) 1 set([0, 32, 2, 3, 4, 1, 8, 9]) 2 set([2, 4, 5, 7, 31]) 10 set([10, 13, 15]) 11 set([11, 12, 15]) 18 set([25, 18, 19, 22]) 20 set([20, 21, 22, 23, 24, 25]) 26 set([24, 25, 26, 19, 21]) 27 set([27, 29, 31]) 28 set([28, 29, 30]) after expand is: 0 set([0, 1, 2, 3, 32, 6, 7, 8, 9, 4]) 1 set([0, 32, 2, 3, 4, 5, 6, 1, 8, 9, 7, 22, 28]) 2 set([2, 4, 5, 7, 9, 30, 31]) 10 set([16, 17, 10, 13, 15]) 11 set([11, 12, 14, 15, 16, 17]) 18 set([18, 19, 21, 22, 24, 25]) 20 set([18, 19, 20, 21, 22, 23, 24, 25]) 26 set([18, 19, 20, 21, 22, 24, 25, 26]) 27 set([27, 28, 29, 30, 31]) 28 set([28, 29, 30, 31]) duplicate [1, 20, 26, 28] 4 after leave is: 0 set([0, 1, 2, 3, 32, 6, 7, 8, 9, 4]) 2 set([2, 4, 5, 7, 9, 31]) 10 set([17, 10, 13, 15]) 11 set([11, 12, 14, 15, 17]) 18 set([18, 19, 21, 22, 24, 25]) 27 set([27, 28, 29, 31]) after expand is: 0 set([0, 1, 2, 3, 32, 5, 6, 7, 8, 9, 22, 23, 4]) 2 set([1, 2, 4, 5, 7, 9, 31]) 10 set([10, 12, 13, 15, 17]) 11 set([2, 11, 12, 14, 15, 16, 17]) 18 set([3, 18, 19, 20, 21, 22, 23, 24, 25, 26]) 27 set([0, 27, 28, 29, 30, 31]) duplicate [2] 1 after leave is: 0 set([0, 1, 2, 3, 32, 5, 6, 7, 8, 9, 4]) 10 set([10, 13, 15, 17]) 11 set([11, 12, 14, 15, 16, 17]) 18 set([18, 19, 20, 21, 22, 24, 25, 26]) 27 set([27, 28, 29, 30, 31]) after expand is: 0 set([0, 1, 2, 3, 32, 5, 6, 7, 8, 9, 4]) 10 set([10, 13, 15, 17]) 11 set([10, 11, 12, 14, 15, 16, 17]) 18 set([1, 2, 3, 18, 19, 20, 21, 22, 23, 24, 25, 26]) 27 set([27, 28, 29, 30, 31]) duplicate [11] 1 after leave is: 0 set([0, 1, 2, 3, 32, 5, 6, 7, 8, 9, 4]) 10 set([10, 13, 15, 17]) 18 set([1, 2, 3, 18, 19, 20, 21, 22, 23, 24, 25, 26]) 27 set([27, 28, 29, 30, 31]) after expand is: 0 set([0, 1, 2, 3, 32, 5, 6, 7, 8, 9, 4]) 10 set([10, 13, 15, 17]) 18 set([0, 1, 2, 3, 4, 5, 32, 7, 14, 18, 19, 20, 21, 22, 23, 24, 25, 26]) 27 set([27, 28, 29, 30, 31]) duplicate [18] 1 after leave is: 0 set([0, 1, 2, 3, 32, 5, 6, 7, 8, 9, 4]) 10 set([10, 13, 15, 17]) 27 set([27, 28, 29, 30, 31]) after expand is: 0 set([0, 1, 2, 3, 32, 5, 6, 7, 8, 9, 4]) 10 set([10, 13, 15, 17]) 27 set([27, 28, 29, 30, 31]) duplicate [] 0 after leave is: 0 set([0, 1, 2, 3, 32, 5, 6, 7, 8, 9, 4]) 10 set([10, 13, 15, 17]) 27 set([27, 28, 29, 30, 31]) after expand is: 0 set([0, 1, 2, 3, 32, 5, 6, 7, 8, 9, 4]) 10 set([10, 13, 15, 17]) 27 set([27, 28, 29, 30, 31]) 3 [0, 10, 27] In [214]: print z {0: set([0, 1, 2, 3, 32, 5, 6, 7, 8, 9, 4]), 10: set([10, 13, 15, 17]), 27: set([27, 28, 29, 30, 31])} In [217]: colors = [ 'r' ] * len ( test . nodes ()) cmap = { 0 : 'b' , 10 : 'yellow' , 27 : 'g' } for e in z : for n in z [ e ]: colors [ n ] = cmap [ e ] print colors [&apos;b&apos;, &apos;b&apos;, &apos;b&apos;, &apos;b&apos;, &apos;b&apos;, &apos;b&apos;, &apos;b&apos;, &apos;b&apos;, &apos;b&apos;, &apos;b&apos;, &apos;yellow&apos;, &apos;r&apos;, &apos;r&apos;, &apos;yellow&apos;, &apos;r&apos;, &apos;yellow&apos;, &apos;r&apos;, &apos;yellow&apos;, &apos;r&apos;, &apos;r&apos;, &apos;r&apos;, &apos;r&apos;, &apos;r&apos;, &apos;r&apos;, &apos;r&apos;, &apos;r&apos;, &apos;r&apos;, &apos;g&apos;, &apos;g&apos;, &apos;g&apos;, &apos;g&apos;, &apos;g&apos;, &apos;b&apos;] In [238]: nx . draw_networkx ( test , node_size = 150 , node_color = colors ) In [ ]: data = np . loadtxt ( '../dataset/com-dblp.ungraph.txt' , dtype = np . int32 , delimiter = ' \\t ' ) In [52]: G = nx . Graph () G . add_edges_from ( data ) g = nx . adjacency_matrix ( G ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-52-22ddee6160e7> in <module> () 2 3 G = nx . Graph ( ) ----> 4 G . add_edges_from ( data ) 5 g = nx . adjacency_matrix ( G ) NameError : name &apos;data&apos; is not defined if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"研究","title":"FOCS总结"},{"url":"/d3js_learn_1.html","text":"流程 一、添加 svg 画布， 得到一个 选择器selection var svg = d3.select(\"body\") //选择文档中的某一元素，根据CSS规范，如 \"body\" .append(\"svg\") //添加一个svg元素, 符合html操作 .attr(\"width\", width) //设定宽度 .attr(\"height\", height); //设定高度 二、布局（转换数据） 2.1 种类 布局种类 2.2 定义布局 var mylayout = d3.layout.tree() // 例： Tree布局 2.3 设置布局的基本属性（接上一项） .size ( [ width , height ] ) // tree 只想到这个属性 2.4 用布局转换数据 var nodes = mylayout.nodes(data)[.reverse()] // reverse 逆序， data格式有讲究， json且属性名字限定 var links = mylayout.links(nodes) // 树图要生成点和边 或者 var piedata = pie(dataset); // 饼状图就不需要边了 三、数据绑定到元素（默认新建元素） 3.1 从画布选择器中，再选择后代元素 svg.select(name) // 选择第一个匹配的元素 或者 svg.selectAll(name) // 选择全部匹配的元素 3.2 加载数据（接上一项） .data ( dataset ) // 元素和数据集一对一加载 , 此时得到的叫 update 部分 或者 .datum ( oneData ) // 一个数据绑定给全部元素 3.3 对应数据来创建元素（接上一项），如果元素是现成的不需要建 .enter () // 只能接上一项 数据加载 , 此时得到的叫 enter 部分 .append ( type ) // 创建需要类型的元素 3.4 数据属性设置（例） 根据上一项的 type 和 数据， 设置需要的属性 . attr ( \"x\" , 20 ) //屏幕上的起始横坐标 . attr ( \"y\" , function ( d , i ){ //屏幕上的起始纵坐标 return i * rectHeight ; }) . attr ( \"width\" , function ( d ){ // 元素宽度（有这个属性的话） return d ; }) . attr ( \"height\" , rectHeight - 2 ) // 元素高度（有这个属性的话） . attr ( \"fill\" , \"steelblue\" ); // 颜色填充 3.5 事件设置（也是属性） . on ( \"mouseover\" , function ( d , i ){}) . on ( \"mouseout\" , function ( d , i ){}) . on ( \"click\" , function ( d , i ){}) 在该元素下继续添加其他子元素 前面3.1-3.5步骤结果保存在某变量中， 即可使用该变量继续append,设置属性和事件 四、动画效果 4.1 方法 transition() // 启动过渡效果, 其前后是图形变化前后的状态（形状、位置、颜色等等） duration() // 指定过渡的持续时间，单位为毫秒 ease() 指定过渡的方式，常用的有： linear：普通的线性变化 circle：慢慢地到达变换的最终状态 elastic：带有弹跳的到达最终状态 bounce：在最终状态处弹跳几次 delay() 指定延迟的时间，可用匿名函数function(d,i) 指定各个的延迟 4.2 创建动态效果 var transition = selection.transition() .duration(time) .attr()... // 定义变量是可能用于4.3 4.3 对子元素进行处理 如果元素中有子元素，需要一并处理。 transition.select(name).attr()","tags":"工具","title":"D3.js 学习心得一"},{"url":"/memory-movies-week1.html","text":"记忆概念 分类 working memory episodic memory semantic memory procedural memory working memory workbench(工作台) -- 维持、操作思维和意识 特点：短暂","tags":"心理学","title":"记忆与电影-第一周"},{"url":"/word2vec-2-mindmap.html","text":"译自： 原文链接 (没有找过作者， 随手就翻译了) 思维导图这一工具因其长于组织大量任务、材料信息，在头脑风暴、 计划和问题解决等领域得到广泛使用、一致好评。 对思路的可视整理有助于整个思考的过程， 并且模拟了我们人类思考时获取脑中知识的方式。 现今有很多工具可以帮助我们画出思维导图， 但还没有一个能自行生成的， \"生成\"是指从文本（语音）内容中提成。 为了做到这一点， 我花了最长的时间（至今快8个月了）， 研究如何结合文本挖掘和图论做成一个框架来生成思维导图（给定一段文本）。 当然， 第一个问题就是， 任意一段文字都不会只有那么一种可行的思维导图。 只是， 如果你要构建自己的思维导图， 有这么一个自动工具， 可能会给你更多的思路和洞见， 特别是头脑风暴时， 或帮你查缺补漏。 那我们先来看看一个思维导图的样式—— 两个关键点： 思维导图并不简单地是一棵树， 不只是递归地将主题划分成子主题。 它本质上更像图， 连接项在语义上是相关的。 正如‘夜晚'可能会让你想到‘白天'， 思维导图中， 意义相反的两个概念之间也很可能存在连接。 还有诸如使用图片强化概念等其他点。但这些并不是本文的主旨（我的设计师风格创造力糟透了）。 有备无患 ， Heres 这篇文章能帮助你熟悉构建和使用思维导图的过程。 在我上一篇博文 链接 中， 我描述了一种从文本生成Word2Vec模型的方法（使用维基的文章作为示例）。 在这里， 我将描述我使用的从 Word2Vec模型生成基本思维导图的方法。 第一步： 从文章中找出前n项 （就像我上一篇博文所说， 我只使用stemmed unigrams（一个词干的n-gram), 你可以自行采用更高阶的ngrams, 想来会更棘手（准确来说， 是当你生成n-gram的算法有效时） 这里的 n 是指思维导图中的节点数， 在我多次尝试之后， 50是个比较好的数字， 太小则信息少， 太大则噪音多。 欢迎尝试其他数字。 我使用了本文 链接 中写的 co-occurrence 方法， 列出文本的前 n 项词。 代码如下： def _get_param_matrices(vocabulary, sentence_terms): \"\"\" Returns ======= 1. Top 300(or lesser, if vocab is short) most frequent terms(list) 2. co-occurence matrix wrt the most frequent terms(dict) 3. Dict containing Pg of most-frequent terms(dict) 4. nw(no of terms affected) of each term(dict) \"\"\" #Figure out top n terms with respect to mere occurences n = min(300, len(vocabulary)) topterms = list(vocabulary.keys()) topterms.sort(key = lambda x: vocabulary[x], reverse = True) topterms = topterms[:n] #nw maps term to the number of terms it 'affects' #(sum of number of terms in all sentences it #appears in) nw = {} #Co-occurence values are wrt top terms only co_occur = {} #Initially, co-occurence matrix is empty for x in vocabulary: co_occur[x] = [0 for i in range(len(topterms))] #Iterate over list of all sentences' vocabulary dictionaries #Build the co-occurence matrix for sentence in sentence_terms: total_terms = sum(list(sentence.values())) #This list contains the indices of all terms from topterms, #that are present in this sentence top_indices = [] #Populate top_indices top_indices = [topterms.index(x) for x in sentence if x in topterms] #Update nw dict, and co-occurence matrix for term in sentence: nw[term] = nw.get(term, 0) + total_terms for index in top_indices: co_occur[term][index] += (sentence[term] * sentence[topterms[index]]) #Pg is just nw[term]/total vocabulary of text Pg = {} N = sum(list(vocabulary.values())) for x in topterms: Pg[x] = float(nw[x])/N return topterms, co_occur, Pg, nw def get_top_n_terms(vocabulary, sentence_terms, n=50): \"\"\" Returns the top 'n' terms from a block of text, in the form of a list, from most important to least. 'vocabulary' should be a dict mapping each term to the number of its occurences in the entire text. 'sentence_terms' should be an iterable of dicts, each denoting the vocabulary of the corresponding sentence. \"\"\" #First compute the matrices topterms, co_occur, Pg, nw = _get_param_matrices(vocabulary, sentence_terms) #This dict will map each term to its weightage with respect to the #document result = {} N = sum(list(vocabulary.values())) #Iterates over all terms in vocabulary for term in co_occur: term = str(term) org_term = str(term) for x in Pg: #expected_cooccur is the expected cooccurence of term with this #term, based on nw value of this and Pg value of the other expected_cooccur = nw[term] * Pg[x] #Result measures the difference(in no of terms) of expected #cooccurence and actual cooccurence result[org_term] = ((co_occur[term][topterms.index(x)] - expected_cooccur)**2/ float(expected_cooccur)) terms = list(result.keys()) terms.sort(key=lambda x: result[x], reverse=True) return terms[:n] get_top_n_terms 函数实现了这个功能， 我希望我写的 docstring 和 注释很好地解释了整个过程（结合起那篇论文）。 如果你有时间， 足够耐心， 你可以看到你Word2Vec模型里的整个词库（entire vocabulary）， 并找到你想加入到你的思维导图里的那些项。 这样做大概能得到最好的结果（就是太辛苦）。 第二步： 选定根节点 根结点是最能表达思维导图中心思想的。 相比起整个词库entire vocabulary， 选中的结点个数小上许多， 所以， 也许最好就是 手工选定根结点的项。 或者， 使用出现频率最高的（has the highest occurrence）。这一步也需要很多尝试（但数学科学能起什么作用吗） 第三步： 生成导图 这是至关重要的一步， 也是我花了最多时间的。 首先， 我需要定义一个项（term）的 情境向量（contextual vector） 假设， 本导图的根是‘电脑'， 连到另一个项‘硬件'， ‘硬件'再连‘键盘'， 那么， ‘键盘'的Word2Vec向量以 model[keyboard]的方式在Python/Gensim中获得。 定义这个向量为 $ v_{keyboard} $ 现在考虑构建过程。 因为你目前已经有了一些东西， 你再想到'键盘' 时， 其实已经处于'电脑'和 '硬件' 的情境（上下文）中。 所以你很难把 '键盘‘ 跟 '音乐‘ 联系起来（最起码不直接相关）。 可见， '键盘' 的contextual vector情境向量（定义为 $ v&#94;{'}_{keyboard} $ ) 一定会将方向偏向到 $ v 和 v_{hardware} $ (be biased in its direction towards). ---- 我们要计算 Word2Vec 模型的 cosine 相似度， 当然只跟方向有关。 从直觉上说， $ v_{hardware} 和 v&#94;{'}_{keyboard} $ 的影响应该大于的影响应该大于 v ， 也就是距离越远， 父节点的影响会越小。 为了考虑这个因素， 我再加入了一个 参数 情境递减因子 αα 。 数学表达如下： $$ v&#94;{'}_{computer} = v v&#94;{'} {hardware} = (1-\\alpha)v + \\alpha v&#94;{'} v&#94;{'} = (1-\\alpha)v_{keyboard} + \\alpha v&#94;{'}_{hardware} $$ 最后， 可以生成实际的导图了， 以下是我使用的算法（我希望行内注释能帮你理解我的工作） from scipy.spatial.distance import cosine from networkx import Graph def build_mind_map ( model , stemmer , root , nodes , alpha = 0.2 ): \"\"\" Returns the Mind-Map in the form of a NetworkX Graph instance. 'model' should be an instance of gensim.models.Word2Vec 'nodes' should be a list of terms, included in the vocabulary of 'model'. 'root' should be the node that is to be used as the root of the Mind Map graph. 'stemmer' should be an instance of StemmingHelper. \"\"\" #This will be the Mind-Map g = Graph () #Ensure that the every node is in the vocabulary of the Word2Vec #model, and that the root itself is included in the given nodes for node in nodes : if node not in model . vocab : raise ValueError ( node + \" not in model's vocabulary\" ) if root not in nodes : raise ValueError ( \"root not in nodes\" ) ##Containers for algorithm run #Initially, all nodes are unvisited unvisited_nodes = set ( nodes ) #Initially, no nodes are visited visited_nodes = set ([]) #The following will map visited node to its contextual vector visited_node_vectors = {} #Thw following will map unvisited nodes to (closest_distance, parent) #parent will obviously be a visited node node_distances = {} #Initialization with respect to root current_node = root visited_node_vectors [ root ] = model [ root ] unvisited_nodes . remove ( root ) visited_nodes . add ( root ) #Build the Mind-Map in n-1 iterations for i in range ( 1 , len ( nodes )): #For every unvisited node 'x' for x in unvisited_nodes : #Compute contextual distance between current node and x dist_from_current = cosine ( visited_node_vectors [ current_node ], model [ x ]) #Get the least contextual distance to x found until now distance = node_distances . get ( x , ( 100 , '' )) #If current node provides a shorter path to x, update x's #distance and parent information if distance [ 0 ] > dist_from_current : node_distances [ x ] = ( dist_from_current , current_node ) #Choose next 'current' as that unvisited node, which has the #lowest contextual distance from any of the visited nodes next_node = min ( unvisited_nodes , key = lambda x : node_distances [ x ][ 0 ]) ##Update all containers parent = node_distances [ next_node ][ 1 ] del node_distances [ next_node ] next_node_vect = (( 1 - alpha ) * model [ next_node ] + alpha * visited_node_vectors [ parent ]) visited_node_vectors [ next_node ] = next_node_vect unvisited_nodes . remove ( next_node ) visited_nodes . add ( next_node ) #Add the link between newly selected node and its parent(from the #visited nodes) to the NetworkX Graph instance g . add_edge ( stemmer . original_form ( parent ) . capitalize (), stemmer . original_form ( next_node ) . capitalize ()) #The new node becomes the current node for the next iteration current_node = next_node return g 备注： 我使用了 NetworkX 的简易图构建架构来完成了思维导图生成的核心任务（使之更易用于可视化）。 要计算 余弦距离， 我使用了 SciPy. 另外注意74和75行， 我使用了上篇博文所写的 StemmingHelper 类， 所以在思维导图中显示的是词干原始形式， 而不是词干。可以将StemmingHelper类直接当做参数 stemmer 传入。 所以， 如果你不需要词干处理， 那就把第4,74,75三行的代码干掉吧。 如果你仔细看过代码， 你会发现， 这看着很像 迪杰斯特拉的单点最短路径， 只是情境不同。 示例输出 原文链接 自己看。 看着不错， 和我人工画的也挺像的。 其他 还有一些可尝试的东西。 比如加入 bi-grams 和 trigrams。 我相信能让 Word2Vec 模型更强大， 能对文本做出更好的释义。 导图中仍存在多余项， 但它给出了文本的（最？）短长度（相对其他文本挖掘任务来说）， 这种关键词提取算法（我在上文提到过的论文）似乎相当不错。 这段翻译有点不确认（There are some unnecessary terms in the Mind Maps, but given the short length of the texts (compared to most text mining tasks), the Keyword extraction algorithm in the paper I mentioned before, seems really good.） 此方法可用于头脑风暴， 从你选择的一点出发， 这代码框架会给出建议的可连项， 你再做出选择， 然后又可以得到新的推荐——就有点像思维导图助手。 无论怎样， 这都是篇长博文了， 谢谢你坚持着读完全文！（翻译也一样感谢您的阅读）。","tags":"翻译","title":"基于Word2Vec生成基本的思维导图"}]}