{"pages":[{"url":"http://sndnyang.github.io/zhimind-entire-solution.html","text":"知维图 知维图 想创造交互式、 启发式教学， 升级一下MOOC。 只是大体方案， 细节和技术缺少数据、实验、条件还没法全部搞定。 源代码链接 新手重新造轮子（看不懂别人的，尴尬），差很多，甚至永远完成不了。 问题和需求 Problems and demands: 易走神：上课容易走神，人多、老师不管或管不过来； 看书（专业、技术书）畏难、敷衍、易走神，没有真正动脑 无反馈：老师讲课容易满堂灌而无效果，不能知道学生的学习效果，无法个性化教育；学生通常也不知道学习效果，会有课本和习题的巨大落差。 方式不好： 简单灌输知识、什么是对的、为什么是对的，但如何找到、想到的探索、创新思维没有融入，不利于培养优秀的思维方式 知识点管理： 看书总看前面部分，不知道知识点的掌握情况、知识点间的关联。 在线教育和线下教育很难结合，即使是SPOC，仍然面临前4点。 问题说明 前提：以上是个人估计多数甚至绝大多数人所遇到的问题， 但只是笔者我一人的体会，完全基于我个人近几年学习一塌糊涂的体验。没看过教育学书，没有教育数据，啥都没有， 不保证想法的正确性。 如果多数人上课专心、看书认真、一听就懂、一看就会，经典教材简单明了，一点就透的话，那我整个方案没有意义，可以关闭页面了。 解决方案思想 在线教育（视频和互动教材）中添加更多小问题，形式多样化（选择填空）。作用： 减少走神， 促进边看边思考 了解学习状态 对比： Udacity,Codecademy多数问题设计得太复杂，打断学习思路的连贯性。Coursera的问题又少又不太好。有示例 svm最大间隔 小问题形式多样化（选择填空），加上题量多，可以有后台数据显示学习状态——也可以把数据显示给学生，还能对比、推荐。 根据小问题、问答式或启发式的思路，重新组织教学内容。以问题引导学生思考、 探索发现， 并能在教学中强调一些思考方法、原则。 思维导图、目录列出更详细的知识点关系，避免线性知识组织， 同时可以标注知识点学习次数、状态等，可视化效果更好。 如果可行，则有好处 减少走神现象， 极小段的视频或文字后接一个小问题 问题的回答可以作为数据反馈， 直观反映了学生学习过程的专注情况及最终的学习效果。 可以和 线下课堂教育 相结合， 解放老师的授课压力，可以轻松了解学生学习状态，多出一些精力用于指导。 以自然语言处理技术作支撑，多使用略主观的填空题，可以通过回答内容，对学生学习、思维方式有更深入的了解，进而更准确的进行个性化教学 略强制、促进学生思考，真正动脑子。需要技术上避免作弊和乱填 培养、锻炼学生多种良好的学习习惯， 培养创新、探索思维， 同时应该能提高学习效率 启发方式或提问方式可以结合到习题中，既能提供必要的提示而不破坏学生的思考，又能了解学生做题式的思维过程，智能指导学生做题（技术上应该非常难）。 花费一定的成本，可以制作足够大的题库，能评估知识掌握情况。 渐进积累，既包括教学资源、数据的积累 学生看思维导图、目录学习有目标、 有全局认知，并且方便查缺补漏、复习。 体系化学习， 像种树长果实一样可以看到学习的结果，有成就感。 后续扩展，学生对知识心里有底， 有明确的学习路线，从效率上，也许能多发现几个天才。 后续扩展，公司也可以看到学生学习效果、知识水平 其中第4点，我认为如果确认没有下面的可行性问题，那么这个在线教育 能较好地和学校教育结合， 对在校学生能加入线下家长、老师的监督作用， 影响、意义大于单纯的在线教育、MOOC。 当然，对于非在校人员仍是在线教育、MOOC。 方案中可能的问题 没有实验、数据证明这种方法行之有效（我没搜过），尤其是启发式教育的教学效果。哪怕仅针对高中、大学以上逻辑性较强的数、理工科教育，也不知道能否实现启发式教育， 其他学科、更低年龄的应该更难。 目前的人工智能和自然语言处理技术上，是否支持比较智能化的问答， 进而支持更合适的启发教学。 似乎很难设计启发式问题，可能只能做到普通问答。 按某模型说的， 专家看问题往往是直觉式的，没有准确的规则，所以无法说清自己的思维方式、思考过程， 如何启发也就没有个明确的标准。 理想与现实 理想是 有一个科幻小说里完全智能的机器人 循循善诱地启发你去学习、做题，同时还起监督作用。 有任何问题（能用已知知识回答的）他都能启发你去自己发现答案。 现实中，老师没空（学生多），也不一定了解如何恰当地引导你去发现、探索； 书本、MOOC视频不会回答你任何问题， 网络和论坛能回答问题，但不会启发你； 贫困地区更缺师资。 网络和电脑手机的成本远低于教师人力资源，以后会更低。 我的方案除了未验证有效性外， 主要就是不够智能。 智能技术发展到理想水平的话，启发式教育自然可行，不需要专门的教育产品。 另外，读书无用论、屠龙术、就业等社会问题没有纳入考虑。个人也一厢情愿地认为，社会对各科优秀人才都还有很大的需求缺口，只有优秀人才不够多的问题。 技术与形式 我个人认为， 应该主要是基于文本而不是视频制作 交互式教程。 虽然嵌入问题的视频也不错，但灵活性、连贯性上没有文本中嵌问题好。 一个交互式教材 简单示例， 基于markdown格式编写， 和写书比较像， 主要是真正尝试过问题后，才能进行下一段，同时问题不能太复杂，小步思考。 svm最大间隔 对比 文本优势： 容易修改内容， 视频需要重录 制作上，视频录制确实快一点，但加入大量小巧问题后，会有更灵活的知识点衔接， 视频弄太碎也不方便吧？ 应用到习题上，更不适合视频了，为每个习题录视频不成立 文本形式更灵活多变， 可以是交互式教材， 可以是聊天机器人， 甚至可以是VR， 视频转VR有意义吗？ 视频更适合用在纯粹讲故事、介绍历史上。 完整流程 将知识点以知识导图、目录形式组织， 并与教程资源、 测试题集关联。 示例： 机器学习导图及目标 有两个思维导图和目录标签页 点可用涂色等方式来表示学习次数和掌握情况 若干点上有教程链接（制作问题，需要鼠标悬浮点上） 教程以文本编写（目前采用markdown，希望能找到更好的组织形式），显示效果： 交互式教材 svm最大间隔 聊天机器人 虚拟现实。 需要语音识别和语音生成，还应该有个虚拟人物形象生成，VR就要真正做到交互。在虚拟现实里看视频、看文字这种死的东西有什么意义。 嵌入其他特殊互动（各式各样太多） 理化生模拟实验， 在线、虚拟操作 计算机编程可以参考 tryRegex 或 Runestone Interactive python。 示例 数学、物理题过程推导（不是简单的填空、选择） 补充 个人觉得， 这个方案除了直接的教育意义外，还能促进以下发展 教育、心理学 真实的人的思考方式数据，虽然是以文字描述，需要分析，但比问卷调查靠谱 学习者思维方式分类 推荐 自然语言处理和知识图谱等聊天机器人技术，总要让学习者觉得越来越智能、像人， 而且也能有数据支撑这个研究进步 虚拟现实、语音识别、生成。 虚拟现实能有更大的应用空间，不交互的虚拟现实没有意义 人物形象更真实， 有合适的动作、表情， 而且可以修改不同的形象， 教授或明星，帅哥或美女， 应该不猥琐吧？ 语音识别、生成， 不然对话就没法进行了，虽然肯定用类字幕形式辅助。 真正的智能解题、智能提示 我不知道属于哪块AI技术~~~机器证明？知识图谱？逻辑推理？ 介绍完毕 文笔不行，请见谅。 以上内容仅是个人想法，没有实验数据作证，如果有读者有想法想讨论一二，或愿意给笔者解释下笔者想法是如何的不切实际，帮我打消这不切实际的想法，非常欢迎，非常感谢，联系方式见博客右部，微博、邮箱都有。","tags":"脑洞","title":"zhimind完整方案"},{"url":"http://sndnyang.github.io/zhimind-entire-solution-english.html","text":"知维图 知维图 to create interactive and heuristic, flexible education, to upgrade the MOOC, online education. This blog is a brief solution, lacking details and technologies description. open source repo a lot to do Problems and demands: easy to distraction: People are easy to distract when watching videos and reading textbooks. So they don't think and learn something. no feedback: Teachers and students don't know how well they learn, and there is a huge gap between knowledges and applications namely contents and quizzes. bad way: simply to teach, cram students. no ideas about how to guide students to discover and explore. knowledge manage: not visuable hard to combine online education or MOOC with traditional school education Note Maybe those problems above are my illusion. The first thing is to verify this problems. if you have any ideas , I'm looking forward to talking with you. solution ideas Embeded many tiny questions, like Udacity,Codecademy, much more questions than Coursera. Improvement: many tiny questions instead of heavy questions in teaching, not let it to break the process of learning. more forms of tiny questions. So we can get more detailed and exact datas on the status of learning process. Then we can visualize it and do many researches on them. Due to tiny questions and heuristic way, reorganizing the teaching contents. Use questions to guide students to think and discover, and we can emphasize the scientific methods and principles. use mindmap to organize the knowledge points and show the study times and master levels(how well) potential problems maybe no experiment datas to prove it's effective and efficient. we don't know the effect of heuristic education, especially by Artificial Intelligence. can Artificial Intelligence technologies(like nlp) support it and provide enough intelligence? it's much harder to create the materials and resources. Experts have no idea about how they are thinking. benifits if it works reduce the distraction. feedback datasets by answering the tiny questions. for further researches on study types and personal education. lead students to think help students to form excellent learning habits and master efficient learning methods. combine with offline school education , reduce the burden of teachers on teaching to better support students. integrate heuristic way into quiz questions. To Provide hints but not break students' thinking. it can build question banks. further ... for jobs and so on Technology Form based on text instead of video, it can have many forms. advantages easy to edit,modify more flexible for tiny question than video it can apply on quiz, problem sets. many forms like interactive textbooks, chatbots, VR(virtual reality) entire process organize knowledge points in mindmaps and tables of contents, and create links with tutorials and quizzes. Chinese example ： 机器学习导图及目标 write tutorials（in markdown，finding a better, more flexible form）： interactive textbooks svm maximum margin chatbots VR. speech recognition and speech generation, Person figure generation for interactive VR mainly to understand learners' answers and response correctly. it needs AI and natural language process. Thank you","tags":"脑洞","title":"zhimind-education-solution"},{"url":"http://sndnyang.github.io/2017-01-15_to_21-diary.html","text":"2017年1月15号日记 星期天 天气：多云 莫愁前路无知己 这次高科技创业课的选课人数太少了， 不过百人吧， 不像最开始的有上千个团队，上千人。 也许真找不到人，但自己也值得一试了。嗯， 还得先完成一篇英文的教程， 翻译哪篇好呢。 明天就回了， 希望飞机不要晚点，那么我能在19点左右到家。 今天有学到什么吗？ 没有。 想早睡吗？ 想， 但没招。 从今天、本周开始， 日记改以周为组，一周的日记写一个文件，不然太多了。","tags":"日记","title":"2017.01.15-21日记"},{"url":"http://sndnyang.github.io/2017-01-14-diary.html","text":"星期六 天气：多云 回来了，做些什么 前段时间再战雅思， 再挂~~~ 没有目标地上课没有意思了， 正好我脑子里有很多——起码有几个——想法， 可惜都是产品有余，创业不足。 有很多新的产品想法， 而且个人觉得真是学术与产业界完美结合的典范， 既是个有用的产品——不是那么有趣的学习产品，但应该有效， 又能给学术界提供数据、 研究方向。但难说适合创业。 首先， 我个人问题。 团队与领导能力上， 远的不认识，近的不好意思找， 创业团队组不起来，乱发了几家BP，基本宣告失败 个人知识与能力上， web,自然语言处理没一个掌握的，这么久了，也没好好提高， 开发的网站简陋， 一些重要想法没法实现。 知识水平不够，写教材太难了。 其次， 创意问题 想法虽然多，但没法验证， 而且并不是什么高门槛的技术产品，比如我认为 zhimind.com 启发、交互式教学是未来发展趋势，但也是MOOC的升级版，那怎么跟MOOC平台竞争教学资源呢？ 他们可以各种浪费（MOOC重复课程非常多），我还在自己慢慢地撰写呢。 义无反顾 不过，创业者总是相信自己的想法的， 哪怕我这么不相信自己能力的人， 也坚信自己的这些方案是最棒的方案（不只是简单的几个想法）。没被人意识到，只是我个人能力问题（组队、技术、知识）和投资人认识问题。投资人只投好的团队或有大量用户的产品。 所以我还是有种怀才不遇的心思， 想参加 Novoed上的高科技创业，试试组队， 另外自己有时间和心境去完成起码完整的内容了，前段时间心烦。 某种程度上说， 这事办成了， 我也就不用心烦了， 心烦不就心烦在一事无成嘛。","tags":"日记","title":"2017.01.14.日记"},{"url":"http://sndnyang.github.io/introduce-computer-networks.html","text":"简单点的开篇 大道理说不出来，少说话， 直接上问题。 为什么要有网络，想来大家都心知肚明，那为什么要学计算机网络呢？ submit 不学怎么样？ 不学计算机网络， 怎么样？ 又不是要制订通信、协议标准， 不学这套理论， 只学点socket网络编程密切相关的内容，对占了多数的程序员来说似乎也够了， 需要再去深入。 个人认为， 多数人学网络不应该只去学习个结果、现成的知识， 而是应该开动脑筋， 分析为什么这么做，尤其是对计算机网络这种标准更像工程而不是科学的知识来说， 要能体会到现行方案的优点。 我对教学内容的重新编排是希望把我认为重要的内容突出出来， 给读者一个更明确的学习指向。 怎么学 我想多数读者都知道计算机网络课程里的7层或5层网络模型，又或是TCP/IP网络模型（现在不了解不要紧）。最常见的网络课是从最底层往上讲， 但同样有一本非常有名的教材《计算机网络--自顶向下方法》则是从顶层应用层往下讲。 读者更倾向于哪种呢？ 自底向上 自顶向下 submit 自成体系 估计绝大多数读者都会选择自顶向下吧， 毕竟国内基本采用自底向上， 然后~~~讲得不怎么样甚至很烂。于是大家可能会觉得换种方式会好一点，然而相同老师的话，结果必然一样。 不过，对多数学习者而言，自底向上的学习也确实是太无趣，物理层浅尝辄止，对多数人来说没有研究价值等等。 可即使按自顶向下的讲法，把人类几十、几百专家呕心沥血精心设计出来的东西（有删减）一股脑地倒出来，读者也很难吸收理解这么多的内容。 除了一股脑地全盘接收外，读者觉得还有什么更好的方式吗？ submit 迭代 笔者尝试使用一种迭代学习的方式， 不是简单的重复学习，先枝干、后叶子地迭代扩展式方式， 希望能引导读者慢慢深入计算机网络的知识体系当中。 整个迭代计划为： 以某种网络应用（如访问网页）为例，自顶向下引导读者分析网络通讯中会遇到的问题，并设计解决方案。 对问题、方案抽象、分类、模块化，即建立层次模型。 以任意方式（顶向下或底向上）对各层扩展。 读者觉得还有什么更好的方式吗？ submit 迫不及待 那我们就开始吧， 下一篇 网络通讯过程分析","tags":"计算机网络","title":"计算机网络导论"},{"url":"http://sndnyang.github.io/database-table-of-content.html","text":"大纲目录 初版， 基础使用部分暂略， 将从理论即设计与管理部分开始。 数据库使用基础 数据库的基本概念 关系型数据库-- SQL练习 非关系型--NoSQL 数据库使用进阶 视图 触发器 数据库编程 查询处理和优化 并发控制--设计与管理 数据库设计与管理 数据库设计 需求分析 E-R模型及设计 逻辑设计 物理设计 并发控制 管理维护 实施和维护 视图 安全性 完整性 恢复技术 数据库原理 关系数据结构 关系代数 函数依赖及范式 等等 文件组织与索引 分布式数据库","tags":"数据库","title":"数据库系统总纲"},{"url":"http://sndnyang.github.io/teach_knowledge_or_teach_wisdom_and_methods.html","text":"授人以鱼不如授人以渔 这是一句耳熟能详的俗语、名言，指给送给他人其需要的既有的东西，不如传授给他人学习获取其所需要的这个东西的方法。 我并不是说这句话有错误， 一切听我慢慢道来。 吐槽 我这几天在知乎疯狂地答题。 不得不承认，就答题来说，可能百度知道更合适一点，百度知道是在\"答题\"， 不断有新问题等待回答，也不会再去回答老问题。 知乎更多是高人回答（或抖机灵、发图），普通群众围观大V、名人， 新提的问题反而缺乏关注。 所以我认真地回答了好些问题，但得到的赞远没有在高票回答里的评论多， 或者说就是基本没人看我的回答。 正题 吐槽完毕， 回归正题。 这几日多半回答 高效学习、学习方法、 拖延之类的问题， 虽然我自己拖延、 学习也各种不高效， 但感同身受， 其实深有体会， 有不少想法， 就是自己也没实践。又扯远了。 显然，很多人、无数人都面临学习方法、 记忆和拖延、纠结的问题， 我在这几日的回答中， 个人想法得到不断地深化， 终于在今晚闲逛时，决定写这么一篇。 我发现了什么？ 最开始是回答如何提高分数、 如何长时间专注。 我主要给出的是 基于费曼技巧的方案， 不管是学习知识还是错题本分析。 还有回答一个 记忆术、 联想记忆法或记忆宫殿问题， 题主知道方法，怀疑是否有效， 像我和很多人则是知道有效，但不能坚持训练。 之后是，回答 如何克服拖延、 大学。 我就明确说了， 费曼技巧好，但问题在于，没有那个自控力去执行它， 当然不是我太自信或自以为是，我真觉得多数克服拖延症的回答如果能起效，早就起效了，大多数人看那些方案也很难去执行， 继续拖延。 特别是其中一个拖延症问题， 拖延症怎么破？ 我明确了拖延主要有两点， 一是不知道做什么， 二是害怕去做。 对学习来说，主要是害怕去学习，觉得有难度， 而所有的学习方法只有在掌握之后才能提高学习效率，所以仍然要先学习 学习方法，于是仍然是畏难情绪主导，拒绝学习。 综上所述， 我就对 授人以鱼不如授人以渔 有新的看法 我的看法 授人以鱼， 排除客气、行贿等问题， 还有排除鱼太多、太重拿不动这种情况， 反正送上门来的鱼， 接受起来没有难度。 而授人以渔的问题就是， 1. 不是每个人都想钓鱼， 2. 学习打渔技巧可不是那么容易的事， 可能一直钓不上鱼，反而先饿死。 对应到学习上就是： 简单不需要动脑的被动接受（填鸭式） 没有陡峭的入门学习曲线，容易被人使用， 但会造成后期难度提高后，无法接受、厌学。 但对应的， 掌握学习方法是有很陡峭的学习曲线的， 学不会， 仍然变成被动接受， 甚至更早地厌学。 所以， 授人以鱼不如授人以渔 只是强调了结果， 成功授人以渔才是好事。 但现在几乎没有人讨论，如何授人以渔。 很多\"渔夫\"会想当然地认为，把\"钓鱼技术\"教给别人，别人就能学会、 练出来， 可掌握\"钓鱼技术\"——克服拖延、学习能力强——的人从古至今，从来都是少数。 看着历史上群星璀璨， 似乎古人个个都有超强的学习能力、从不拖延， 可如果换成人口比例， 他们拖延、没能力的比例 比现代高多了，不过是拿自己和周围的人和古代精英比较了， 古代甚至近代的识字率才多少呢。 要解决拖延、 提高学习能力， 肯定不是针对人类社会精英， 那些精英或天才虽然不至于完全没有拖延问题、个个天赋异禀，但他们已经是精英了，克服困难的意志比普通人强多了。 所以， 能否帮助更多的普通人解决拖延问题， 提高学习能力？ 让更多、越来越高比例的人拥有精英的品质呢？ 我的方案 好像又写长了。 首先是一个我的观点， 除了智商不讨论外， 绝大多数品质还是能后天培养的， 等于是养成一个优秀的习惯， 在大脑不损伤又不能\"整容\"的情况下， 最起码能提高到不错的水平。 最直接地说， 自控力是培养习惯的基础， 有自控力，想培养什么习惯都成（甚至戒毒？但大脑损伤了）。 所以， 第一要培养的就是自控力。 自控力培养 那有较强自控力的人 小时环境因素是否有什么共同点， 还是都是天生的？ 家教好的，自控力肯定不会太差， 家庭条件不好的似乎又跟性格相关，性格又跟先天或后天的环境相关， 太绕了。 总之， 我认为， 有可能的话， 自控力培养可以利用别人的监督。 TED上有相反的观点， 认为你把你的计划说出来后，更难坚持——但个人体会，该不完成的，说和不说有区别吗？ 监督 和 透露是不一样的。 监督需要监督者有一定权利对你奖励或处罚， 这就是显式的反馈。 一般来说， 找父母、 朋友监督自己。 但很多人会反感\"监督\"、\"管\"之类的字眼，因为这不自由。只能有监督， 才有压力迫使你去培养习惯， 包括自控的习惯。 而如果都讲求自由了，没有外部压力，只能靠自己， 但自己本来就没有自控、或毅力， 这就是个死循环， 内部的死循环需要借助外部事物来打破。 自控都是从小培养的， 父母看着、监督着， 奖励或惩罚。 长大后，确实很难再监督了。 这是通过 外部因素， 引入外部监督作用 ， 起码给自己一个开始， 很多人纠结、拖延着没有开始。 但学习或很多事情不是你盯着就能学会的， 它自有难度在此， 外部监督帮不上忙，反而容易引起厌学，怎么办？ 就需要内部降低难度。 方法化成内容 降低难度不是指减少内容或具体难度， 不学数学分析只学微积分的降低难度不是我所指的。 我指的是利用学习方法来降低难度，但学习 学习方法本身就有难度， 所以我的方案是—— 方法化成内容 或叫 方法内容化。 首先， 方法要训练， 所以不是所有方法都能变成内容， 比如健身方法就要练，不能直接贴肌肉嘛——蛋白粉是什么效果不知道， 又比如照相记忆， 先假设是可行的， 它的训练不怎么要动脑，不太容易造成畏难心理，但一定要练， 不然怎么内容化？ 改造基因直接达到训练后的效果？ 主要是一些方法可以化成内容， 而且我指的内容化，不是自己来内容化，而是别人提供的现成的、结合方法的内容， 自己来做内容 就是方法的练习、应用了。 联想记忆法——公司给单词编写联想记忆法， 学生直接使用， 除了基础词I, you 这种强行外， 偶尔遇到新词就可以自己编了，因为见得多了， 编一个不难， 没想法编不出来，硬背一个也不难。 还有就是像 知维图 融合的启发式教学和费曼技巧。 大概就这么多，已经太长了。","tags":"随笔","title":"授人以鱼不如授人以渔另解"},{"url":"http://sndnyang.github.io/2016-12-01-diary.html","text":"星期四 天气：多云 标题： 没有时间了， 专心点 未来一段时间真心不能再写这些了， 为英语做准备才对了。 方案很简单， 阅读、 听力、 口语、 写作各种练就是了。 新安排： 英语必须搞定，写作、口语、听力好好练","tags":"日记","title":"2016.12.01.日记"},{"url":"http://sndnyang.github.io/gewu-learning-methods-template.html","text":"说明 费曼技巧（或许有其他人提出过类似的方法，但最有名的提出者大概就是费曼了，如果爱因斯坦提出的话，那就叫爱因斯坦技巧了，爱因斯坦名气更大）在学习、自学上有一定帮助，但相对难以实践， 所以弄个粗制滥造的计算机版来辅助一下。 装点文艺 我是谁,我从哪里来,我要到哪里去。 学习时，也可以先考虑这个问题： 它是谁，它从哪里来，它要到哪里去。 它是谁 您要学习的内容是 。 submit {{它}}的性质是什么？ {{它}}属于什么？ 问题 概念 方法 定理 公理 定律 算法 其他 submit 格物君认为，大概就这些可能了吧， 或有其他情况， 欢迎补充。 接下来是 它从哪里来,它要到哪里去 {{它}}从哪儿里？ {{它}}从哪儿来或为什么要有{{它}}？ submit {{它}}为何而来？ {{它}}为何而来或为什么会有{{它}}？ submit {{它}}要去哪里？ {{它}}要去哪儿？哪里需要{{它}}？ submit 学习知识总要知道它的应用不是吗？ 不一定能应用于解决现实世界里的具体问题，但一定与其他事物存在关联——完全不存在关联的事物，看不见、听不到、摸不着、推不出来，也就不会被发现了吧。 接地气 哲学问题之后，来点接地气的问题， 其实也就是说， 我们该深入学习{{它}}的细节了。 希望通过格物君提供的思路， 让您能理清 {{它}} 的脉络， 把握它的精髓及思想， 摆脱学完就忘的尴尬局面—— 待博物君（的作者）积累足够经验后， 也许能抹平教学及练习、题目间的那道槛， 希望能减少知识理解了，题目完全不会做的情况。 不过面对 知维图同学 的竞争，到底谁更厉害、 谁更能帮助学习， 鹿死谁手 犹未可知也。 概念类 如果是 问题&概念&定义 这种， 我们套用记叙文的起因、经过、结果来进行分析， 起因和结果在上面的\"从哪里来，到哪里去\"已经给出了。 {{它}}的经过是怎么样的？ submit 过程方法类 如果是 方法&定理&公理&定律&算法 这些，都有一些过程、步骤，步骤之间有一定的逻辑、因果关系，请读者开动脑筋，真正把这过程想清楚。 注： 每一步可以猜测、尝试、错误后返工，但不能直接\"显然易得\"地给结论、引入其他东西，就是每一步一定要有理由。 举个反例（感觉教材经常这样）。 证明向量内积满足$$|\\langle \\alpha, \\beta \\rangle | \\le |\\alpha| |\\beta|$$ 某书（其他书不想找了）先证 $\\mathbf{\\beta}=\\mathbf{0}$，这正常。然后证 $\\mathbf{\\beta} \\ne \\mathbf{0}$， 一来就是，令 $$ \\xi = \\mathbf{\\alpha} - \\frac{\\langle \\mathbf{\\alpha}, \\mathbf{\\beta} \\rangle}{|\\mathbf{\\beta}|&#94;2} \\mathbf{\\beta} $$ 没头没脑地引入这么个辅助变量， 是不是很难理解？ 关键 就是 为什么 能 想到这么引入？ 怎么想到的， 这个理由起码得给出啊。 我们老师当初解释了，但好像只解释了这个和$\\beta$正交， 至于有没有讲 为什么能想到引入这个辅助变量，我忘了， 没认真听。 所以，写过程时，一定要 submit 过程学习 submit 其它类 暂时没想到还有哪些类型， 欢迎补充。 恭喜 恭喜你， 学习了{{它}}， 格物君希望通过这种方法——费曼技巧， 能使你在学海中多一些方向感。","tags":"教育","title":"格物学习法"},{"url":"http://sndnyang.github.io/2016-11-27-diary.html","text":"星期天 天气：雨转多云 标题：日记太久没写了~~~ 这段时间写 知维图教程， 明显写到疯了， 太难写、太难编了。 还在反向传播算法上卡住了， 明天一定要写完这篇，才能进入下一章节——话说每周都在最后赶 神经网络的作业~~~。 不过，这段时间也还是写了若干篇的， 只是浪费时间太多了。 最后， 今天实在是因为编写累了， 突然想到新的脑洞， 格物 学习法， 其实就是费曼技巧， 不过用计算机来实现模板化、 流水线作业而已， 以后还有扩展余地， 毕竟会越来越智能。 最后~~~我的英语啊， 不能光念两句英语就当练口语了， 要准备作文、 口语话题啊， 听力、 阅读（GRE阅读）都要练起来啊！！！！ 新安排： 英语必须搞定，写作、口语、听力好好练 zhimind学习顺序： DL->NLP->AI->MACHINE LEARNING 数学复习： 概率->线代->暂空 计算机基础： OS->DS,ALGORITHMS->COMPILER python代码： runestone 尝试调flask","tags":"日记","title":"2016.11.27.日记"},{"url":"http://sndnyang.github.io/thoughts-on-creative-thinking.html","text":"生活中从不缺少美，而是缺少发现美的眼睛——罗丹 不是我文艺范上来了， 我估计永远文艺不起来， 为什么用这句在创造性思维的文章里作开头呢？脑子想到，没办法，大脑知道我为什么想到它，但我不知道。 首先， 在我们没有充足知识时， 有没有创造性思维呢？ 我个人认为很难有。 我说的充足知识是相对的，比如小学的知识对小学或初中的问题就可以是充足的。 一个连微积分都不懂的人， 不可能创造、发现相对论。 当然，卡特或第一台蒸汽机、内燃机的发明者不一定有足够的物理学知识，但他们有足够的经验， 经验也算知识吧，所以他们有足够的知识去发明机器。像我开发 知维图 ，我既没有足够的教育学经验， 也没有足够的计算机技术——特别是人工智能和自然处理技术，web开发能力也难当大任， 假设 知维图 算创新或微创新的话，那一定是我受教育的经验充足，并且我有一段时间在分析、反思我的教育经历——没有调研，也有看过别人的学习方法，所以我的知识还算充足。 那我们有充足知识时，为什么不能创新？ 为什么没有创造性思维？ 确实， 因为他们没有在生活、学习、工作中发现问题、需求。这块确实是因为他们没有养成创造性思维的习惯， 没有\"发现生活中美\"的那双眼睛。 为什么没有培养出来， 是不是怪体制，怪应试教育？ 个人认为应试教育本应只是个结果， 即使素质教育也少不了要参加一些很困难的考试。双方在教育过程中都有问题。应试教育考试压力过重， 教学资源不足，灌输式太多，灌输式无法培养出创造性思维，但也不能说扼杀，很多在大陆接受本科及以下教育的学生出国后，也有创新成果，扼还只说是扼制，但杀应该是杀死，杀死了创造性思维后，还能救活？美国素质教育这么强，咋不上天呢， 没听说美国博士满地走、硕士不如狗、创新堆成山啊。另外，日本也是应试教育， 别跟我说日本当年那些人出的成果都是去国外才有的，大陆自己培养的人才也有很多创新成果。应试教育出创新成果比例低？算了，为什么要解释。 那还有些人发现问题、需求，也有知识，又为什么没有最终创造出来？ 被人抢先了呗——开玩笑。 主要是因为没有智慧、 毅力和钱。后两者好理解， 钱和知识类似，没知识、经验搞不出来，有些行业没钱也搞不出来。 爱因斯坦再厉害，一个人搞得出原子弹吗？没钱连原料都买不起。 主要是智慧是什么？我对此处智慧的理解是， 学习工作科研中的正确有效高效的方法、原则、思维、习惯。 科学家长期从事探索、科研、创新工作，往往会显式或潜意识里总结出一套方法、思维。 虽然没有什么仙丹能让你从此在创新上所向披靡， 甚至可能一层窗户纸就足以让最顶尖的科学家抱憾终生、无法捅破，但科学的方法、原则至少给了愚公移山的方向。 一个问题有无数种路，但不是条条大路通罗马， 就好比愚公要移山，却天天对着天空挥锄头， 永远没有成功的可能。","tags":"随笔","title":"关于创造性思维的看法"},{"url":"http://sndnyang.github.io/gradient-descent.html","text":"本文来自以下内容： Hinton coursera《neural network for machine learning》 课程主页 Andrew Ng Coursera 机器学习 stanford平台Stephen Boyd 凸优化 因为读者可能是从如下章节跳转过来的： 知维图系列-机器学习目录链接 的 线性回归基础 或 逻辑回归基础 知维图系列-深度学习目录链接 的 神经网络反向传播算法 知维图系列-最优化目录链接 所以本篇内容会与以上内容有一定重复。 如果了解、熟悉梯度下降法的读者可以不用看， 如果有兴趣一试，欢迎继续阅读。 原则上我会从什么开始？ 问题 定义 故事 历史 submit 目前遇到的问题 我们现在遇到的是什么问题呢？ 不出意外， 是已经给定了一个没有任何限制(unconstrained)的函数， 指自变量的定义域、取值范围没有额外的限制条件， 不是说$\\frac{1}{x}$里 x可以等于0。 给定了一个没有任何限制(unconstrained)的函数f(x)， 问题是如何求这个函数的最值（最大最小），如果最值存在的话。 如果函数本身不存在最值的话，可能就是有限制的最优化问题，本篇介绍的方法是 无限制最优化问题的求解方法。 为了配合讲解， 我们需要一个例子， 也就是找一个函数作 小白鼠， 让我们可以实践、学习。 这里，我们用线性回归里的损失函数 $J(x)=x&#94;2$平方函数作为例子， 实际问题中则代入实际的函数。 先来点直观的 $x&#94;2$有什么特点呢？一个开口向上的抛物线， 对吧？所以它确实存在最小值。如图： 先明确下，我写的函数是 $y=3 x&#94;2-2 x+5$。所以最小值不是在x=0。 人眼能直接估计最小值在哪个位置，准确结果是经过公式变换，可以求出来，对吧？ 你觉得计算机程序怎么做的？是用公式变换求最低点吗？ 是 否 submit 它怎么做的？ 公式变换就好比你知道你的目标地点，直接空投过去， 可现实中不一定知识目标地点，空投也有一定成本问题。 总之公式变换涉及具体问题、具体函数，不是这篇的内容。 函数求最值（极值）的最优化问题 一般都是搜索函数的解空间， 解空间是无限大的， 这就是最优化算法要解决的问题， 怎么找到目标解。 那既然是搜索， 就一定要有开始位置， 进而继续搜索空间其他位置。 那你觉得计算机程序应该怎么做、怎么开始? 不知道 非常小 非常大 从一个随机位置开始 从0开始 submit 从随机状态开始 现实中， 类似的搜索性问题多半是从随机初始状态开始的， 随机 是一种很重要、很有用、很强大的思想。 我们现在随机选择x=5这一点（实际经常用零向量、零点）， 在上图的红点X处。 搜索解空间 接下来我们就要开始继续搜索其他解了。但我们怎么选定下一个搜索的解呢？ 我们知道， 解空间其实是连续的， 但我们搜索肯定是搜索离散的若干个点，上一个点到下一个点间会有一定距离， 就像我们要往某个方向径直走出一步， 而不是陷入芝诺悖论在那里讨论无限、连续与离散的关系。 所以， 我们找的下一个点， 先要知道方向， 再要知道距离。回顾我们在上图的红点X处。 那么，我们应该怎么找下一个解的方向，来接近最低点？根据_ submit 别南辕北辙 我们能根据导数（偏导）的结果，决定方向。 如果只有一个自变量，则是求导； 多个自变量则求偏导（是这样吧？我数学没学好）， 找出下一个点所在方向。 考验读者是否了解导数值和方向的关系， 假设现在位置在x=x0点处, 求得导数值为 x'。 为了找最小值（不是最大值），我们从x0移动x'应该移动到哪里？ x0-x' x' x'*x0 x'+x0 x'-x0 submit 方向找好才有距离 x'如果为正，即x0点的切线斜率为正，x增大（x0+x')，J(x)也会增大，和我们找最小值的目的不符， 我们应该往反方向走，所以是 x0 - x'。 x'如果为负呢？ x0 - x' > x0， x增大（x0-x')，J(x)减小， 所以，没错， 就是 x0-x'。 上面两行结论很简单，但我描述时把自己绕了半天~~~还是觉得写得不好。 所以 x是当前坐标， 要减掉求导得到的结果（沿着梯度方向滚）， 才能使得J(x)变小。 接下来，我们要考虑，沿这个方向走多远了。 读者你的想法呢？ submit 距离 先深入认识问题， 找的这个\"距离\"是什么？这个解到下一个解的距离， 意味着什么？意味着\"变化\"， 自变量x的变化。 那它影响了什么？ x变化$\\Delta x$，会引起y=J(x)变化$\\Delta y$。 此处你会想到一个什么？上文刚提到 submit 变化多少合适 每个点有每个点的切线斜率， 意味着 x变化$\\Delta x$，引起y=J(x)变化的剧烈程度， 即$\\Delta y$的大小。 所以， 不同点到下一点的距离 这个变化量 跟 这点切线的斜率相关，相关有两种方案： 斜率越大， 变化的距离越大， 正比， 典型的就是线性相关。 斜率越大， 变化的距离相对越小， 反比， 典型的互为倒数。 本文介绍的梯度下降法就是使用正比 线性相关， 比如令 $\\Delta x = x'$， 我们就得到一个式子来算到下一点的距离。 OK了吗？有问题吗？ OK 没有 submit 还有问题 你能找到是哪里有问题吗？ submit 就是找边界、或叫临界状态， 看看有没有问题。 最低点就是临界状态 如果我们步子迈得太大了， 容易什么？ 容易翻过临界状态最低点， 无法继续逼近。如图: 为了能逼近最低点到一定范围（允许误差），所以要给x'乘上个小系数， 相对的， 就会慢一点，要多走几步。 如图： 这样我们就得到了到下一点的 方向（当前点的导数梯度x'的正负，视找最大或最小而定） 和 距离（$\\alpha * x') 接下来是一个方案对比，要对比什么？ submit 讨论 斜率越大， 变化的距离越大， 正比， 典型的就是线性相关。 斜率越大， 变化的距离相对越小， 反比， 典型的互为倒数。 哪个更好？如果看过 牛顿迭代法 会发现牛顿迭代法里距离跟斜率是反比关系。 而梯度下降 法采用正比。 都有道理，斜率大，J(x)变化本来就很剧烈，x也来变化很大，就像悬崖、陡坡，虽然垂直高度变化大，但水平变化很小。 斜率高的区间一般就不会很大。 反过来说， 如图： 离得很远时，斜率大，步子走得很小，很慢， 离最低点近了，反而迈开步子，一不小心不就走过头了吗？ 总之就是如何判断离目标点的距离大小， 如果能判断出来，距离远就走大步一点， 距离近，就小步一点。如此而已。 我们知道如何找下一个点了，那接下来做什么呢？ 完成了 判断什么搜索结束 不知道 submit 什么时候结束 $\\alpha$的取值太大太小都有问题，但这是工程实践上的问题，跳过。 综上所述， 我们有 $x=x-\\alpha \\frac{\\partial}{\\partial x}J(x)$， 作为每次迭代对x的更新。 迭代总要有个停。大概有以下这些方案： 迭代指定次数，取其中J(x)最小的。 两次迭代的J(x)值之差足够小——这个好像比较常用。 偏导值x'奇迹般地等于0 算法部分总结 以上就是线性回归求解的算法，即梯度下降法 大致步骤为： 已知一个待求最优（最小或最大）的一元、多元函数 给自变量选取一个随机的起始值 对自变量的各个分量求偏导 根据偏导的方向（值）来适当更新自变量 迭代3、4步直到满足你设定的收敛或其他条件 梯度下降实践 梯度下降法在更新w上可以采取不同的策略。 从之前的公式， 我们得到完整用于整个训练集的公式为（原先的2是常数，跟alpha合并即可）： $$ w_i = w_i - \\alpha \\sum&#94;m_{j=1}(h(x&#94;j)-y&#94;j)*x&#94;j_i $$ 这个式子每次都要把整个训练集X求个和， 所以叫 批量梯度下降 Batch Gradient Descent 在数据量比较大的时候就会很慢。 那相应的就是不批量策略。比如stochastic随机梯度下降也叫增量梯度下降。 简单来说， 就是不求和，不停扫描。 Repeat { for j=1 to m { $w_i = w_i - alpha * (h(x&#94;j) - y&#94;j) * x&#94;j_i$ (for all i) } } 恭喜 梯度下降法的主体内容为以上， 希望读者能理解梯度下降法的思想。 严格来说， 梯度即求导或偏导并不难理解， 主要是要准确把握 用梯度下降法迭代更新向量时 系数乘梯度 的含义。 对比同为求导（偏导）的 牛顿迭代法 有助理解。 所以，下一篇就可以是 牛顿迭代法","tags":"机器学习","title":"梯度下降法基础"},{"url":"http://sndnyang.github.io/backpropagation-nn.html","text":"本文来自以下内容： Hinton coursera《neural network for machine learning》 课程主页 Andrew Ng Coursera 机器学习 学堂在线 袁博 数据挖掘：理论与算法 原则上我会从什么开始？ 问题 定义 故事 历史 submit 目前遇到的问题 我们刚刚学习了 神经网络前向传播 , 现在有什么问题？ 权重参数向量如何更新 输出结果不对怎么办 如何初始化权重向量 submit 关键问题 这三个问题中，你觉得关键问题是啥？ 不知道的话， 逐个分析一下。 如何初始化权重矩阵（向量）？ submit 继续分析 初始化权重一般是全部置0。 第二项 输出结果不对怎么办？ 这个很明显就是要去更新 权重参数向量！ 所以 这两个选项是同一个问题。 难道本质不是同一个问题吗？ 是同一问题 否 submit 最后一项 要更新权重参数向量，您现在有想到什么方法，之前学过，也许能用？ submit 方法对吗？ 上面的空不知道读者会填成什么。 以前在 逻辑回归 使用的 梯度下降 或 牛顿迭代法 是否能直接拿来用呢？ 你觉得能直接拿来使用吗？ 可以 不行 submit 哪里不行了？ 还记得 梯度下降法 或 牛顿迭代法 的过程吗？ 求什么运算 进行迭代，迭代到什么时候 结束。 submit 详细分析 来看看式子， 逻辑回归的迭代式： $$ w = w + \\alpha \\nabla_w l(w) $$ 其中： $$ \\nabla l(w) = y&#94;T (y - h(x))\\ l(w)= y&#94;T log(h(Xw)) + (1-y&#94;T)log(1-h(Xw)) $$ 而牛顿迭代法求解一次导$J'(\\theta) = 0$时得到的极值， $$ w_{n+1} = w_n - \\frac{J'(w_n)}{J''(w_n)} $$ $J(w) = l(w) 或 =(h(x) - y)&#94;2$ 可以看到， 式子里都有 y ， 也就是数据的label， 神经网络的哪里可以跟label比较？ 输入层 隐藏层 输出层 submit 所以梯度下降和牛顿迭代直接用在神经网络上会有什么问题？ submit 那怎么办？ 总不能只管调整最后输出层的 权重向量吧？梯度下降法或牛顿迭代法应该只能调整传给输出层的权重向量~~~ 反正隐藏层没有label标签可以比较，不能直接用梯度下降法。 不能直接用，那是否应该借鉴梯度下降牛顿法的思想，调整一下后使用？ 应该试试 直接想新办法 submit 试试又不会掉块肉 创新不是无根之萍，要善于从已有方法、 相似领域方法中进行微创新，多借鉴，多模仿，多改进，多尝试。即使爱因斯坦提出相对论也是基于前人很多工作，特别是数学上的~~~ 我们来借鉴、分析下梯度下降法， 因为神经网络使用的是 sigmoid神经元，我们来看看对应的数学式。 模型： $$ \\mathcal{l}(w) = y&#94;T log(h(w x)) + (1-y&#94;T)log(1-h(w x)) \\ $$ 梯度下降法： $$ w = w + \\alpha \\nabla_w l(w) \\ \\nabla l(w) = y&#94;T (y - h(w*x)) $$ 可以总结出几个特点，请选出你认为在神经网络可能有用的特点。 求梯度即求导或偏导 迭代更新 更新系数 矩阵相乘 y-h(w*x) submit 差什么？ 有梯度（导数）， 有系数，要对权重向量（矩阵）更新，那差什么？ 首先，$\\nabla l(w) = y&#94;T (y - h(w x))$ 里有y-h(w x)， 所以，我们得找新的梯度、导数。 对了，能理解为什么要梯度、导数吧？ 能 否 submit 否定后要提方案 OK， 首先肯定不能在隐藏层用损失函数了，那需要y标签数据，而隐藏层应该输出什么，没人知道。 那我们要先在已使用的函数里找一找有没有可用的函数、方案。 毕竟没有线索去创造一个新函数。 注： 很明显我是根据BP反向传播算法的结论来猜测、反推这些理由，希望和真正创造这方法时的思维比较相近。 但实际上是否一定要梯度、 真的没有线索来发现新方法函数吗？这倒不好说，读者如果发现了新的想法那绝对是意外之喜。 又一个显然，除了输出层的损失函数， 还有神经元的模型、函数， 本节使用的是sigmoid神经元，即sigmoid函数。 sigmoid函数能影响到权重矩阵（向量）的所有值吗？ 能 否 submit 对比一下 假设第k层(k>=1)有m个神经元，下一层k+1层有n个神经元 所以从k到k+1层的权重矩阵是 维。 预览: submit 乘号请用 * 。 所以呢 第k层有m个神经元，下一层k+1层有n个神经元，即有n个sigmoid函数， 第k+1层第j个神经元记为： $$a_j&#94;{k+1}=g( \\sum_{i=1}&#94;m w_{i,n}&#94;k * a_i&#94;k)$$ k+1层的每个神经元都有m个输入向量（偏置单元bias unit先不管），所以n个sigmoid都可以对m个输入向量求偏导， 所以可以看到每条权重边都跟sigmoid有关系， 那我们就可以试用sigmoid函数来代替原先梯度下降法里的函数。 sigmoid求偏导部分暂略。 OK？ 已求出 sigmoid的导数， 调整了梯度下降法的计算式，可以直接使用了吗？ submit 还要考虑系数 还记得梯度下降用梯度和 学习速率（系数）来更新权重向量吧？ 神经网络中，这个学习速率能用常数吗？ 也许可以 不可以 submit 系数有什么问题 我不知道是否可以使用常系数（有兴趣的读者可以一试），但一定不好。 似乎显而易见，但为什么不好？ submit 以常为镜，可以非常 虽然不能直接使用常系数，但系数、学习速率背后的思想还是得借鉴一下，不能直接扔了吧？ 如 梯度下降法 里所说 这个系数、学习速率应该代表了什么？ submit 找呀找呀找系数 如图所示（暂时没有）： 梯度可以看作是概率， 学习速率或系数其实是 $\\Delta x$，所以二者相乘 = $\\Delta y$。意味着 x的变化率对y的影响。 我们已经找到了梯度， 但还差 $\\Delta x$， 不同层不同神经元不同边有不同的$\\Delta x$。 读者觉得它应该是从何而来？ submit 我跪了 扯不下去了， 赶紧结束这篇。$\\Delta x$ 可以理解成差异、 变化量等等，但终究是一个减法， $\\Delta$ 这符号一般就用来表示\"差\"。 目前， 唯一的已知的\"差\"值 就是 数据实际的标签label 和 前向传播输出的差， 实际值和预测值的差别 $y - a&#94;L$， 就叫 误差 。 所以要从输出层的这个$\\Delta x$开始，往回一层层地更新权重矩阵。 反向传播为什么叫反向，不需要我这段解释，很简单，对吗？ 对 否 submit 接下来 视频和文字各有不好，视频不好改，要重录， 文字不好做动画——特别是我不会做动画~~~ 从输出层怎么反向到上一层呢？输出层的神经元通过突触（边）影响到上一隐藏层的神经元， 所以有两个数据了。 输出层L层神经元的误差 $y - a&#94;L$ 上一层l层到输出层L层的边 的权重 $w&#94;l$ 那系数就是这两货的乘积了，那梯度、求导再结合进来， 先明显应该使用输出L层的g(x)求导——事实上从这里看不出来， 用l层的g(x)求导，然后除一下，可能更符合逻辑关系——前一层的变化通过导数、斜率影响下一层。为什么在下一段。 推广 接上面的，为什么不用l层神经元的函数，很简单，输入层到第一个隐藏层这里怎么办？ 输入层可没有这个函数~~~ 如何推广到其他层？ 其他层没有输出，不知道具体误差， 所以要利用下一层的误差， 下一层的误差则是来自下下一层，直到输出层的误差。 所以， 下面给出总结： 反向传播算法步骤 先计算输出层L层误差 $y - a&#94;L$ 第l层l(w)使用sigmoid神经元的sigmoid函数 g(x)。 系数$\\alpha$ 则是上一层的 误差 ，反向乘以边的权重 第k层的误差 上面第2步的系数 乘上第1步函数的导数。 权重向量更新是 输出$a_j&#94;l * \\delta_i&#94;{l+1}$ -- 有点乱 如图： 总结 在前一篇 神经网络前向传播 之后， 我们研究了如何更新神经网络里的权重（矩阵）。 借鉴 梯度下降法 的思想和数学式$ w = w + \\alpha \\nabla_w l(w) $进行改进， 进而得到 反向传播算法 。 实验品， submit 恭喜 不好意思， 本篇因为有点复杂， 捋顺整个逻辑比较难， 拖的时间太长， 又心情不好， 写不下去了~~中间关键步骤敷衍了事，草草结尾。 神经网络反向传播算法就是以上内容。 浅层神经网络一般讲完反向传播算法 就结束。 所以， 有兴趣的读者可以准备看 深度学习 了。 知维图-深度学习目录","tags":"机器学习","title":"神经网络反向传播"},{"url":"http://sndnyang.github.io/feed-forward-nn.html","text":"这部分非常简要地介绍下， 数学符号来自： Andrew Ng Coursera 机器学习 原则上我会从什么开始？ 问题 定义 故事 历史 submit 还记得学了什么吗？ 我们学习了 神经网络的引入 ， 回顾一下 基础的神经网络（框架）包含了 。 submit 还有呢？ 还学习了 感知机 ， 逻辑回归 ， 及 简单的神经元模型 所以，我们接下来要研究的是 。 submit 触即是边，边即是触 神经元间的突触，看成是两个点（神经元）间的边。 那就是一堆点， 然后点间拉红线， 随便画， 比如： 很明显, 神经网络的边 也可以有不同的连接方式，即神经元的输出是如何被下一个神经元利用的。 你了解、听说过几种神经网络的连接方式？请诚实作答 submit 以架构之名 不同神经网络的边连接方式 肯定是有不同的核心思想， 根据各自的核心思想、原则，建设出神经网络， 估且把神经网络的连接方式叫做神经网络的架构——要真说架构，神经元应该也考虑在内。 神经网络、 深度学习主要就是学习几种架构、实现、应用及它们的优缺点（限于个人认识水平）， 这将是以后教程的内容。本篇后续内容是介绍最基本的一种架构。 你觉得最简单的神经网络连接方式应该是什么样的？ submit 不要解释 很明显，最简单的连接方式， 就是把n个其他神经元输出给它的值当作自己的输入， 单向， 多对多关系， 如图所示（ 图片引用 ）： 但只能有两层吗？ 是 否 submit 隐藏层 输入层和输出层中间可以有多层的神经元，如图： 把这些中间层叫做隐藏（隐含？）层， hidden layers。 前向传播feed-forward，也可以叫前馈神经网络。 就是逐层计算各个神经元层的输出，传给下一层作输入。 具体神经元如何算输出值，则根据神经元采用的模型而定， 比如我们之前 常见简单神经元模型 里学习的五个（3个？）模型。 一般使用前馈神经网络解决分类问题时， 用 sigmoid神经元，即逻辑回归模型的比较多。 接下来做什么？ 总结 实践 数学化 应用 给定义 结束 submit 前向传播 总结前面的内容， 前向传播feed-forward，也可以叫前馈神经网络的组成为： 神经元——逻辑或sigmoid神经元 架构——输入层和输出层，中间的有n (n>=0)层神经元，叫隐藏（隐含）层。 比如： 接下来做什么？ 总结 实践 数学化 应用 给定义 结束 submit 数学符号 先回顾 逻辑或sigmoid神经元的模型：$$h(x) = g(w&#94;Tx) = \\frac{1}{1+e&#94;{-w&#94;Tx}}$$ w代表权重向量 weight vector, x 代表输入变量， y代表输入变量的标签label即实际结果， h(x)就是我们的模型对x的预测结果。 接下来我们要对 神经网络进行数学符号建模。还是两部分： 神经元（点） 突触（边），即权重 你觉得神经元建模还有什么元素要符号化？ submit 神经元符号化 随便用个字母a表示， 第i层第j个神经元的输出 就记为 $a_j&#94;i$ 注意： $n \\times 1$维的输入向量既可能直接作为有n个输入单元的输入层（第一层），也可能只是第一层各神经元的输入向量。 Ng的课程使用的是前者。 还记得线性模型$ w&#94;Tx=w*x+b$吧，偏置项bias(b)可以被记作 $w_0$即每层的第0个神经元$a_0&#94;i$，偏置神经元。 权重建模 单个神经元的权重是 w向量(Coursera 使用的是 $\\theta$)， 我们知道神经网络里的权重是层与层之间的多对多关系。 所以是上一层的m个神经元与下一层n个神经元形成的 $m \\times n$维矩阵。 记 $w&#94;k$ 是第k层到第k+1层的 权重矩阵 。 其中，$w_{i,j}&#94;k$是第k层的第j个神经元 到 第k+1层第i个神经元的权重， 指第k+1层第i个神经元接收第k层第j个神经元的权重， 而不是i号传给j号的权重。 一个小练习 现在要计算第2层第1个神经元$a_1&#94;2$的输出，第一层是输入单元$x=(x_0,x_1,x_2)$,映射函数g(x)。 A. $a_1&#94;2=g(w_{1,0}&#94;1 x_0 + w_{1,1}&#94;1 x_1 + w_{1,2}&#94;1 x_2)$ B. $a_1&#94;2=g(w_{1,0}&#94;2 x_0 + w_{1,0}&#94;2 x_1 + w_{1,2}&#94;2 x_2)$ C. $a_1&#94;2=g(w_{0,1}&#94;1 x_0 + w_{1,1}&#94;1 x_1 + w_{2,1}&#94;1 x_2)$ D. $a_1&#94;2=g(w_{0,1}&#94;2 x_0 + w_{1,1}&#94;2 x_1 + w_{1,2}&#94;2 x_2)$ E. $a_1&#94;2=g(w_{0,0}&#94;1 x_0 + w_{1,1}&#94;1 x_1 + w_{1,2}&#94;1 x_2)$ F. $a_1&#94;2=g(w_{0,0}&#94;2 x_0 + w_{1,1}&#94;2 x_1 + w_{2,2}&#94;2 x_2)$ 以上哪式正确？ submit 输入输出建模 和逻辑回归一样 输入向量即 $x = (x_0, ..., x_n)$，其中 $x_0$对应偏置项$w_0$ 输出单元，即最后一层神经元的输出。 OK? 看完上面内容，尤其是那个小小的练习，对神经网络的数学符号化搞清楚了吗？ 清楚了 并没有 submit 编程实践 纸上得来终觉浅，绝知此事要躬行。 所以我制作了一个ipython notebook的练习， 前向神经网络notebook#todo 有ipython notebook的朋友知道如何使用。没有ipython notebook的朋友， 如果想了解python，可以自己下载python，安装ipython\\jupyter包。 觉得配置麻烦（其实不麻烦）的，可以参考 虚拟机 恭喜 前向传播是神经网络的基础， 毕竟一个神经元将信号传给下一个神经元是再直观简单的事。 感想： 复杂的知识点很难讲清， 简单的知识点很难讲通， 到处都是显然易见， 很难把里面的逻辑线捋清楚。 目标是让读者自己发明一遍，但这篇明显差得太远。 下一篇 反向传播算法","tags":"机器学习","title":"神经网络前向传播"},{"url":"http://sndnyang.github.io/udacity-debug-basic-concepts.html","text":"基础概念集合 最后来点概念了。一堆英语名词解释，让我们尽情地晕吧。无所谓。 失败(failure)——用户在程序外看到的错误。 错误(error)——偏离了正确，不再正确的都是错误。晕。 缺陷(defect)——代码中的error，将有效的状态变成错误的状态，进而可能引起程序的失败(failure)。 Fault——也就是缺陷(defect)，不过它还用于描述数据data的问题。 Bug——缺陷的最著名的说法，等价。指bug本不属于代码的一部分，自己爬进来的。但Zeller认为bug就是程序员写出来的，所以他更偏好defect这个词。 Infection(不清楚有没有对应的术语）——程序状态中的错误，由带缺陷的语句引入。 上面几个名词间还有一些比较绕的关系，什么不是每个什么都是什么什么的。 再提一个后续单元经常用到的概念。 Cause effect chain，我只能想到翻译成因果链。因果链其实就是 defect (代码中的缺陷) -- infection (运行时的缺陷) -- progate(扩散) -- failure (失败)。 调试的目的就是 发现因果链，并打破它（从源头解决）。 其他概念 科学方法 清晰调试 冥界、仙界调试指南","tags":"CS","title":"Udacity调试课之基础概念集合"},{"url":"http://sndnyang.github.io/Explicit-Debugging.html","text":"显式调试 显式调试(Explicit Debugging，自译不标准)的内容主要有两点： 记录 描述 记录 所谓记录，即及时记录重要调试信息。如程序输入，预期结果，实际结果, 尤其在使用科学方法提出的假说、 假想的解决方案。 也许读者习惯了记在脑中，可工作中干扰过多，记忆的时长也有限，一旦遗忘，大侠又要重新来过了，简直痛彻心扉。 只要下班之前，将信息记录在案，第二天上班时，就可以以清醒的大脑再战江湖，何乐而不为？ 描述 描述、阐述是指向某人（甚至某只泰迪熊. 玩具鸭）描述自己遇到的问题。 我们知道，其实大脑同时在整理思路、理清脉络，使之更加结构化，于是可能说着说着，自己突然霍然开朗，手舞足蹈地离开，留下满头雾水的同伴， 所以才要换作一只泰迪熊. 玩具鸭 :) 恭喜 恭喜您看完全篇。 本来打算写 知维教程的， 但感觉编不出问题， 因为几个步骤都是语言描述的， 很难引导、判断回答。","tags":"CS","title":"Udacity调试课笔记之显式调试"},{"url":"http://sndnyang.github.io/scientific-methods.html","text":"科学方法 科学方法其实一个流程图就列完了。如图（呃，直接从视频中截了图片过来）： 用段流程化的文字来描述就是： 根据程序的代码. 运行情况. 失败情况. 观察结果等等因素，提出一个假说（hypothesis），也就是上一部分，准备工作中的第3条，可能的bug解决方案。转步骤2. 预测结果，记录，并进行实验，再记录观察结果，比对。转步骤3。 如果比对结果一致，表示实验结果支持提出的假说，可以对假说进行改进，再转步骤2（图片中是转步骤1）；否则表示假说不符，不正确，拒绝掉，转步骤1，提出新假说。 直到假说不能再改进，且所有实验（所有测试）结果都符合，则转入步骤4. 假说能解释并预测实验（测试）结果，所以变成一个学说，调试里就是诊断结果（诊断报告）。OVER。 这个过程很简单，看过一遍后，多数人都能理解并记住。难点就在于应用上。视频里，Zeller教授说，对于经验丰富的程序员来说，如果能在5分钟内解决问题，确实可以不用这么系统或者叫死板地使用科学方法。 但对于新手、 生手来说，需要强制地使用这么一个方法、过程。根据一万小时理论，不断地刻意练习，这样才会进步，真正地向高手转变。 例外，只在于当看到程序出错，甚至运行程序的那一刹那，就能想到bug所在，这种情况下自然是直接修改代码即可。 但只要想法不正确，或10秒内没想到问题所在，1分钟内无法解决bug，就要强制地使用科学方法。当成长为专家. 高手之后，科学方法已经成为习惯，自然能掌握好使用它的时机。 恭喜 恭喜你看完科学方法这篇， 本来打算写 知维教程的， 但感觉编不出问题， 因为几个步骤都是语言描述的， 很难引导、判断回答。 下一篇 显式调试","tags":"CS","title":"Udacity调试课笔记之科学方法"},{"url":"http://sndnyang.github.io/three-elememts-in-machine-learning.html","text":"其实我是拒绝的 本文来自以下内容： 李航《统计学习方法》 看过的人不用再看这篇， 全是概念而已。 接上一篇 机器学习的主要类型 我是拒绝的，因为这三个要素写起来，跟照抄有什么区别， 怎么编写才合理，真是头痛。 简单来说，下面这问题怎么回答~~~ 原则上应该——我规定——从什么开始？ 问题 定义 故事 历史 submit 哪有问题？ 你说这对应什么问题？说不出来啊，更多是对多种机器学习方法进行归纳总结后，提出它们都有模型、策略、算法。 所以，应该是直接上定义，让大家牢记于心。即： $$机器学习方法=模型+策略+算法$$ 简单的定义也比较好记~~~太复杂、大段文本就难记了，不过话多倒是可以学着自己总结， 这种定义，要学过很多种方法后才能总结出来。 那么，是先学定义，指导后面具体内容学习好呢，还是先学具体方法，再总结定义、特点好？ 先定义总结后内容 先内容后定义总结 submit 很明显，两种都好也都不好，有多种方案、多个方法时， 两种顺序都有好处，也都有缺点。除非是仅有一个方案，那不需要总结多种东西的特点。 反正， 具体的模型、策略、算法后面都会讲到， 没有条件全部讲完再来总结，或先讲一两个再来总结？ 那反映到目录上就会比较乱。所以采取先定义总结，再来指导具体内容。 牢记什么？ submit 模型是什么 模型就是所要学习的条件概率分布或决策函数， 即 y = f(x) 或 P = P(Y|X) 我们知道函数有 系数（参数） 和 未知数（变量）， 变量对应的是作为输入的数据， 所以机器学习主要是要求出函数（条件概率分布）的所有参数， 一般写成一个参数向量。 那么参数向量每个分量都有个\"定义域\"， n维向量取值于n维欧氏空间， 那这n维的定义域就称为 参数空间 （parameter space)。 所有可能的决策函数或条件概率分布就组成了 假设空间 （hypothesis space) 都是一些纯粹的概念， 会不会被人玩出花， 不知道。 下一步要做什么？ 有了假设空间， 我们要找目标（最好的）模型， 需要什么？ 评估模型好坏的准则 选择模型的方法 假设空间降维的方法 数据 submit 评估模型好坏 我们当然要度量模型在数据集上 预测的准确度或错误程度， 称作 损失函数 (loss function) 或 代价 (cost function). 模型f对给定输入X，输出预测值f(X)， 监督学习里会有真实值或label Y, 所以把损失函数记作 L(Y,f(X))。 度量错误程度方案很简单， 比如说 错多少个是多少的 0-1损失函数：$$L(Y,f(X))=\\begin{cases} 1& {Y \\ne f(X)}\\ 0& {Y = f(X)} \\end{cases}$$ 错误距离多远就是多少的 绝对损失函数：$$L(Y,f(X))=|Y-f(X)|$$ 这两种是基本的，但毫无疑问总有点不足。 针对绝对损失函数， 有两个问题，一是 不好求导 ，求导就变常数了， 二是希望让错误距离影响更大一点， 线性条件下，距离差4个单位是差2个单位的2倍，但这不够，再大一点。 所以我们怎么做？ submit 平方损失函数 这个是最常用的一个损失函数， 平方损失函数 quadratic loss function。 $$L(Y,f(X))=(Y-f(X))&#94;2$$ 线性回归等基于距离的方法使用的一般都是这个 平方损失函数。 那么平方损失函数能几乎没有问题地使用到其他问题上吗？ 能 否 不知道 submit 试试 我们知道，线性回归是解决回归问题方法， 机器学习还有一类可能更常见的问题是 问题 submit 分类问题怎么样？ 分类问题能用平方损失函数$L(Y,f(X))=(Y-f(X))&#94;2$吗？ 我们要先来看看 Y 和 f(X)是什么。 Y是X对应的label，取值是什么？ submit 分类的标签 分类问题有分两类或多类， 以后可能会讲到很多机器学习分类时是把多类转成多个两类问题处理——但也有可能没了。 那拿分两类来说， 前一篇 机器学习的主要类型 里写或没写。 习惯上， 我们把要预测的两个类别记为 0和1 或者 -1和1， 只有AB两类，就 是A和不是A（即B）两种。 这就是Y 的取值。 那预测值f(X)是什么？ submit 预测输出形式 人脑怎么判断分类的， 科学家还是\"sorry, i don't know\"， 很多时候我们胸有成竹，但也经常说\"这个也许、大概、有可能、或许是XXX\"， 比如\"看这天，可能下雨啊\"。 所以，我们可以勉强把分类和数学的一门 关联起来。 submit 分类是个概率问题 显然， 人作分类判断时，也是一定的概率概念， 虽然没那么数学、数字， 而且和计算机、机器学习里分类的概率 似乎也差了很多。 尤其是条件概率引出的贝叶斯推导， 一定程度上和人脑的习惯是不同的。这是概率课内容，不深究。 所以 f(X)一般来说是个概率值， 当然是不是根据人脑\"可能、大概\"决定f(X)要用概率值的， 多半不是， 应该还是顺理成章的多。 Y是标签， 为了概率，取0或1， f(X)是概率值。 那`L(Y,f(X))=(Y-f(X))&#94;2`有问题吗? 有 没有 submit 为什么？有什么问题 考虑到它是概率值， 概率值可能有什么问题? submit 怎么办？ 概率经常就很可能遇到这种情况， 联合概率多个事件概率一乘， what， 接近0了，对吧？ 那这种情况怎么办？ submit 对数(似然)损失函数 $$ L(Y, P(Y|X)) = -log P(Y|X)$$ 这样几乎就能避免概率、联合概率值时的问题。 损失函数总结 所以常见的损失函数就是以上四种： 0-1损失函数：$$L(Y,f(X))=\\begin{cases} 1& {Y \\ne f(X)}\\ 0& {Y = f(X)} \\end{cases}$$ 绝对损失函数：$$L(Y,f(X))=|Y-f(X)|$$ 平方损失函数: quadratic loss function$$L(Y,f(X))=(Y-f(X))&#94;2$$ 对数(似然)损失函数$$ L(Y, P(Y|X)) = -log P(Y|X)$$ 还有一些其他损失函数， 如 softmax回归 中使用的损失函数 交叉熵Cross-entropy 损失函数， 但不在本文范围内。 我们找到的模型越好，意味着损失函数值越 submit 风险函数 以下照抄《统计学习方法》 模型的输入输出（X，Y）遵循联合分布P(X, Y)。所以损失函数的期望是： $$R_{exp}(f) = \\int_{X \\times Y}L(y, f(x))P(x,y)dxdy$$ 上式是理论上模型f(X)善于联合分布P(X,Y)的平均意义下的损失， 称之为风险函数risk function 或 期望损失 expected loss 所以，我们的目标找最优的模型就得到个比较明确的定义——选择期望风险损失最小的模型，进而引出学习策略。 策略 显然， 期望损失 是整个数据集的总的损失， 不好直接拿总数，对大数据集太不公平， 要拿人均、平均损失来比较， 即 经验风险 empirical risk , 记作$R_{emp}$ $$R_{emp}(f) = \\frac{1}{N}\\sum_{i=1}&#94;N L(y_i, f(x_i))$$ 策略一就是求上式最小化的 经验风险最小化 经验风险最小化 $$\\min_{f \\in F} \\frac{1}{N}\\sum_{i=1}&#94;N L(y_i, f(x_i))$$ 其中，F是假设空间。 但是当样本容量很小时， 经验风险最小化学习很容易产生 过拟合#todo 现象。 简单说就是用 过于复杂的模型 在训练集上达到非常好的拟合效果，但测试集就跪了。 为了尽量防止使用过于复杂的模型，我们可以做什么？ submit 结构风险 那就是要考虑模型复杂度，要把它也算到损失里。 估计因为模型复杂情况就叫模型的结构， 所以我们有 结构风险 structural risk ， 求这个的最小化， 即： $$\\min_{f \\in F} \\frac{1}{N}\\sum_{i=1}&#94;N L(y_i, f(x_i)) + \\lambda J(f)$$ $J(f)$就代表模型复杂度，称之为 正则化项(regularizer) 或 罚项(penalty term)——具体怎么算得具体问题具体分析. 策略总结 以上就是我们的两种策略： 经验风险最优化$$\\min_{f \\in F} \\frac{1}{N}\\sum_{i=1}&#94;N L(y_i, f(x_i))$$ 结构风险最优化$$\\min_{f \\in F} \\frac{1}{N}\\sum_{i=1}&#94;N L(y_i, f(x_i)) + \\lambda J(f)$$ 所以， 机器学习问题就变成了二选一策略的最优化问题。 我看的是李航《统计学习方法》，原句是监督学习问题， 无监督、强化问题 应该也差不多， 估计主要是具体函数会有区别。 算法 没什么好说的， 因为不可能在这里就提供具体的算法方案。 机器学习问题一般会归结为最优化问题，使用最优化的算法， 最优化也是一门课程~~~还有数值计算等等。 一般常见的最优化算法有： 梯度下降法 ， 目前可以在 线性回归基础 看到一点梯度下降的内容。 牛顿迭代法 不知道了 8-) 恭喜 机器学习三要素涉及的 模型、 策略、 算法 就这么过了一遍概念， 接近抄了一遍书， 李航《统计学习方法》珠玉在前， 我抄一遍， 这效果确实不太好， 但还是希望读者能有所收获。 我一开头就说了， 熟悉这三要素概念的就不用看这篇。","tags":"机器学习","title":"机器学习三要素"},{"url":"http://sndnyang.github.io/udacity-debug-debugger-work-methods.html","text":"默认的开头 本文来自以下内容： udacity调试课 原则上我会从什么开始？ 问题 定义 故事 历史 submit 身临其境. 痛哭流涕 我想， 没有哪个程序员没有过漫长而又黑暗的调试经历吧？一说到调试， 必然都是痛苦的表情。 有 没有 submit 调试的基础知识 回顾自己的调试经历，是否有种调试调到海枯石烂. 天荒地老，山无棱. 天地合，乃敢调出来的感觉，是不是想着和bug同归于尽了？:,'( 来看一份地府调试指南说明书，看看你熟练掌握了其中几条呢。 地府调试指南： 处处print。想看谁就看谁，想哪儿看就哪儿看， 哪里不通，p(rint)哪里。 纯靠蒙、巧合式编程。哥是无敌幸运星，bug一向随意改改就搞定，嗯，也经常改到面目全非也没搞定。 从不备份。什么？难道你没听说过破釜沉舟. 刻舟求剑. 背水一战？给自己留退路的人，怎么能勇往直前！ 程序无师自通。为什么要理解程序的目的、意图？看注释. 代码很晕的好吗？！ >:( 神来之笔。兵来将挡，水来土掩。大不了，哥用一堆if，也要跟你卯上了！脚痛医脚，头痛砍头，这么浅显的道理，难道你不懂吗？ 亲爱的读者，你中了几条呢？ submit 第5条解释 第5条的神来之笔何意？ 我忘记英文原文了，反正不好直译。 举例，你写了个求平方根的函数，输入4应返回2，结果却返回了1.2，甚至直接崩掉。调试了半天，仍未解决，一怒之下，在函数开头来了句\"神来之笔\">:(： if x == 4 then return 2; 了解啥叫\"神来之笔\"了吗？ OK 不OK submit 继续解释 上面这个例子过于极端，只是我也想不出来靠谱的例子。 神来之笔的实际含义就是 面向 症状 而不是 问题 来进行修复，只要症状解决. 不再出现（出错），也就不再管bug是否还在，即所谓治标不治本。希望大家能理解。 显然，上面的地府调试指南会让你调试时如坠地狱， 读者要引以为戒。 现在让我们速速到仙界一游（我喜欢仙界不喜欢天堂）， 看看仙界调试指南是什么样的。 我会直接告诉你吗？ 会 不会 submit 怎么直接会教给你！ 神仙的书可是天书， 岂是你说看就看的？ 我们针对上面地府调试指南， 一条条地汲取教训， 总结仙界调试指南吧！ 因为一些原因，我们倒过来。 神来之笔。兵来将挡，水来土掩。大不了，哥用一堆if，也要跟你卯上了！脚痛医脚，头痛砍头，这么浅显的道理，难道你不懂吗？ 说过了， 这条是指 有些人调试只修复了表面的 症状 ，而不是实际的 问题 所以我们调试时，要解决 ，而不是 。 submit 意味着什么？ 我们要面向 问题 调试， 意味着什么？ 意味着要真正找到问题所在， 意味着你对程序代码有足够的认识。 针对第四条： 程序无师自通。为什么要理解程序的目的、意图？看注释. 代码很晕的好吗？！ 我们就该 。 submit 有备无患 明确要修复程序里的问题， 理解了程序， 开始动手调试， 好不容易改了大半， 眼看胜利在望， 突然——发现你改错了，可你从不备份，退不回去了， 悲剧啊 :@！ 从不备份。什么？难道你没听说过破釜沉舟. 刻舟求剑. 背水一战？给自己留退路的人，怎么能勇往直前！ 所以，调试、编码过程中，要记得 。 submit 真正的问题 目标有了（修复问题）、 准备有了（先理解程序）、 工具有了（备份或版本控制）， 但我们需要的是 方法 啊！ 方法 啊！ 谁不想能真正修复问题、bug呢？找不到啊！ 谁不愿意先理解程序呢？ 看不懂代码啊！ 谁会拒绝用工具呢？ 呃，这个纯粹是懒或没有拥抱变化，跟不上变化 ,:( 归根结底， 真正难点还是以下两条所显示的调试的无奈 :,'( 处处print。想看谁就看谁，想哪儿看就哪儿看， 哪里不通，p(rint)哪里。 纯靠蒙、巧合式编程。哥是无敌幸运星，bug一向随意改改就搞定，嗯，也经常改到面目全非也没搞定。 找不到bug， 没有头绪，就只能像个无头苍蝇一样乱飞了。 你说是也不是？ 是 不是 submit 不专业到专业 没有头绪就瞎试吗？ NO，这就显得不专业了。专业人士一定会有自己一套流程，可以有条不紊、安步当车地处理问题。就好比做好计划和规划。 所以我们要掌握一种 系统方法 ， 不只能用于调试， 还能应用到方方面面， 成为一个了无生趣的人，开玩笑，成为一个办事有条不紊、胸有成竹的人。 仙界调试指南总结 根据地府调试指南， 我们反向推导出仙界调试指南有： 解决问题。解决bug，而不只是bug引起的症状。针对魔鬼法则5 理解程序。理解程序的意图，分析输入和期望输出。针对魔鬼法则4 版本控制. 备份。针对魔鬼法则3 使用系统的调试方法。针对魔鬼法则1和2 所以，作为一个专业人士，在我们磨刀霍霍向bug时，要做到以下几点： 理解问题。理解程序出了什么问题。 理解程序目的。程序原本是打算做什么，结果应该是什么。 预测下如何解决问题，而不是症状。也就是提出个解决问题的可能方案。 第3点将是下一篇 科学方法 的前提。 好处 坚持使用仙界调试指南，好处大大地有，它是你变得职业的必经之路。不强迫自己养成良好的专业习惯，如何称得上专业人士？ 王晶在《中国电影人物访谈》（是叫这个吧）里，从编剧的角度说过，什么叫专业。专业就是，在你刚刚失恋之后，让你写一个爱情喜剧，男女主角幸福生活在一起的剧本，也要高质量、不受情绪影响地写出来，这就叫专业。所以他说，靠直觉. 灵感的都不叫专业。虽然他现在拍的喜剧电影也是越来越不靠谱了。 恭喜 恭喜您学完本章节内容， 建议您简单了解一些 调试的基本概念 下一篇 科学方法","tags":"CS","title":"Udacity调试课笔记之调试者的工作方式"},{"url":"http://sndnyang.github.io/thoughts-on-education-hobby-career.html","text":"没标题的必要 不知道自己是否已经写过了—— 讨论教育、在线教育， 或学习兴趣爱好， 天赋等话题时， 总有教育应该培养天赋或兴趣所在的观点， 然后就对应试教育大加鞭挞，理由就是应试教育扼杀天赋等老生常谈的内容。 不会为极端的应试教育作辩护， 只想反驳上面观点的一些漏洞。 兴趣不一定真实 首先， 表现出来的兴趣、爱好不一定是真实的。小孩兴趣多变是再明显不过的了， 小孩兴趣多变是再明显不过的了， 今天喜欢钢琴， 明天改玩绘画， 上午还在唱歌，下午就改去踢球， 你说他到底对什么感兴趣呢？ 难道朗朗从小一接触钢琴就情不自禁地爱上了钢琴？不需要家长、老师的督促就自觉练习？从来没有过放弃的想法？ 就算这种儿童是存在的， 你是？ 你子女是？ 长这么大， 见过几个呢？ 那成年人的兴趣总是真实可靠的吧？ 我明确回答， NO！看科普、非教材书籍时，觉得很美好，非常感兴趣，可你真正拿起专业书籍学习， 进行专业训练时， 你就发现没兴趣了。 别说数学、物理基础学科，很多人觉得对经济、金融很感兴趣，其实只是对钱有兴趣，对这学科知识并没有想像中的热爱。 就算是歌手、演员这种没人拒绝的行业，可真学习、训练起来，这兴趣就陡然下降了。 当然， 真正感兴趣、热爱一个行业的人非常不在少数， \"不疯魔不成活\"的人有，但相对总人数来说，比例可以忽略不计。 教育针对大众， 不可能每个学生都一定会对某个行业充满热爱， 绝大多数人并不知道自己喜欢什么， 有条件可以去试，但王思聪最有条件， 可他热爱什么了？ 天赋不一定足够 这也是一大误区，找了别人的一句话\"围绕着自己的特长与天赋去选择发展方向，在特定领域做到业内一流。\" \"围绕着自己的特长与天赋去选择\" 这话没错， 我承认， 虽然这事很难找， 关键是后面半句\"在特定领域做到业内一流\"， 跟前半句没什么关系。 首先，不怎么理会天赋论的人肯定不会说前半句， 不讨论。 认可天赋论的是大多数，但一说到教育就会说\"教育要找天赋所在，成为一流\"， 难道没问题吗？别说全世界、全国所有人， 光一年高考就有600万考生（算720万）， 这世界总共才有多少行业、 领域？ 以前说360行， 现在3万6千行，那平均下来，每行就有200人， 又要考虑很多人能力强，好几个行业、领域通吃。所以如果论天赋，多数人在所有行业都算中下水平。 自己天赋不足， 教育怎么让你在天赋、特长的领域做到业内一流？ 天赋和兴趣 也不是说 没天赋就混日子，发现天赋本来就很难。 我认为\"在特定领域做到业内优秀\"才是正确的， 简单例子， 你在班上不一定是前10名或前10%，但你可以做到90分或80或最起码及格。 前10是跟别人比， 优秀是跟自己比。 别说什么挂科名额，那是针对挂科人数太多时要放一些人不放科设置的， 个个都实打实的80分90分，会强行把学生从80分拉到不及格？ 这样说有点从反鸡汤回归到鸡汤， 跟自己比进步是比较鸡汤。 另外， 业内一流和业内优秀 也差不了太多， 一流可能是相对二流三流顶尖而言， 那优秀是不是相对良好、普通而言呢？ 这语义主要看个人怎么想了。 反正我就觉得，即使天赋有限也可以在领域达到一定水准， 除了运动天赋， 智力上能对一个领域感兴趣， 有条件接受恰当地训练，成一个优秀的领域人士是做得到的。 牛顿时期微积分就是智力之颠， 现在大学老师讲得好一点，普通学生都能学会。 也许可以引用畅销书《异数》提过的， 若干年前被认为非常复杂、难以弹奏的曲子，现在正常音乐系学生都能演奏。 你说上面两个例子可能只是长时间人类智力提升的结果， 那说爱因斯坦的相对论，够近了吧， 不过100年，当年也是难倒一群人的，现在不照样很多人能看懂了吗？ 个人观点，不一定正确，也许真是智力提升的结果，那又如何，抱头痛哭？为自己的\"低智\"痛哭？ 大众教育要保证基本知识素养 中国现在是搞大众教育， 相对一视同仁， 问题主要是全国经济条件不同， 有些地方非常好， 有些地方连义务教育都困难， 二来也是资源不足， 比较明显的应试教育。 大众教育是要保证民众基本知识素养， 九年义务教育， 强制要求若干学科达到某个知识水平。 你天赋在其他领域，不在数学物理， 就不学数学物理了吗？ 又不是弱智和精神病。 放羊教育想不学就不学， 那多数人就真的都不用学习了。","tags":"随笔","title":"对教育、爱好、事业之间关系的看法"},{"url":"http://sndnyang.github.io/models-of-neurons.html","text":"本文来自以下内容： Hinton coursera《neural network for machine learning》 课程主页 第一周第三个视频 在下不才，专业水平、 写作水平都很次，若有意见或建议， 欢迎通过下方微博或邮箱联系。 原则上应该——我规定——从什么开始？ 问题 定义 故事 历史 submit 问题是什么？能吃吗？ 我们已经学习过两类典型的神经元模型， 感知机 和 逻辑回归 。 很明显，这种模型不少，按一定规则可以创造出新的模型，所以不值得为每一个模型都写上一篇教程，所以在这里我们将一口气学习若干个模型， 作为一个合集， 总结其中规律，最好是了解下各种模型的优劣、做个对比（然而我没做到）。 废话说太多，开始正文。 首先明确一下我们的目标—— 学习若干神经元的简单的抽象模型 重复一遍，我们已经学习过 感知机 和 逻辑回归 ， 接下来我们要有信心， 重新造出这些轮子。 感知机和逻辑回归，哪个简单？ 感知机 逻辑回归 submit 很简单的感知机 回顾感知机的模型， 数学式如下： $$ f(x) = sign(w x+b) = \\begin{cases} +1& {w x+b \\ge 0}\\ -1& {w*x+b < 0} \\end{cases} $$ 现在要把它看作是一个 神经元 ， 所以得到下一部分内容 二元阈值神经元 二元阈值神经元 binary threshold neurons, 于1943年由McCulloch 和 Pitts 提出，这两位是生物学家，研究神经生理学的。 而我们知道 感知机(perceptron)感知器(perceptron)1957年由Rosenblatt提出。 它俩的数学模型是一样的， 但感知机还有一套完整的模型有策略有算法， 区别应该是在这里。 二元阈值神经元binary threshold neurons有两类形式， 一种如上， 另一种简单变换下就是： $$ f(x) = sign(w x) = \\begin{cases} +1& {w x \\ge \\theta}\\ -1& {w*x < 0} \\end{cases} $$ 其中$\\theta = -b$。 来个命题作文，看了二元阈值神经元和感知机，你有何感想？ submit 个人感想 题外话， 两个神经学家（好怪的感觉）1943年提出的神经元模型，直接影响到了1957年的计算机算法 感知机， 而且据 Hinton 说也影响到了冯诺依曼设计电子计算机。 所以，一个人的命运当然要靠个人的奋斗，也要考虑到历史的进程。 不对， 学习和研究要做到触类旁通、 举一反三， 所以知识面广是有好处的， 可以从其他领域借鉴思想。 新瓶装新酒当然是最好的， 但能做到旧瓶装新酒甚至新瓶装旧酒也是有意义、有很大贡献的。 接下来回归正题。 如何继续？ 知道了二元阈值神经元或感知机，下一步可以做什么？ 对它进行扩展、变化 另起炉灶 submit 从XX说开去 标题很套路，个人感觉中小学时期看了若干篇这名字的作文。 我们确实可以另起炉灶， 从其他领域模仿（借鉴）到新的想法，但确实不在这篇文章的范围之内了。 还是从 二元阈值神经元 开始，作为切入点。 找到切入点后， 做什么呢？对一个原型有些什么方案来进行扩展呢？比较尴尬，我本来以为六顶思考帽有讲如何做扩展、变化，结果不是这本书，不知道是哪本书讲的了。只能说说自己的观点， 不太靠谱。 扩展、变化 不外乎三种结果： 简化 复杂化 等价、难度相当 扩展、变化的方案我记得（难不成真是我自己编的？）有几种： 替换 逆向分析 加一点 减一点 以上都是指导性原则（如果是前人、大师总结的，那很好，但现在是我总结的就尴尬了）， 我们记住这些原则，有意识地、刻意地使用， 进而形成良好的习惯、本能。 我们现在是不是要对二元阈值神经元（感知机）进行扩展啦？ 是 不是 submit 扩展的方向 还记得这篇教程的名字吧？ 《一些简单的神经元模型》， 没有人知道正确、准确的数学化神经元模型是什么样。 所以可以发挥自己的想像力，创造出各式各样的神经元模型，就好比DNA结构发现之前，生物学家提出过很多种模型，但DNA证明是双螺旋结构后，还跳出来说DNA是其他结构就成笑柄了。 扩展先看目标， 有简化、复杂化、等价三种结果。 找句名言壮壮胆， 爱因斯坦说过 \"Everything Should Be Made as Simple as Possible, But Not Simpler\"(这句从Udacity的机器学习导论上看到的)。 复杂化肯定是有无限可能的， 等价不好说多少种可能，一种没有都可能。 那我们希望找找最简单、最原始的方案，问题就来了。 二元阈值神经元是不是最简单的神经元模型？ 是 不是 submit 怎么简？ 要简化， 主要思路肯定是\"减一点\"； 替换能不能成功简化，不好说； \"加一点\"却简化原问题的例子太少， 我也就能想到 17个东西分给3个人的智力题。 好，我们来做简化， 二元阈值神经元 $f(x) = sign(w*x+b)$， 简化后f(x)= 预览: submit 线性神经元 所以我们得到了$$f(x) = w*x+b$$ 如果读者学过机器学习，或看了 机器学习系列 就知道，这个又对应到 线性回归 接下来不能再化简了吧， 线性函数已经是最简了， 那我们接下来就转向到复杂化。 可以从哪个开始扩展、复杂化？ 线性神经元 二元阈值神经元 submit 很明显，都可以，但我们还是考虑从 线性神经元进行扩展吧， 毕竟是基础，变化形式要更多样点。 sigmoid神经元 没有前后逻辑， 逻辑回归 是一种神经元方案， 对应模型： 对应数学式： $$ z=w*x+b\\ y = \\frac{1}{1+e&#94;{-z}} $$ 叫做 sigmoid 神经元， 因为 y=g(x) 这个函数叫 sigmoid 函数。 好处、优势不在这里讨论。 对比线性和sigmoid,二元阈值神经元，这种扩展的思想是什么？ submit 扩展方案 对比线性神经元， 就是 $y = g(f(x)), f(x) = w*x+b$， 给线性回归加了个包装，多加了那么一点东西。可以看出， 二元阈值神经元也是一个对线性神经元的包装 y=g(x) = sign(x)。 你还可以自行扩展， 比如 Hinton 视频里提的 Rectified Linear Neurons，我不打算扯了。 完了吗？ 完了 没完 submit 概率化 视频不太看得明白的一块，也不讲， stochastic binary neurons，是对 sigmoid的概率化扩展。 $$ z=w*x+b\\ P(s=1) = \\frac{1}{1+e&#94;{-z}} $$ 把sigmoid的输出看作出 P(s=1)的概率， 所以，最后的结果应该是sigmoid求出概率后，再加一个 softmax 得到最可能输出， 也可以说是再过了一遍 threshold。 完了吗？ 完了 没完 submit 恭喜 Hinton的视频里提了5种模型， 我这里主要提到了四种神经元的模型， 想把这种东西不割裂地串到一起，确实有点困难， 写得不好， 请多多包涵。 有了神经元模型， 下面就让我们来把神经元连起来组成神经网络，进而用于机器学习吧！ 下一篇 前向传播","tags":"机器学习","title":"一些简单的神经元模型"},{"url":"http://sndnyang.github.io/flask-problem-summary.html","text":"sqlalchemy 更新数据问题 要替换，特别是json类型数据，不能在原数据上添加， 没法更新。 需要新建变量，遍历原json数据。 如： stored_data = word_dict.data new_data = data for k in stored_data: if k not in new_data: new_data[k] = {} for e in stored_data[k]: if e not in new_data[k]: new_data[k][e] = stored_data[k][e] word_dict.data = new_data","tags":"web开发","title":"flask开发问题汇总"},{"url":"http://sndnyang.github.io/introduce-neural-network.html","text":"本文参考以下文献： Hinton coursera《neural network for machine learning》 课程主页 学堂在线 袁博 数据挖掘：理论与算法 本文内容比较简单，甚至有点空洞无物， 因为从人脑抽象出最基础的神经网络还是很直观的， 本文又不涉及具体方案的实现和验证。了解神经网络是什么一回事的读者完全可以跳过。 原则上我会从什么开始？ 问题 定义 故事 历史 submit 为什么要引入神经网络 很明显， 人类大脑是目前已知学习能力最强的可模仿对象。 机器学习要有学习能力， 怎能不从人脑这里取点经？ 所以科学家们模仿大脑、神经网络，提出了人工神经网络artificial neural network, 缩写ann，一般计算机相关领域就简称 神经网络 （neural network），缩写 nn。 人脑的介绍部分 大家在中学或科普书上应该都了解过， 可以在Hinton课程第一周第二个视频再看一下。 接下来，我们要做什么? 归纳 演绎 抽象 推导 证明 实验 关闭浏览器 submit 抽谁，对谁进行抽象 生物问题就不问了。暴露一下自己的无知， 人脑神经系统主要是神经元和突触， 对吧？ 就抽它俩。 我们知道， 人脑视觉、听觉各种觉、活动都是神经元细胞间通过突触来传递信号，传着传着就传出了非常复杂的大脑功能——有说大脑进行量子处理的，这完全超出我的知识范围， 没法介绍， 现在我们就假设人脑活动就是 神经元间用突触传信号。 8-) 这里有没有必要提问，如果提问，该如何问。 学过图论、离散数学、数据结构的读者知道突触肯定抽象成图， 没有基础的呢？ 神经元抽象成数学模型、函数，看起来顺理成章，但我们自己能否有这意识，细胞具体做了多少工作我们不知道，为什么不被抽象成一个图灵机？ 我们能想到借鉴人脑神经结构、抽象神经系统吗？ 抽象结果 抽象起来不难， 一个神经元可以看作是一个数学模型或函数， 接收信号，输出信号。 而多个神经元通过突触连接显然是个有向图， 每个神经元都可以有多个神经元向它传信号，也有多个神经元可以接受它传出来的信号。 所以，我们抽象得到一个基础的神经网络（框架）： 整个神经系统看成一个图， 有输入有输出， 不闭合。 单个神经元看作是一个数学模型、函数， 即图上的一点。 神经元间的突触，看成是两个神经元间的边。 一个典型的图示: 那之后呢， 就是在这个框架基础之上的各种不同的实现方案， 神经元的具体方案、 突触的具体方案。 历史部分 最开始的是相当于神经元的感知机模型(perceptron)， 发展起来浅层的神经网络， 可人工智能之父之一明斯基1969年与人合著，证明感知机能力有限， 神经网络方法一度沉寂。 06年后，Hinton开始，发展出深度学习，神经网络再度火热。 其他课程里可能有讲历史的部分。 安排 先介绍类似神经元的 感知机 和 逻辑回归 再介绍神经网络的经典算法 前向传播 和 反向传播 介绍深度学习网络——#todo目前本人还没看过深度学习，所以没法估计后面内容。 思维导图 深度学习导图 完成！！！ 真的没话说。","tags":"机器学习","title":"神经网络引入"},{"url":"http://sndnyang.github.io/2016-11-01-diary.html","text":"星期二 天气：晴 标题：不重要的日记还是不要写， 显得太注水 新安排： 英语必须搞定，写作、口语、听力好好练 zhimind学习顺序： DL->NLP->AI->MACHINE LEARNING 数学复习： 概率->线代->暂空 计算机基础： OS->DS,ALGORITHMS->COMPILER python代码： runestone 尝试调flask","tags":"日记","title":"2016.11.01.日记"},{"url":"http://sndnyang.github.io/deep-learning-catalog.html","text":"深度学习目录 自引用： 知维图系列-深度学习目录链接 思维导图 鼠标停在结点上时，如果有链接则会显示出来， 不然就看这个目录吧， 目录和导图转换还没有实现。 基础概念 机器学习定义 机器学习主要类型 机器学习三要素 神经网络 神经元 感知机 逻辑回归 其他模型 学习方式 前向传播 反向传播 深度网络#todo 自编码器auto encoder#todo 卷积神经网络cnn#todo 递归神经网络rnn#todo 等等 待更新 在看 Hinton在Coursera 开的 neural network for machine learning","tags":"机器学习","title":"知维图系列-深度学习目录"},{"url":"http://sndnyang.github.io/machine-learning-define.html","text":"序言 本文参考到、没参考到的内容有： Hinton coursera《neural network for machine learning》第一周 Andrew Ng在斯坦福的cs229， cs229 李航《统计学习方法》 我原则上会从什么开始？ 问题 定义 故事 历史 submit 从问题出发 现在问题是，我们到底遇到了什么问题？没钱，房价太高，生活压力大，都是问题。 好尴尬，没头没脑的问这么个问题，谁能回答得上来，是吧！ 所以说，发现新问题，或者说创造性思维都是要基于足够的领域积累，比如正在讨论的这个机器学习概念，就需要在计算机领域及智能领域有一定的研究才会有这经验， 比如说， 对目前计算机领域，以下哪个任务最难？ 存储10*1000G的视频数据 看几十张图说话 处理100万人同时访问 submit 人和机器不一样 看图说话简直太容易了，根本不是问题，但对计算机行业来说， 看图说话这种人工智能就是天大的难题。 如果不了解这个领域，那想发现、提出问题就很困难。 这里必须假设读者明白看图、对话等人工智能问题的难点。 如果哪位觉得这些问题都小菜一碟、不理解为什么会难住计算机，那我该如何聊下去呢？ 对计算机来说，这些问题最难的地方是哪里？ 规则\\模式未知 规则\\模式很复杂 运算速度不够快 容量不够大 现实没法用二进制模拟 submit 知道问题的难点 毫无疑问，人脑如何识别图片、如何掌握语言， 这些我们都不知道！难就难在这里，根本没法用数学来表达。 规则、模式很复杂算不算难点呢？ 一般算，但相对未知规则、模式而言，个人认为还能接受，反正现在复杂的程序、数学定理多了去了， 1亿的复杂度和10亿的复杂度对普通人来说，没区别。 规则模式不知道、说不出来，你能想到一句什么名言? submit 知难而退？ 不可道， 我们就永远学不会了吗？ 人的世界里\"道可道 非常道\"，那人也还要学\"道\"，虽然不是每个人都学或能学会，勤奋或天赋不一样，但机器又没天赋这事，你250核CPU，250万核 GPU， 250T内存了不起哦？也就比我快一些，\"智商\"本质上还是一样的。 扯远了，总之， 我们知道， \"道\" 和这些图像识别、语法规则一样，都是讲不清楚的。 那人是如何近（接近、学习）道的？ 看四书五经 看道家书籍 打坐冥想 模仿体验 抄袭 submit 学与习 模仿体验就是一种学习，没有限定学习范围，也许还能看出一点归纳（模仿）、演绎（体验）的意思。 所以结论就来了， 因为图像识别、语法规则\"不可道\"， 所以希望机器自己去\"模仿体验\"，去归纳、演绎，自己去学习到\"不可道\"的规则， 也就是机器学习。 前面也是废话， 说白了， 就是希望机器（计算机）和人一样聪明、一样有学习能力，能像科幻电影里的机器人一样。我们也都知道现在的计算机程序， 蠢，很蠢，非常蠢。 就希望\"机器学习\"技术能让机器学习起来。 终于，引出了我们教程的主角——机器学习 下一步干什么？ 举个栗子 看个应用 给个定义 讲个故事 submit 什么是机器学习 名字易懂， 但我们要学会自己进行抽象，用准确的语言来描述一个概念，而不是看着别人写好的概念、定义 说\"WTF\"。死记别人写的定义非常枯燥、困难，我们学着自己去抽象、总结，一来容易记忆，二来是对抽象能力的锻炼。 我们来看名字，机器没什么说的，机器、里面的计算机程序、系统都行；学习呢？ 广义：是人在生活过程中，通过获得经验而产生的行为或行为潜能的相对持久的行为方式 百度百科 ——怎么感觉有点病句？ 8-) 看不明白，套点熟悉的东西，语文里写作文、故事不是讲究 时间&地点&人物&起因&经过&结果 吗？ 如果把机器学习就是一个过程或一件事，你觉得\"机器学习\"*还*需要哪几大要素？ 时间 地点 人物 起因 经过 结果 submit 套路模板 既然是起因、经过、结果， 来分析下 起因是什么？毫无疑问，某个方面不够好； 结果？ 就是希望某个方面得到提高，这是目标，不一定真做到，邯郸学步还退步了。 经过。 这就是从哪里学，怎么学的问题。 从哪里学？从人这里学————没错。 这里得强调下是指程序已经完成的机器。有学习能力的机器能\"自学\"， 就是指接受外部的信息进行学习。 外部信息对机器来说是什么？ 输入 输出 存储 数据 网络 通信 submit 自己的定义 外部信息是输入，也叫数据。 我们就说是从数据中学习到经验。 怎么学？这对人来说都不是件简单事，各种教辅书、学习方法大行其道，大赚特赚，就是因为大家不知道人到底怎么学的，天赋能否培养等等。 那机器要学习就有机器学习的方法， 一方面像飞机摆脱对飞鸟的模仿，发展自己的方法，另一方面则是了解、模拟、逼近人类大脑。这就是《机器学习》课程、 本系列教程要讲的内容。 现在我们人物、事件的起因、经过、结果都讲过了，所以定义就是： 人物 ,起因 ，从 里经过机器学习的方法学习到经验，结果 submit 请使用原文里的字词，比如结果这一空 不要用\"更好、长进、进步\"等词语——同义词、句判断对计算机来说就是个很难的问题。 定义 我们可以看到， 和人学习的定义也没什么区别，不过是主体从人变机器， 具体方法不一样而已。 现在给出 Tom Mitchell的定义： A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. 我们的定义里主要少了 性能评估performance measure P， 或者说当了点数学、数学符号，还是能用的。 你猜我接下来要扯些什么? submit 机器学习的应用 有很多应用， 效果好不好的都有很多。 其他课程里都会有介绍， 这块我还是不要乱发言的好。 结束 没话说了~~~描述性语言过多，很难编写启发式问题，选择太简单，填空很难填。 恭喜 \"为什么\"讲得不好，读者应该试着总结了下机器学习的定义，希望您觉得有所收获。 下一篇 机器学习主要类型","tags":"机器学习","title":"机器学习定义"},{"url":"http://sndnyang.github.io/machine-learning-types.html","text":"序言 本文参考到没参考到的内容都挺多。 Hinton coursera《neural network for machine learning》第一周第五个视频 Andrew Ng在斯坦福的cs229， cs229 李航《统计学习方法》 不才在下专业水平、 写作水平都很次，若有意见或建议， 欢迎通过下方微博或邮箱联系。 原则上，上一篇讲的是 机器学习定义 这一篇将向各位介绍机器学习的主要类型。 原则上从什么开始？ 问题 定义 故事 历史 submit 不是每个东西都能启发的 个人认为，对一个领域进行分门别类 需要较高、较深的专业知识水平，有了足够的问题、知识，进行归纳、分类，这是我个人理解的做法。 但一般教材都没这么做，直接把各种分类一股脑地列出来，因为也确实做不到，不可能列出足够的例子，慢慢地归纳出各类的特点。 并且，分类的方式可能多种多样， 往往不存在唯一的权威分类方式。 所以这块的教程写起来就会比较难，读者也不是很有体会。 所以，我要尝试做到更自然地从定义来预测大概有些什么问题，可以划分哪些类别。也算是锻炼了直觉、 猜测的能力。 还记得， 机器学习的起因、经过、结果吧？ 起因：某些方面不够好，经过：从数据中使用某些方法学习到经验，结果：提高了性能、准确率、知识等等。 哪总分的特点可作为分类依据？ 起因 经过 结果 submit 那又如何？ 从数据中使用某些方法学习到经验， 这句话能有什么特点？ 学完机器学习，也许 我们可以在方法上进行分类（我也没学完，所以瞎分一下）——线性、非线性、核方法；最优、非最优；树状、网状、线状；单挑、群殴。 但是，我们还什么都没学呢， 怎么分？ 也许以后值得一试，但现在没法。 那我们只能看\"数据\"了，分成图像、文本、 数字（整数、虚数）？估计也是一种方案——不过肯定得不到大家的认可。 数据有什么不一样的呢？ 肯定又要从人类的日常汲取养分了。 那人最早是怎么学习的？ 写写你的想法 submit 一种学习 是不是父母指着一个东西说这是什么？ 但是不是所有的学习都是这样呢？填是或你认为的其他学习方式。 submit 反馈学习 我现在不知道你填了什么~~~不好意思。 除了父母、课本明确指明对象的名字、标签的学习外，还有种反馈学习，什么时候会有反馈、反馈是对是错都不知道， 但反馈对之后的行动又非常重要。 就好比你在恋爱，你自以为体贴周到，可从对象那里什么时候会有反馈、 什么样的反馈、甚至是否能正确理解，如何能从对象那里持续获得反馈、而且都是真正的积极反馈（你们相处得很好），这就是恋爱的学习了~~~ 那我们现在就有两类学习： 有明确、及时的标签。 有不一定及时、完全不明确的反馈。 还有没有其他可能？ 有 没有 submit 还有什么？ submit 还有 有明确、及时的标签。 有不一定及时、完全不明确的反馈。 那肯定还有个， 完全无标签、 完全无反馈的学习。 想想， 人什么时候这样学习过呢？没头没脑地一堆数据扔给你， 什么解释都没有， 这种学习形式应该是比较少的。 你能想到这种例子吗？ submit 为什么少？ 个人感觉就是对人而言， 学习还是要有个明确的、形式化的描述，模糊、朦胧的感知很难达到学习、提升的效果。 比如说机器学习里需要的聚类， 人手工也能聚， 但你说能从中学到些啥说不清楚， 假设班级中，人以类聚，但说不清楚。 至于数据降维就更是没几个人自己弄过——求特征值、特征量，但没拿来用过。 所以这种学习形式不难理解， 就是亲自尝试、有体会的不多。 还有其他类型吗？ 有 没有 submit 总结 我们总结出了三种 有明确、及时的标签的学习。 有不一定及时、完全不明确的反馈的学习。 完全无标签、 无反馈的学习。 这就是机器学习的三种类型： 监督学习： 有标签的数据的学习 无监督学习： 没有标签的数据的学习 强化学习： 有反馈影响行为的学习 完结撒花！ 真是不容易啊， 没有多少关联、因果关系的内容， 我就这么强行捋了一条线索， 虽然效果估计有点差， 对填空题支持不够。 总之，还是希望您能有所收获。 下一篇， 机器学习三要素","tags":"机器学习","title":"机器学习主要类型"},{"url":"http://sndnyang.github.io/notes-on-design-sprint.html","text":"简述 因为还是有那么点想法的，所以想找这类书籍来学习一二， 其实说到底， 技术不过关， 产品没研究， 营销没了解， 设计同样搞不定， 又是一个人单干， 真是什么都干不成。 简单做点笔记而已。 搭建舞台 即开始之前的准备 识别挑战 从大问题入手 从原文中感觉不出是大问题， 应该是重要、关键、困难的问题。 对以下挑战、困难，很适合用设计冲刺： 风险高：能降低失败成本 时间紧：5天时间可以说足够短 起步难：带来新方法和视角，从而找出解决方案 对于创业团队来说， 核心业务当然是重要、重大、困难的问题，所以都没错。并不仅仅是指时间长、人力多的任务。 但很多项目的首要就是外观、 界面，这个会给用户留下第一印象。 这块就算设计挑战有帮助，我也没办法。 组建团队 确定决策者、 引导者及多样化成员 组建7人以内团队： 决策者、财务、营销、客服、技术、设计或产品 兼听则明，所以可能的话，要有反对人士 确定时间、地点 原则：排除干扰 定出5天连续的工作日， 合适的会议室， 拒绝使用电子设备， 多使用白板。 星期一 确定方向 从结果出发 设定一个长期目标 \"我们为什么要做这个项目，从现在起，6个月后、1年后、5年后，希望公司变成什么样？\" 目标不是业绩指标， 是团队的原则，是野心、愿景、梦想。 目标背后的假设 目标背后的假设很危险，比如 zhimind的几大假设： 启发式教学模式好 * 启发式教学以目前的技术可以模仿出一定智能水平——其实做的很傻 * 思维导图式的学习进度也有一定作用 ** 人的思考过程可以去显式获取，逐渐逼近 **** 可以根据人的思考过程的数据，进行个性化区分、甚至天赋的判断 **** 需要将假设转成问题，进行验证 必须要解决的难题 列出冲刺要解决的难题： 想解决什么问题？ 为了实现目标，必要条件有哪些？ 想象下穿越到未来，发现项目失败了， 可能是什么原因造成的？ 把假设、 阻碍、 必要条件转成问题 zhimind的核心问题： 如何实现智能化引导式教学？问题太大，因为没头绪。 * * 人工智能是否能使用到？ **** 如何更好地匹配填空题的回答？ **** 如何实现比较大、多步的问题的答案匹配及思路指导？ * 如何记录思考过程，并利用？ 这个比较遥远了。 绘制地图 地图就是用户使用服务或产品的轨迹。 对zhimind而言： 读者、学习者： 选择感兴趣的领域，即选择相应思维导图，相当于选课，生成自己的导图（才能记录进度） 学习导图里各个知识点的教程或练习——没有标准程序，一般从根到子，从上到下（从左到右）。 不过这教程和练习接口不好， 应该像个下拉菜单，更不应该和操作部分冲突。 学习时，看教程、做练习，进行交互学习。 学完会有进度显示，知识点颜色会变化。 制作者： 按交互式的思路、原则编写教程和练习。 上传到某个地方。 在个人主页或思维导图主页 添加新教程、练习， 这个交互接口也很难用。 绘图还真不好弄，左侧列出参与者， 右侧写出结局。 中间简单地绘出情节过程。 请教专家 通过请教专家， 集中所有的知识储备。 需要请专家谈及下面论题： 策略：怎样才能让项目成功，独特优势或机遇是什么， 风险是什么 用户的想法： 从用户角度看问题 运行机理。 设计师、工程师、 营销人员。 前期投入。 通过\"我们应该如何\"的笔记，来整理、排序，并投票——也是提出问题、目标 选择目标、方面 上一步得到了很多的目标（或小方面）， 就该选择、决策了。 由决策者 选择一个目标用户和一个目标事件 引导者笔记： 坦诚地征求团队许可 随时记录， 把团队讨论都综合记录在白板上 刻意发问， 确保成员之间没有误解，可能引出一些重要细节。 照顾成员： 多中场休息， 每60、90分钟休息一次。 晚点吃饭 少食多餐 迅速做出决定， 继续前行 星期二 想出解决方案 最好的方案作为原型计划 重组与改进 回顾已有的点子和灵感， 闪电之旅， 团队成员轮流用 三分钟 介绍自己最喜欢的方案， 找到原始素材，而不是抄袭竞争者。 方案可以来自：其他产品、其他领域、 公司内部。 草拟方案 在纸上画出详细方案 跳过部分内容 星期四 实现原型 假装 用样品替代真正产品 写不下去了~~~","tags":"创业","title":"设计冲刺读书笔记"},{"url":"http://sndnyang.github.io/udacity-deep-learning-notes.html","text":"总结 udacity的课程还是讲得不深入，蜻蜓点水式的讲了一点内容， 记不了多少内容。 没做他提供的练习， 感觉不是很值得。 大纲 四部分： 基础？ 逻辑回归做分类——然而好像没给出模型式子，只给了损失函数 随机优化（随机梯度下降） 数据和参数调试——并没有讲多少的样子 初试 深度网络 正则化 卷积神经网络 convolution neural network 应用 嵌入 embedings 词向量 Word2Vec Recurrent Models LSTM softmax $$ S(y_i) = \\frac{e&#94;{y_i}}{\\sum_j e&#94;{y_j}} $$ One-Hot encoding 没整明白 cross-entropy 熵 输入 ， 经过线性模型（逻辑回归）转成， Logits ， 再经过softmax转成， 概率值 ， 再与 one-hot encoding labels 一起计算 cross entropy。 以上就是 multinomial logistic classification 其他 提到以下内容： SGD 随机梯度下降 rule of 30-- 影响到验证集中30个样例的可以被相信 交叉验证可能很慢，所以更多数据往往是正确方法 SGD技巧： 动量(momentum): 即保存梯度的移动平均（running average): $$ M = 0.9M + \\Delta J$$, 没确认视频里的符号是什么。 学习速率衰减(learning rate decay) Rectified linear units(ReLUs) 前向反向传播forward/back propagation early termination: stop before overfitting $L_2$ regularization dropout 丢弃法 cnn pooling 池化， max pooling 和 mean pooling 1x1 conv inception 应用一堆","tags":"机器学习","title":"udacity深度学习笔记"},{"url":"http://sndnyang.github.io/2016-10-23-diary.html","text":"星期天 天气：晴 标题： 昨天吃多了~~~ 昨天板栗吃多了， 肚子涨了一整天， 疯了， 今天总算把全部板栗干掉了，明天还有一天的闹腾。 网络课算是讲得挺好的了，尤其是对比当年的雷老师和下午讲信息检索的老师。 知维图继续--深度学习和数学 英语考试--gre作文，口语和听力 学习--python和AI和NLP","tags":"日记","title":"2016.10.23.日记"},{"url":"http://sndnyang.github.io/2016-10-22-diary.html","text":"星期六 天气：晴 标题：这课上得啊 课讲得太坑了。照老师这么生硬地讲课， 真不如自己看书呢， 可老师在讲台讲，我也没法集中注意力看其他的——不如老实听课？等着睡着？ 我的这个网站还是主要当个预习吧， 今天分析了下，觉得想结合人工智能还是不太现实，没有太多人工智能使用的余地。 比如说选择吧，完全都是个人想法而已，人都不一定想得明白，怎么用人工智能来描述呢？填空题目前也多半是名词而不是句子，弄点or或者同义词判断就够了，没有知识推理的需求在内，真的要求填句子的话，同义句判断可就难了，提示也难了。 公式推导、计算题的推导过程可能也只需要动态规划与答案进行匹配即可，如果是要判断两步之间的变换是否成立，这个会不会太难了？所以可能还是我想得太简单了点，先继续把深度学习教程写好吧。","tags":"日记","title":"2016.10.22.日记"},{"url":"http://sndnyang.github.io/2016-10-21-diary.html","text":"星期五 天气：多云 标题：日记又忘记写了~~~ 今天发现了一个小bug, 百度等手机浏览器没有 endsWith 函数，所以内容加载不了， 添加这个函数原型就好了。 雅思成绩出来了， 哭， 6.0分， 作文5.5，口语5.0 1800多又要搭出去了——改考托福？也没那么容易。 搜了一下， 上半年厦门的创业活动似乎真不少——下半年相对少了点，都跟我无关，先把技术原理搞透再说。 明天又要上课了， 这么大的笔记本搬来搬去麻烦， 那我看什么呢？手机视频吧， 上课好像太~~~枯燥了。 有人给我提建议了，虽然是写作方面了——","tags":"日记","title":"2016.10.21日记"},{"url":"http://sndnyang.github.io/thoughts-on-learning-science-engineering.html","text":"起因 写zhimind的 感知器基础 时，突然有所感地写了这么个问题。 探索世界要从什么开始？ 算法 数据 策略 模型 问题 哲学 方法 submit 因为我认为，理工科教育有条件的话也应该以一种探索世界、探索未知的模式进行，所以我才开发zhimind.com，希望能通过结合启发式教育和人工智能技术，实现让学生在学习过程中也体验到探索发现的奇妙。 尝试去探索未知领域时，一定有一些值得采纳的思维模式。这听起来像是流水线作业，把探索这么有创意、灵性的事情说得充满了匠气。可事实就是如此， 管理学、社会学听起来也是全看个人灵性、悟性的东西，现在也有不少很有效的模式。创造性思维、探索的过程照样有很多人在研究它们的模式，不是说一定有一套放之四海皆准的流水线操作，无数细节上的不同决定了只会有指导性的准则，而不是具体的操作。 那我目前认为的探索、启发式教育模式或准则是哪些呢？ 一、从问题，而不是从定义、概念出发 教材喜欢从定义、概念出发，因为这些对编写者是已知的知识， 但对学生而言， 这些多半是全新的、未知的知识， 多去尝试、摸索，锻炼思维，既能更好的掌握知识，对思维的锻炼也不是灌输所能比的。 典型例子就是线性代数里的矩阵，很多教材直接说的就是m行n列数字组成的什么什么就是矩阵，三个字母形容，WTF。合理的方式一定是从线性方程组开始，遇到未知数重复的问题，提取未知数、简化、抽象而来。而线性方程组则是从实际问题抽象而来，已经好理解，找出一个生活中让学生有共鸣的应用线性方程组的问题反而不是很容易。 所以一定要从问题出发， 要懂得问题是如何发现的， 培养发现问题的习惯。 二、一定要让学生自己抽象，给出定义或概念 很多时候，就算从问题出发了，也很可能只是做个简单的计算后，就直接给出定义、概念。 所以我们学习时，对定义、概念都会觉得枯燥。 其实定义、概念的提出就是抽象思维的体现，这个过程是抽象思维的一种锻炼，自己抽象后的定义和别人的对比，才会有更深的体验。 像很多学科开始总要介绍下学科历史，强行学习学科历史也是很无聊的，没有什么体会。有可能的话，要把历史巧妙地融合进整个讲课过程中，完全过时的东西作为期末科普介绍一二。为什么是期末，因为一个学期教好，学生对学科有一定了解，再回首历史才会深有感悟，同时还可以展望未来。 所以，有条件一定要让学生多去抽象，给出定义和概念。 三、 做题、思考过程一定要是启发式的 这点在很多证明题里最明显，推导过程没错，但为什么这样推或这样变形是没有任何解释的，事实上这才是难点。尤其是构建一个特殊变量的方式，引导出整个证明过程更不合理，怎么想到这么构建的，根本没说。 证明题及计算题都应该给学生自主探索解题过程的机会，只给出思路上的引导，让学生自己通关。如果不强制让学生思考，学生可能偷懒，即使学生很勤奋，但死记已有证明过程或例题，效果也会比较差。 所以做题、思考过程一定要是启发式的。 四、 完成后一定要给出显式、完整的思考过程说明 完成思考后，主要是潜意识中的模糊的认知，这样会缺少记忆的线索。给出比较合理的完整思考过程，就转换成显式认知后，更难遗忘。 所以，一定要给出显式的说明。 不仅是解题， 其他思维、习惯也可以如此。 总结 目前想到这么多， 以后想到啥补充啥。 准则日志记录 2016-10-20 从问题出发，而不是从定义、概念出发 一定要让学生自己抽象，给出定义或概念 做题、思考过程一定要是启发式的 完成后一定要给出显式、完整的思考过程说明","tags":"随笔","title":"理工科学习的一些想法"},{"url":"http://sndnyang.github.io/Todo-in-Article-List.html","text":"说明 写的zhimind文章对应的链接是随机字符串而非指定名字，特别是提到下一篇的链接时，没法预测结果，而且文章并不一定是按顺序一篇一篇写的，比如我现在正准备写感知器介绍，但分类、监督学习之类的概念还没有介绍，只能是留坑待以后了。 所以现在觉得需要这么记录下， 有更好的是搞个工具获取到网页上所有链接，然后设定好格式，自动替换。 这步倒是还能做，但还有文件上传到七牛云去更新的问题， 这个不像git这么方便。看着办了。 其实openshift还有几百兆的空间，放点文本文件绰绰有余的，不过每次push都会引起服务器更新~~~不值。用github之类的，又不是静态文件存储，速度慢。真是两难哪。 记录 感知器基础： 缺分类、监督学习等介绍 牛顿迭代法剩下部分太敷衍","tags":"机器学习","title":"教程中留的坑汇总记录"},{"url":"http://sndnyang.github.io/machine-learning-env-config.html","text":"为了节省环境配置工作， 可以使用网上配置好的虚拟机——个人使用足够。 数据科学工具箱 链接见： 数据科学工具箱 下载安装步骤 下载安装vbox 链接地址 下载安装vagrant 链接地址 下载和启动数据科学工具箱，使用的是命令行方式 登录，在Vagrantfile所在路径，输入： vagrant ssh 如果是windows用户， 网站推荐使用putty，但我推荐 mysy-git 给右键菜单添加git bash。 设置ipython notebook环境，补充说明： 在运行dst setup base，自定一个密码后，要exit，才能继续执行vagrant load(vagrant 命令是在windows下的，而vagrant ssh意味着已经连入虚拟机内)，不过先完成后续步骤，再exit和reload 安装额外工具包，如果采用下方网盘中的box（太老旧了），起码需要sudo pip install -U pip ipython jupyter notebook 网盘下载 由于网络原因，若网站上的命令下载过慢，可使用我已经下载好的文件 链接 密码: 6jei ，使用方法: vagrant init data-science-toolbox/dst // 这一步和网站上所写一致， 应该是生成Vagrantfile 打开 Vagrantfile 文件，查看 config.vm.box = // 即找到对应box名字， 我这里的是 datascience vagrant.exe box add ./data-science-tool.box --name datascience //即vagrant box添加一个名字叫datascience的box， 对应路径是你下载文件所在路径 在Vagrantfile中找 config.vm.network \"forwarded_port\", guest: 8888, host: 8888， 没有则添加这一行 安装工具包 这块有点复杂， 主要是网络状况影响（没办法）， 需要自行研究各路软件的更新方式。 比如我现在的网络访问 pypi就很不方便，需要设置pip 的访问路径, 参考 pip配置 建议使用豆瓣源 然而好像还是有问题 spark工具包 同理，可使用edx课程的spark.box spark.box 密码: cjpm， 带一个现成的vagrantfile，只需要box add， 即可使用。虚拟机里若缺什么工具包，请用vagrant ssh 登录安装。 vagrant 简要使用 vagrant up，启动虚拟机，之前设置好的话， 可以用浏览器访问 localhost:指定端口 访问 ipython notebook， 也是主要的练习方式。不过 ipython notebook是否自动启动，我也不太清楚 vagrant ssh，登录虚拟机，如果要安装什么新软件的话 vagrant halt， 有up就要有down vagrant -h， 帮助。 一些问题 data-science-toolbox升级ipython 到 jupyter后，外部访问会有问题， 网站的命令不管用了，要使用 jupyter notebook --no-browser --ip=0.0.0.0 最好是在 Vagrantfile 里添加 config.vm.provision \"shell\", run: \"always\", inline: <<-SHELL jupyter notebook --notebook-dir=/vagrant/notebook --no-browser --ip=0.0.0.0 & SHELL 这样应该能在vagrant up时，自动运行 jupyter notebook了 notebook使用 打开浏览器后， 会有上传功能， ipynb和其他文件都可以在这里上传 配置部分大概这么些 另外有个网站 实验楼 也不错，可以推荐 深度学习环境的配置 方案一 vagrant github 不熟悉github，可以点页面中绿色按钮clone or download , 选download zip。 下载解压， 然后和上面一样，vagrant up 或先下载box（ 链接 密码: sfde）后 vagrant box add 但网络不通， 最起码我下不来， 设置了代理也成 方案二 docker 直接 github 同上， 下不来。 方案三 使用daocloud加速器 指南 注册daocloud帐号 http://www.daocloud.io/ 登陆后取得专属加速器地址，类似这样http://xxxxxx.m.daocloud.io, 在控制台（个人首页）商标daocloud 那一排有个加速器，然后找——你的操作系统 安装docker windows（其他系统差不多，还简单一点，只有网络问题） 创建新的docker，可以是执行桌面上的Docker Quickstart Terminal，也可以是命令行docker-machine create -d virtualbox mydocker // mydocker可以是任意名字 环境设置docker-machine.exe env daocloud， 成功会提示你Run this command to configure your shell，运行下面那行命令 配置 Docker 加速器，参照 链接 。 下载docker镜像， docker search udacity 或 docker search tensorflow 注意，下载之前可能要先 docker-machine ssh 名字，不然好像也是连不上。 docker pull kyoungrok0517/tensorflow-udacity（感觉这个比较好） 方案四 使用daocloud在线环境 博客 没试过 方案五 使用cloud9在线环境 指南 也没试过 方案六 大学镜像 指南 但步骤不是很明确，操作不太明白。 执行 windows: winpty docker run -p 8888:8888 --name 名字 -it kyoungrok0517/tensorflow-udacity","tags":"机器学习","title":"机器学习练习环境配置"},{"url":"http://sndnyang.github.io/jobs-requirements-summary.html","text":"所以近期计划 强化： 深度学习 数学 自然语言处理 python原理 机器学习 具有扎实的数学功底、数据结构和算法功底； 熟练使用R，Python等语言； 对数据敏感，具备较强的特征发现能力； 熟练掌握Oracle/MySQL/SQLServer/DB2中的一种或多种； 有推荐系统、用户画像经验者优先； 具有spark或storm技术框架的海量数据处理经验、数据挖掘经验者优先。 熟悉编译原理及操作系统； 熟悉并行计算或者分布式计算； 自然语言处理 主要技术要求 熟悉Java或Python；熟悉C/C++/Java语言中的一门或多门； 熟悉分词、词性标注、文本分类、关键词提取、自动摘要、语义分析、主题发现、情感分析，对话生成等自然语言处理技术； 熟悉N-Gram、PCFG、L-PCFG等； 熟悉HMM、CRF、MaxEnt等； 熟悉数据结构和算法设计； 熟悉Linux系统和脚本编程； 熟悉深度学习的理论与实践； 对CNN、RNN、LSTM、Word2Vec等理解原理并能应用于实践 至少掌握一种常用深度学习的框架theano，keras，mxnet，caffe，tensorflow 熟悉并掌握常用算法softmax,GBDT,CRF等 优先： 熟悉Scala、spark者优先； 有深度学习、搜索引擎、推荐系统相关技术经验者优先 熟悉 hadoop 生态，有大规模实时/离线数据处理的经验 熟悉分布式搜索引擎技术，有 elastic search, solr 使用/维护经验 熟悉知识图谱，知识表达，本体论等人工智能理论 java 多线程和线程池 设计模式 优秀的 java 基础，优秀的 OOP 编程思想，了解 JVM 工作原理","tags":"日记","title":"岗位要求描述总结"},{"url":"http://sndnyang.github.io/2016-10-20-diary.html","text":"星期四 天气：多云 标题：人工智能不好搞啊，哭 本日的日记忘写了， 没有认真搞项目， 英语也没好好学、练。","tags":"日记","title":"2016.10.20日记"},{"url":"http://sndnyang.github.io/perceptron-foundation.html","text":"说明 本文参考以下内容： 李航《统计学习方法》第2章 感知机 没有参考但可以参考 林轩田《机器学习基石》第二单元？ Hinton Coursera Neural Network for Machine Learning 有问题、建议，欢迎讨论。 再说明 因为计划一直在变更，最开始看《统计学习方法》，后来改成看Ng的机器学习（斯坦福cs229课堂录像)， 最近又因为写整个机器学习内容太多， 打算先看深度学习，打算先看udacity的深度学习课程，看完正好接Hinton的机器学习中的神经网络。 如果说深度学习、神经网络是一堆细胞的话，那它们的基础 感知器(Perceptron)就是一个细胞\\神经元，udacity的深度学习课居然用逻辑回归作引子——个人觉得还是要从感知器说起，于是又转回李航《统计学习方法》。 现在看Hinton的机器学习中的神经网络， 也是从感知器开始的。 所以这篇的一些内容会是其他篇（线性回归、逻辑回归）的重复，就是因为不同路线的混杂。 本段我提到了 本书和 门课程？ submit 就看你对这些名字的印象了， 涉及书、 公开课和MOOC。 李菊福的开头不容易 首先，假设读者了解 监督学习、分类问题等概念(见 机器学习的类型#todo )， 另外也假设了解 统计学习三要素#todo ，我是在李航《统计学习方法》看到的，其他书是不是基本这么个描述方式，不确认。 回顾下， 三要素： 模型。建的什么模，即目标条件概率分布或决策函数。 策略。选择、评估模型好坏的准则，一般用损失函数、风险函数等。 算法。具体的最优化求解算法，比如线性回归的最小二乘法、梯度下降法、牛顿迭代法。 如果直接上结论，那我就没办法搞这么个网站，专门来写这个教程了，别人知识的深度广度、语言的精炼到位都甩我十八条街，我唯一的优势就在于，要理清这一切的来源，给出这条关系链。 探索世界要从什么开始？ 算法 数据 策略 模型 问题 哲学 方法 submit 我想改填空，怕直接卡死在这里，那就尴尬了。不然从你各种回答记录里也许也能了解更真实的你自己。 模型 反正我是觉得吧，有好奇心、有需求，就会提出问题， 才会想办法去解决问题。不过，好奇心和需求只是准备条件， 提出问题才是真正的开始。 从问题开始，设定模型、完成建模，所以我们现在要先分析三大要素里的 模型 。 从最简单的问题出发 比如区分男女的二类分类问题，二类分类是最简单的分类问题——多类一般在它基础之上的改进。问题要求很明显，根据输入的实例的特征、属性值， 输出该实例的类别，取0、1或-1、+1。 这个抽象过程应该比较显然了，不再另行解释。 假设每个人（个体）在高维空间中都是一个点，那么我们 可能 用什么来划分男女呢 ？ 超平面 超直线 超曲线 超曲面 超球面 其他不规则的面 方便面 submit 这些面是什么？ 我们有这么多种面，方便面、干脆面，不，超平面，超曲面，超球面，超不规则面， 可能可以 把男女分成两堆。 所以，这些面是什么？ submit 模型的思考 我们有这么多种面， 三千碗面，只吃一碗， 就是我们要学一个模型， 一个目标条件概率分布或决策函数。种类这么多，是因为模型的假设空间(hypothesis space)包含所有可能的条件概率分布或决策函数，也就是说空间里有无数种面，每种面还有无数个可能。 我们肯定是要找到足够好的面——最好吃的面你知道是什么吗？有些人觉得这个好吃，有些人觉得那个好——所以找足够好的面，一个就够。 选择就要有个原则，超平面，超曲面，超球面，超不规则面，你会选 ？ submit 为什么？ 为什么选超平面，你的原则是 ? submit 简单原则 没错， 有用、有效之外， 简单原则重要，没事搞那么复杂不合适，除非是发现需要这样做，比如现在量子力学和相对论就没能合并成一个更简单的理论。 根据简单原则选择超平面，那对应的决策函数或条件概率分布应该是什么样的？ 一次 二次 指数 n次 对数 开方 submit 模型终于出来了 不管绕了多远， 总之我们现在打算根据简单原则先弄个线性模型， 即 y=w*x+b 有问题吗? submit 最终模型 那简单，$y=w x+b$ 是个超平面，分开了男女，一边男一边女，不就是平面上平面下各是一类吗，所以对应数学表达式，就是求w x+b的正负，写作： $$ f(x) = sign(w x+b) = \\begin{cases} +1& {w x+b \\ge 0}\\ -1& {w*x+b < 0} \\end{cases} $$ 下一步是？ 选策略 写算法 给定义 submit 抽象定义 上面得到的 f(x) = sign(w*x+b)， 可以用下图表示： 或 学术界就把这么个模型叫做 感知器(Perceptron)，又叫感知机，我更习惯前者。 我们来明确下，感知器是做什么的？ 看数学表达式，可知， 是二类（正或负）分类的线性（w*x+b)分类模型 输入是实例的特征向量——特征、属性组成的向量 输出是类别，取+1或-1 对应空间中将实例划分为正负两类的超平面 模型部分讲完了吗？ 完了 没有 submit 策略 上面说了， 感知机学习是求一个将训练集划分成两部分（两类）的超平面，最好当然是要把正负实例点完全正确分开，确定感知机器参数 w，b。 根据定义、目标--把正负实例点完全正确分开， 损失函数可以选择为? 误分类点的总数 误分类点到超平面的总距离 所有点到超平面的总距离 误分类点距离平方和 submit OK？ 显然可以用 误分类点的总数 ，表达式未记录，自行领会。 但大家 注意 ，在机器学习对模型进行优化时，一般采用导数、偏导、梯度一类的概念，找损失函数的极值点，所以， 找极值点自然是要在连续可导函数上进行的 ， 离散、不可导的函数就尽量不要来凑热闹，虽然它们可能很直观， 这个应该是机器学习、最优化里一个小小的准则了。 很显然， 误分数点总数就不是w,b的连续可导函数， 不能采用， 所以问题又来了， 损失函数可以选择为? 误分类点的总数 误分类点到超平面的总距离 所有点到超平面的总距离 误分类点距离平方和 submit 推导距离 好，那我们来推导距离的表达式——并没有，因为是中学的知识，虽然没推过或时间太久，都会造成没有深刻的印象，但还是请读者自己推导任一点$x_0到超平面S的距离。 你推导出来了吗？ OK 没有 submit 一大波结论来袭 $x_i$ 到超平面的距离:$\\frac{1}{||w||}|w*x_i+b|, ||w||$是w的$L_2$范数。 去掉绝对值，需要乘上$y_i$(这算常用技巧不？)， 因为对误分类数据而言， 乘上y_i一定会 < 0，还得再取个反，乘上-1。 最后求所有误分类点的总距离为： $$-\\frac{1}{||w||}\\sum \\limits_{x_i \\in M} y_i(w*x_i + b)$$ 完了吗？ 可以了 还有 submit 一次就好 看 SVM-最大间隔 时会发现，几乎一样的式子，但下一步就大相径庭了。 所以这一步算是最难解释清楚的， 所以我不知道编写什么引导问题。 其实是利用了线性函数（方程？）乘常数不变的性质。因为我们求的实际上还是超平面，即$w x+b=0$, 我们知道 $2(w x+b)=0, (w*x+b)/w = 0$等表示的都是同一个方程，所以我们可以将 $||w||$ 看成常数1， 反正都能变换成常数1。 所以， 我们把$||w||$固定看作1， 就等于是忽略掉了分母——SVM方案就是搞分子，更难理解。 习惯上会觉得b也应该受影响，如果把b看作是$w_0*x_0$ 就没这个问题了。 另习惯上，我们也会认为 y=w*x+b，那||w||变换，y也会变，但这里二者并不相关。 最终的策略 感知机$sign(w x+b)$学习的损失函数定义为 $L(w,b) = -\\sum_{x_i \\in M} y_i(w x_i + b)$ 是否是w,b的连续可导函数， 书上也只是概念上的分析，并没有推导、证明，反正它就是。 感知机学习的策略就是求: $$\\min_{w,b} L(w,b) = -\\sum_{x_i \\in M} y_i(w*x_i + b)$$ 补充可分性 显然， 能否没有误分类点，即损失函数值为0，关键还看数据集， 如果数据集能完全正确地划分开，那就是有可分性，如果能用超平面S完全正确地划分到两侧，那就是线性可分性——这个定义确实很明显。 你觉得数据集有线性可分性的可能性高吗？ 高 低 submit 不可分怎么办？ 凉拌！ 损失函数迭代次数求最小,或见下面的算法部分。 算法 绕了大圈，终于开始搞算法了——计算机专业一般都听说过 程序=数据结构+算法，但在机器学习中，模型选定很难，策略选择一开始好说，但转换就麻烦了，反倒是最后的算法部分，很多都使用梯度下降法(stochastic gradient descent)，要么像SVM直接说凸二次规划问题，算法细节上的缺失让我经常有名不副实的感觉 8-) 感知机学习的算法也是用的 随机梯度下降法#todo ， 原始形式： 选取初值$w_0,b_0$ 在训练集中选取数据$(x_i,y_i) 如果 $y_i(w*x_i+b) \\le 0w = w + \\eta y_i x_i \\ b = b + \\eta y_i 转至（2），直到训练集中没有误分类点（或可分性未知情况下，迭代若干次，选误分类最小的） 算法收敛性 收敛性部分#todo 暂缺 算法对偶形式(dual) 意义不是很看得懂，也空缺。 编程实践 纸上得来终觉浅，绝知此事要躬行。 所以我制作了一个ipython notebook的练习， 链接下载地址 有ipython notebook的朋友知道如何使用。 没有ipython notebook的朋友， 如果想了解python，可以自己下载python，安装ipython\\jupyter包。 觉得配置麻烦（其实不麻烦）的，可以参考 虚拟机 恭喜 本篇到此为止， 希望您能有所收获。","tags":"机器学习","title":"感知器基础"},{"url":"http://sndnyang.github.io/newton-methods.html","text":"免责说明 本文参考以下文献： Andrew Ng在斯坦福的cs229， cs229 希望能做到好玩、易学。 开篇引入 在前面的 线性回归基础 一讲中，我们引入了 梯度下降法 这一概念。 在 逻辑回归基础 中，介绍逻辑回归的数学模型及策略， 它的算法同样也是用的梯度下降法。 我之前对梯度下降法的描述可能有所不足。但这里还是要让大家回顾一下： 梯度下降法是怎么执行的? 求平方 随机采样 求导 二次求导 求对数 submit 回顾梯度下降 梯度就是求导（偏导），得到各个分量（变量）上的偏导，然后呢？ 各分量更新 各分量减偏导 各分量加偏导 submit 公式回顾 所以，我们复习了梯度下降法 似乎挺好的， 就像这张图显示的 但是，一般来说，它是否近乎完美了呢？ 你觉得它可能有什么问题? submit 分析 我们来一点点地分析， 看概念和公式，有什么值得商榷的内容。 首先， 梯度下降法的计算过程就是沿梯度下降的方向求解极小值（或极大值）。 在不使用其他思想（非求导方法）的情况下，你觉得这话有没有问题？有什么问题? submit 分析公式 公式 你觉得有没有不足？哪块有问题? submit 目标是智能提示，现在没做到，但不能等机器完全智能化了再来，那时候已经没意义了，现在是想抢占先机。 系数的意义 系数就是限定了沿梯度方向移动的距离， 假设这一点上的偏导是a，但如果我们的步子迈得太大，容易~~~你不知道整体的偏导值，所以移动距离远了，就可能跑过头了。错过了极值点。所以一般会把这个系数设得比较小。 可这个系数终究是人工设定的， 离极值点近的时候要小，远的时候要大才好，不管人工怎么设置，终归是猜测。 那我们是不是该想办法让它自动呢？ 能自动找到合适的距离 怎么可能 submit 探索方法 梯度下降的一个示意图: 说明 前面写复杂了，觉得这块没必要讲这么多。后面简略。 可以想到，我们可以延长切线到y=0，此时的x值就可以作为下一次迭代的x值，这样我们就可以自动地找合适的距离。如图： 那这个x值 怎么求， 明显要推导了。如上图$x_{n+1} 和 x_{n}$， 显然有： $$ f'(x_n) = \\frac{f(x_n)}{x_n - x_{n+1}} $$ 所以有$$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$ 步骤与梯度下降法相似： 初始于某值作为$\\theta_0$ 根据上面的方法求出下一迭代$\\theta_{n+1}$ 直到 $f(\\theta) 接近 0$，有多近看你要求了。 另外，我们在回归问题中要求解的是一次导$J'(\\theta) = 0$时得到的极值， 所以在回归里的式子就是$$ \\theta_{n+1} = \\theta_n - \\frac{J'(\\theta_n)}{J''(\\theta_n)}$$ 速度分析 不是时间复杂度， 具体分析也没说， 只说是 二次收敛， 速度上比梯度快很多——但为什么以往更多用梯度下降法、 线性回归没说用牛顿法没说（可能是求导结果不好）。 泛化 泛化到向量、矩阵， 但没有详细推导。 $$ \\theta_{n+1} = \\theta_n - H&#94;{-1}\\bigtriangledown_{\\theta}J$$ H 是 Hessian 矩阵 $$ H_{ij} = \\frac{\\partial&#94;2 J}{\\partial\\theta_i\\partial\\theta_j} $$ 都没听说过这个矩阵~~~这种比线性代数高阶的内容，是高等代数还是矩阵论还是啥 我也不知道 恭喜 恭喜您读完本节内容， 完结撒花！！！","tags":"机器学习","title":"牛顿迭代法"},{"url":"http://sndnyang.github.io/2016-10-19-diary.html","text":"星期三 天气：多云 标题：昨天日记忘记写了 搞docker配置， 花时间太多了~~~累了， 20号14：37才基本完成，哎， GFW造成的效益损失真是不知道多大啊","tags":"日记","title":"2016.10.19日记"},{"url":"http://sndnyang.github.io/2016-10-18-diary.html","text":"星期二 天气：多云 标题： 大改太费时，设计要想好。 今天主要是完成了 zhimind前后端的一次调整，说不定效率更差了。 不过因为使用了redis来缓存文章，整体速度肯定有极大的提升，话说我1个G的空间，才用了不到300M，完全可以放心大胆地用。但还是不想这样搞。 需要业务、内容推动了，自己拍脑袋想出来的也不见得好，还是看需要才行。 单词看了点，需要研究下重要单词的掌握问题——还有作文问题，真的等21号雅思出成绩了才着急吗？","tags":"日记","title":"2016.10.18日记"},{"url":"http://sndnyang.github.io/2016-10-17-diary.html","text":"星期一 天气：多云 标题： 今天又双叒叕过了 玩的时间比较多， 如何耐心看英文视频是件麻烦事，Ng的机器学习。 zhimind编辑器搞出来了， markdown-it之前 强行转了， 不会用它的模式。 就这样了。OVER","tags":"日记","title":"2016.10.17日记"},{"url":"http://sndnyang.github.io/2016-10-16-diary.html","text":"星期天 天气：阴 标题： 上课两天就疯了 上午老师有事停课， 下午~~~信息检索先讲知识产权，老师自己讲自己的，完全没考虑听众问题。 上午总算把zhimind.com 昨天突然挂掉的问题解决了， ssl还是不行， 只是zhimind.com转到www.zhimind.com，在openshift上添加www.zhimind.com 就OK了， sndnyang.github.io也添加到 blog.zhimind.com了。 明天继续写机器学习教程。 如果明天晚上， 胡和姜都没回复， 就给 赵 发邮件，当雷锋也是好事啊。 感想， 打铁还要自身硬， 还是得靠自己。如果自己一个人能从界面设计到前端到后端到人工智能、机器学习都搞得定的话，当年数学学得好的话，现在多好啊，后悔也没用了，界面设计是肯定搞不来的，只能看到什么抄什么了。 今日常规 单词 托福APP背完一遍 zhimind 准备继续写机器学习 健身 四组哑铃 口语 英语流利说打卡——感觉craig方法真是有一定作用。","tags":"日记","title":"2016.10.16日记"},{"url":"http://sndnyang.github.io/thoughts-on-education-gamify.html","text":"以前觉得用游戏来实现教育是十分棒的想法，绝大多数人估计是这样想的。但现在觉得教育游戏化空有其表， 是比较低水平的教学方法。 浅层 写了半天，发现思路混乱，不敢表达观点，非要足够理由， 太罗嗦，扔， 简单点。 我认为教育游戏化只能教授浅层知识，诸如背点诗词歌赋，记点人文地理，了解些或许很深刻的学科概念，一旦试图讲解内在原理等深层知识，会立刻破坏游戏的流畅性、娱乐性。 也许有人会举知乎上玩H游戏去学计算机程序的编译原理、 玩大航海掌握各国特产大气环流、 坎巴拉太空工程等游戏作例子， 但没有教育者认为哪个游戏能吸引\" 玩游戏到学计算机知识，比例非常小，可以认为是个例，别的不说，有人看教材都能拍案叫好，更多人只求娱乐性、刺激感、成就感，挫败感为主的学习是没有吸引力的。 其次，他学习还是通过看书，而不是游戏。 特产、大气环流都只是他在游戏中得到的浅层记忆，正式学习时回忆起来而已，最简单来说，花了多少时间玩游戏才获得这点记忆呢？大气环流、洋流明显是航行时的体验，要游戏里的多年时间才能记住，即使有攻略告诉你哪地方什么季节什么风向。 太空工程这游戏就是最明显代表教育游戏化不足的示例， 没人强制就用户少，自发进行深入学习的少，学习过程还是靠书本等传统手段，游戏里只能以低效率了解点表面概念。 教育学习化用来学点业余爱好、 开阔下见识还行，深入学习是基本做不到的，除了一个号称通关能当软件工程师的游戏，听说没试过。真要让人学习，还是得靠平时的培养，既要有监督，也要让学习有成就感。而监督可能也是教育研究者的误区，总想着自由和监督是对立的，总想着监督、管是错误的，所以总希望教育游戏能让人自发学习。 我说的是我观念上电脑、手机、主机游戏， 现代的智力竞赛、答题综艺、一些学习应用不带明确游戏内容的（比如英语流利说）不算在游戏内，如果教育游戏化是指广义的游戏，我可不知道广义的游戏边界在哪里，音乐和舞蹈等艺术学习更是如此，对我来说，学弹琴唱歌跳舞就是游戏，但被家长教师压迫着学习的儿童们可不这么认为，像节奏大师、QQ炫舞这种对我们来说是游戏，对弹钢琴有一定帮助，但谁用这种功能练钢琴能成为比较专业的人士（不要求是职业钢琴师）。","tags":"随笔","title":"不看好教育游戏化"},{"url":"http://sndnyang.github.io/2016-10-13-diary.html","text":"星期四 天气：阴 标题：时间过得太快，明天回校 老天保佑， 希望自己的创意能靠谱啊~~~不过不靠谱又能如何。不靠谱就自己继续试， 唯一就是压力有点大，因为还有英语，想到英语有点头痛。 其实也就是几十篇作文而已， 没事， 加油！！！！！！！！！！！哦， 雅思还不知道呢， 其实还是钱的问题。所以希望创意能带来好运。","tags":"日记","title":"2016.10.13日记"},{"url":"http://sndnyang.github.io/2016-10-12-diary.html","text":"星期三 天气：阴 标题：时间过得太快，发挥创造力 这几天太懒，晚上睡太晚了。日记也断了一天。 后天就要回去了， 想想也挺无聊的。 昨天晚上想了很久，好吧，做梦了大半天，谁不认为自己创意前景远大，恨不能运筹帷幄、指点江山，投资人纳头便拜。我虽然认为自己有没水平等等问题，但也对自己的创意充满信心:)。 所以 想像着能进行一番讨价还价——也不是讨价还价，没讨价还价的资本，只是承认自己在初创团队没位置，CEO/CTO/普通员工都搞不定，看着办吧。 好吧，想太多了，能有个邮件回复说，你到在行上定一下行程，来某地（一般投资人都在北京？）聊一聊，来不来，不来拉倒。这已经是很好的结果了吧。最大的可能是没回复，谁说你创意好的，想多了。能打个电话聊几句，都是做梦了。还想要自行车？Jeaf Dean还是 Thrun还是 Andrew Ng啊。 今天磨了一版商业计划， 效率最低的时候就是办不会、不熟、畏惧的事，磨了一整天。总算做完了， 发了两个——然后发现，有一个没必要，没团队没产品没戏。本来还想发一份的，结果居然没找到邮箱，我记错了？ 最后，想明白一件事，一个小项目不怕别人抄——不对，我本来就不怕。我只是英语没考好，头痛、烦躁而已。那我想明白了什么？好吧，想错了——那应该是有比较好的机会创业的话，这个留学项目就可以扔了，太尴尬了。","tags":"日记","title":"2016.10.12日记"},{"url":"http://sndnyang.github.io/on-english-speak.html","text":"最近在用英语流利说练口语， 发现纠正了些单词发音， 甚至是最基础词的发音，但整个口语情况貌似还没改变。 主要问题定位 词汇的使用， 这个不用说 句子、表达， 这个也很明显，生成中文不见得快，再转成英文~~~简直了，但英语思维很重要吗？求进步，有时间练习去接近就好了，英语思维很了不起，汉语思维很丢人？谁强谁有道理，中国给英语造了long time no see，英语也给中文造了一堆原本病句的表达。 单词的发音， 这个是很多国人自己觉得有问题，母语者当然听得出来，但这并不是中式发音最大的问题，对比下自己句子和原句的发音就可以明白，英音、美音自己就经常有区别，有些词就不同读法都行， 欧洲其他国家像东欧大舌头音就更夸张了。 语调， 这个可以区分出口音，特别是中国，但相对来说，不像词汇量、句子表达有那么多要掌握的，句子的语调类型不会特别多，也不是特别准确，有时候别人升调我降调甚至原调，别人降调我升调也是可以的，全部一个调的关谷神奇才是问题。 mining massive datasets这门课就是典型例子， 他们1、2基础没问题，但三哥和东欧小哥的单词发音是3的典型反例，Ullman作为纯粹美国人（？）可以作为4的参考，他照着稿子在那里讲， 这水平的语调是比较好达到的。 所以，我觉得最重要的是什么呢， 略读、连读后面的气息问题 ， 就是英美欧（甚至印度）没有的问题， 也是中式英语口语的最明显特点（日韩不知道），把这个克服了，中国人听起来口音没问题，英美人听起来可能顺畅得多。 我听自己的录音发现 很多词读完后断气、断声了， 跟句子不流畅有点关系，但熟悉也是断的。 有时候强行气连上了， 就没语调，太平。 有些单词音太重， 突兀。 单词习惯上在尾音最重。 分析来看，是因为很多单词尾音是纯辅音，比如 bad， flight。 这种单发的辅音却重读，于是在整句听来 是比较突兀的，对气息的连贯性是种干扰， 像weather这种连读起来就很顺，而health这种 th结尾的就会断， 于是把它当重音来读了。 可以引申，词典发音是错的吗？没有那么多错， 因为单独念这个单词时，确实是这样，重读时也是这样发音，这是降调、强调，但我们用平时读单词的方式来读句子就会有问题，每个词都重读，整个句子就支离破碎。 实际上我觉得是这样： weather这种不是辅音结尾的单词尾音重，但和后面单词的气息是连得上的。 无论清浊辅音结尾的单词，三种选择： 升调来弱化，比如单独念bad是四声，降调，但 bad weather不能用bad降调weather降调，bad要用升调第二声，连贯性就好多了。 略读或连读， 这英语课一般讲过， 后一个单词是什么辅音就略读， 是元音talk about就连读。 以往的英语教学里， 对2.2有提，但练得少， 对2.1语调的解释， 可能是我没注意， 我当初以为句子各部分的语调， 其实多半不是， 是的那部分太简单，疑问句升调，肯定句降调，掌握得很快， 没掌握的是单词的语调。 我想， 中式口音重点还是在这里， 这块消灭了， 就基本摆脱了中式口音，虽然离纯正口音还有差别，但原本我们的口音和三哥是半斤八两，这个解决后，肯定可以甩三哥，只是说口音问题，表达、运用又有其他问题了。","tags":"随笔","title":"对自己练英语口语的一些感想"},{"url":"http://sndnyang.github.io/2016-10-10-diary.html","text":"星期一 天气：晴 标题：时间过得太快 周一了， 没几天了， 这几天有点懒。 今日常规 单词 托福单词app今日任务完成。 zhimind 单词应用今天没动了，拆了出来。 口语 10分钟口语练习","tags":"日记","title":"2016.10.10日记"},{"url":"http://sndnyang.github.io/2016-10-09-diary.html","text":"星期天 天气：晴 标题：忘记写日记 今日常规 单词 托福单词app今日任务完成，若干GRE单词联想编写完成。 zhimind 快捷键和词库更新功能完成。 口语 10分钟口语练习","tags":"日记","title":"2016.10.09日记"},{"url":"http://sndnyang.github.io/NaoDong-list.html","text":"不指望创业， 当CEO\\CTO、CXO、联合创始人或普通员工都不适合我。但还是想做出点有重大意义的贡献，影响能有多深远有多深远的那种，当然，正面影响。 最近各种小想法小脑洞，为了避免遗忘，特意记下。反正每个想法肯定都有很多人想到过，我又不是爱因斯坦他们，也不是啥商业机密，目前基本都是教育的创意，偶尔有些医疗的。那如果创意有效，能起到那么一些作用，那肯定越早实施，越早有效果越好。而且创意归创意，有些创意不是自己能做出来的，只是脑洞。 知维图 知维图 就是我想出的第一个我觉得有重大意义的创意。详细想法有专门的地方在记，就不写这儿了。 个性化在线教育 其实是上面 知维图 的一部分，但现在机器学习、推荐系统感觉太合适了，所以写呗，也许学堂在线、中国大学MOOC们想到没想到试试？ 个性化也没个性到几亿的水平，规模最多就类似推荐系统协同过滤。甚至全世界人除了语言不同，教育个性上可能只有几百种不到，商品数量多多了。 关键是怎么把教育个性化、每个人的特点 给区分看来。如果无法描述的话， 听起来像图像一样，只管往深度学习里扔一下，也有个结果。 因为很多数据可以给教育学家、 心理学家拿来分析，哪个教育学的心理实验能有这么大量、无意识而自然的测试者，这些数据都是真实的（MOOC上长期作假没意义），问卷调查问别人学习习惯，自己不见得能准确说清，但他过往的学习过程不会骗人，所以如何区别化每个人的学习过程、 学习效果是关键。 MOOC没做到主要是同一门课程，基本只有视频一种教学手段，看PDF都下载看，根本不知道有没有看，视频下载了也一样。效果更加无从评估。就那点练习题， 也只管了个答案的， 怎么对的、怎么错的这过程根本不知道。 你说做不到？ 我觉得这不是做不到，像数学、理论物理、计算机可能想了解别人思考过程比较难，但帮助他们培养是可以的， 具体 过程是zhimind的内容，我现在也没做到，差挺远的。 主要就是想根据一些不同来做， 是看视频的、看文字的？ 视频或文字教程看了几遍？ 尤其是用我zhimind完整的想法的话，思考过程、思考方式就有同性，错的地方、 对的地方都是特征。 相比之下， 我真不知道 15年学堂在线预测退课率 有多大意义， 退课率这么高， 非退课那部分及格、掌握的可能也不高， 这数据就太不平衡了。而且对用户的偏好、帮助也不算大。 要帮助学生，归根到底一定是帮助他们： 更好更快地掌握知识 培养思维、 学习方法、 习惯 脑洞单词 脑洞单词 应该是少有的重在联想记忆法的背单词方案吧。其他背单词APP背重要、常用单词大概不错，但GRE这种偏门词求个认识，刷再要你千3000刷10遍以上的背诵方法~~~其他补充想法： 词根词缀是很有效，如果能找到比较权威的，把它取来就好了，一来版权问题，二来肯定英文的。 词根词缀记忆字典太不容易了，自己学习时拿它查，但不会用爬虫这样拿别人的成果。 另外偏门词偏偏有很多单词是最基本形态（非词根词缀组合），能联想到肯定是最好的。 把词缀去掉，理出变化线(例unprepossessing->prepossessing->prepossess->possess)。词根变化应该搞不搞 例句部分，太忙，只留了有道的链接。不然像爱词霸api里就有很多例句，还有整句的朗读——不过gre单词可能没这么多例句。 复习方式，原则上背单词应该有词->意，音->意，句->词，音->词等多种复习方式，但对GRE级别的词来说，看词识意就够了，其他方式没空做，没例句，而且选择题效果不好，填空题手机打字慢一点，全部采用询问-确认式就够了。 不靠谱医疗脑洞 -- 疾病查询系统脑洞 如果有疾病症状（外部表现）到疾病的关系的数据集，那是否可以用自然语言处理查询用户描述的病状，返回个可能的疾病列表。 纯粹靠用户描述返回一个非常不准确的结果。 老早就有这想法，但有点像现在蒙面唱将那个根据声纹猜歌手的机器人。准确率肯定比那个还低。 不使用其他数据，如果知道如何使用其他数据来诊断， 人工智能不说代替医生，最起码医生的工作量能极大降低。 文学（包换英语）阅读和写作练习方法 今天就是突然想到并确认这个创意才临时起意写这个文档的。 基于富兰克林自传里的重排序法。主要步骤： 前提步骤：取一段文章（不要太多句），拆成句子，但每句长度要合适，有些太长，有些太短，要适当处理。 方法一、 打乱，然后让用户重排序。这个重在结构的学习， 如果要强化文章段落结构学习，就来大段文本，以段为单位。 方法二、 去掉其中不相连的几句（或一句），让用户自己发挥，写完后和原文对比。重在语言的运用，完整写一段可行性不知。 分析： 好处是材料可以不需要人工处理（名著没有版权问题），不用人力参与评价，也能练习， 人的模仿能力是很强的， 说不定比老师讲些空泛的评价更有用。 担心有些用户瞎写、不写？不想写、捣乱的用户是有，但用户有没有收获自己清楚，如果用户经常来瞎填一气或交白卷，那是他乐意，怪我开发这个系统？因为有这种用户，所以所有的人都不要用这系统了？ 这系统不要开发了？只能继续家教一对一？ 隐约还有一些想法， 有点忘了， 所以写这篇很重要啊。 格物学习助手 -- 费曼技巧 格物学习 知维图 太难编写了， 等我写好教程或发展起来和高校名师合作，请名师编写， 不知道什么时候了。 所以先让个人用 费曼技巧 学习吧， 本来 知维图 和费曼技巧就比较近。 学习到的东西，要向计算机解释清楚， 就是这么厉害。 名字来由： 格物致知 一堆2012年的想法 12年9月中旬 因为参加腾讯的编程马拉松决赛，要求每人提一个创意。 其实我当时就不认为他们的方案能在水潭里扔石头泛起多大涟漪，同时我水平也很烂，马拉松是要组队做创意项目，虽然要求准备创意，但我真没意识到，我前端、后端、android、iphone没一个会的，都没学过，整个开发过程小透明，酱油中的酱油，偏偏我简历里不得不拿这个充实一下，太丢人了。之后又遇到点别扭，以至于我工作时有深圳和杭州可选，哪怕深圳会高几百，也没去深圳，因为公司就在腾讯旁边。 但那段时间确实有很多个小想法，其实倒是很多适合练手的。 像尊师，老师的标签+学生的评价+课程总结，类似MOOC学院（13年才建的吧），不过MOOC之前这种网站也有，但只有MOOC的热门课程能有学生评价。 像LBS巧遇，其实就是微信的附近的人，别人已经推出了，然而当时我没有微信。而且，巧遇要基于提前设定的标签，计算机匹配， 附近的人是来者不拒~~~今年或去年有类似的吧。 电子图书馆，就是思维导图，原来就是现在的zhimind的一个功能的原型啊！ 近日心情， 这个纯粹情感分析而已，毕竟当初没了解过，情感分析原来这么基础。 鬼脸、卡通， 这个没写说明， 不会是脸萌、 表情包那种吧—— 快乐大本营的穿越火线的电脑版， 就是用摄像头或体感控制物体过障碍别碰到。 不用道具就容易玩了——但我不知道有没有人已经开发了。 类似的还有一些，简单的也没练手， 难的太学术甚至要高端人工智能才行，像坐姿纠正、 手语识别。","tags":"创业","title":"创意即脑洞清单"},{"url":"http://sndnyang.github.io/machine-learning-catalog.html","text":"机器学习目录 自引用： 知维图系列-机器学习目录链接 思维导图 鼠标停在结点上时，如果有链接则会显示出来， 不然就看这个目录吧， 目录和导图转换还没有实现。 基础概念 机器学习定义 机器学习主要类型 机器学习三要素 线性模型 线性回归 线性回归基础 标准方程 概率解释 局部加权回归 逻辑回归 逻辑回归基础 牛顿迭代法 指数分布簇 广义线性模型","tags":"机器学习","title":"知维图系列-机器学习目录"},{"url":"http://sndnyang.github.io/2016-10-08-diary.html","text":"星期六 天气：晴 标题：脑袋里的想法控制不住 单词好了，是给别人用的？也不是，自己也可以用了。 今日常规 单词 托福单词app今日任务完成，若干GRE单词联想编写完成。 zhimind 在做单词app 主体完成， 修复优化。明天还要加下快捷键，忘了。 口语 10分钟口语练习","tags":"日记","title":"2016.10.08日记"},{"url":"http://sndnyang.github.io/2016-10-07-diary.html","text":"星期五 天气：晴 标题：代码写得太高兴了 今日常规 单词 托福单词app今日任务完成 zhimind 在做单词app 主体完成， 更新数据 口语 10分钟口语练习","tags":"日记","title":"2016.10.07日记"},{"url":"http://sndnyang.github.io/2016-10-06-diary.html","text":"星期四 天气：晴 标题： 今日常规 单词 托福单词app今日任务完成, 30个左右gre联想记忆法编写 zhimind 在做单词app有点效果 口语 10分钟口语练习","tags":"日记","title":"2016.10.06日记"},{"url":"http://sndnyang.github.io/frontEndFunctionImplement.html","text":"tabmenu切换 见map项目 recommendlist.html user.html 或 reciteWord.html ListTabMenu.css里纯css 实现。 #tab3 :checked ~ #content3 { display : block ; } 更好方案，使用 bootstrap, 见reciteWord.html <li class= \"active\" > <a href= \"#recite\" data-toggle= \"tab\" > 背单词 </a> </li> <div class= \"tab-pane fade in active\" id= \"recite\" > <a href= \"#start\" data-toggle= \"tab\" > 开始学习 </a> //再点另显示content </div> 导航栏颜色 active class bootstrap.css 加 map项目 layout.html 里 var pgurl = window.location.href.substr(window.location.href.lastIndexOf(\"/\")); $(\"ul li a\").each(function(){ if ($(this).attr(\"href\") == pgurl || ($(this).attr(\"href\") == '/index.html' && pgurl == \"/\") ) $(this).parent().addClass(\"active\"); }) 回到顶部 悬浮窗 map项目index.html 及 tutorial.js function backToTop() { //滚页面才显示返回顶部 $(window).scroll(function() { if ($(window).scrollTop() > 100) { $(\"#top\").fadeIn(500); } else { $(\"#top\").fadeOut(500); } }); //点击回到顶部 $(\"#top\").click(function() { $(\"body\").animate({ scrollTop: \"0\" }, 500); }); if ($(window).scrollTop() > 100) { $(\"#top\").fadeIn(500); } else { $(\"#top\").fadeOut(500); } } Load a database from the server sql.js load db var xhr = new XMLHttpRequest (); xhr . open ( ' GET ' , ' / path / to / database . sqlite ' , true ); xhr . responseType = ' arraybuffer ' ; xhr . onload = function ( e ) { var uInt8Array = new Uint8Array ( this . response ); var db = new SQL . Database ( uInt8Array ); var contents = db . exec ( \"SELECT * FROM my_table\" ); // contents is now [{columns:['col1','col2',...], values:[[first row], [second row], ...]}] }; xhr . send (); jquery click onclick $(document).ready(function(){ $(\"#ok\").click(function (){ }) }); 多个div 并排且居中 <div style= \"text-align:center;\" > <div style= \"overflow:hidden; display:inline-block;\" > <div style= \"float:left;\" ><a href= \"#\" id= \"ok\" > 知道 </a></div> <div style= \"float:left;\" ><a href= \"#\" id= \"fuzzy\" > 模糊 </a></div> <div style= \"float:left;\" ><a href= \"#\" id= \"wrong\" > 不知道 </a></div> </div> </div>","tags":"前端","title":"前端功能及实现代码对照表"},{"url":"http://sndnyang.github.io/zhimind-bmc.html","text":"商业模式画布 BMC 这里按business model canvas分析下 合伙人(key partners): 院校， 内容提供者 核心活动(key activities): 编程（AI技术、 机器学习），制作教学内容 核心资源(key resources): 教学内容 和 技术 价值定位(value proposition): 知识管理: mindmap 交互式学习教学内容， 以交互式教材（视频、课件）、 对话机器人、 虚拟现实 甚至 真机器人形式 个性分析、 个性化学习方案， 学生学习更详细、准确的数据。 用户关系(customer relationships): how to get/keep/grow？ 不了解这块内容。如果是问B2C/C2C/O2O模式，应该是C2C？院校提供优质教学、符合要求的资源， 渠道(channels): web 和 手机 细分用户(customer segments)： 也不太看得懂， 最初是针对数学、计算机甚至物理偏理论方面的学习者， 理想是任意知识领域。 用户一般是在线学习者，如果能和学校合作，就是全体学生。 成本架构(cost structure): 运营、 内容制作、 技术、 市场 收益流(revenue streams): 增值服务， 个性化学习方案 政府、社会资金支持， 学校购买，数据使用 广告 上面三条跟没说一样。 按思维导图里的完整一块知识点收费，不以整本为单位，免费看其中三四个知识点。 也可以对练习、习题收费，如果制作了习题库的话。 竞品分析 对比MOOC 优势： 应该更有效，学习效率更高，更能减少走神 更能与学校教育结合， 为学校所使用，而不是纯粹的在线教育 更准确的反馈数据。 劣势： 已经有若干MOOC平台， 很多资源(课程)， 很多合作， 很高知名度 模式未验证，很难抢用户 但因为MOOC是录制视频，并没有考虑互动性（Udacity的问题也设计得不好），差不多都是要制作新的内容。 MOOC浪费相当严重， 以中国大学MOOC平台为例， 微积分就有5版以上， 重复太多， 基本都是入门级，没有进阶。 现在MOOC陷入瓶颈期，需要新的方式和技术。 直播 优势： 个性化，可重复 某种意义上， 请最好的老师、学者，质量更高 互动形式多 劣势： 确实没有人那么聪明、 智能 在线一对一 优势： 成本低，不受人力资源限制 某种意义上， 请最好的老师、学者制作最高质量的教学资源（相当于写书），质量远高于一般一对一的授课者 劣势： 最即时的互动反馈 职业教育 冲突不大， 我给知维图(zhimind)定位更多是升级版MOOC，也可用于帮助学校教育，更偏理论学习、思维锻炼， 不太适用于职业教育的技能培养和锻炼。 可能对有一定理论的软件开发 学习也有帮助，但这块只占很小一块。 更新记录 20161005 初版 20170211 修改","tags":"创业","title":"zhimind商业模式画布"},{"url":"http://sndnyang.github.io/zhimind-bmc-20161005.html","text":"虽然不指望创业， 当CEO\\CTO、CXO、联合创始人或普通员工都不适合我。但还是想做出点有重大意义的贡献，影响能有多深远有多深远的那种，当然，正面影响。 之前小想法有，现在也还想搞个背单词软件。 zhimind 就是我想出的第一个我觉得有重大意义的创意， 那我得赶紧分析下，做个比较完整的原型产品出来（web加android），才能给别人展示。 能得到别人认可，投资创业是最梦想、做梦的愿望了；别人觉得好，拿去开发一个好的系统也行，虽然就不是我自己的成果了。如果真的觉得我这个创意、想法不成熟，不具备可行性，事实证明了如此，那也有个结论，我也就不抱侥幸了。 这里按business model canvas分析下， 无聊嘛。 合伙人(key partners): 院校， 内容提供者 核心活动(key activities): 编程（AI技术、 机器学习），制作教学内容 核心资源(key resources): 教学内容 和 技术 价值定位(value proposition): 知识管理: mindmap 交互式学习教学内容， 以交互式教材（视频、课件）、 对话机器人、 虚拟现实 甚至 真机器人形式 个性分析、 个性化学习方案， 学生学习更详细、准确的数据。 用户关系(customer relationships): how to get/keep/grow？ 不了解这块内容。如果是问B2C/C2C/O2O模式，应该是B2C为主？院校提供优质教学、符合要求的资源， 渠道(channels): web 和 手机 细分用户(customer segments)： 也不太看得懂， 最初是针对数学、计算机甚至物理偏理论方面的学习者， 理想是任意知识领域。 用户一般是在线学习者，如果能和学校合作，就是全体学生。 成本架构(cost structure): 运营、 内容制作、 技术、 市场 收益流(revenue streams): 增值服务， 个性化学习方案 政府、社会资金支持， 学校购买，数据使用 广告 上面三条跟没说一样。 按思维导图里的完整一块知识点收费，不以整本为单位，免费看其中三四个知识点。 也可以对练习、习题收费，如果制作了习题库的话。","tags":"创业","title":"zhimind商业模式画布记录-20161005"},{"url":"http://sndnyang.github.io/2016-10-05-diary.html","text":"星期二 天气：晴 标题： 做出一个完整的zhimind教程 昨晚想清楚了， 先做一个完整的zhimind教程， 暂定做 机器学习的 最热门。 做好之后， 才能展示给别人， 确认是否有效， 就算无效也算是有了个结果， 最烦躁的就是 不上不下的状态， 让人七上八下的无奈。 所以今天退了好几门课。 但英语不能丢， 因为不就 153+4 + 6.5 嘛， 搞定它们！ 计算机基础要看只看操作系统， 网络、数据库都相对没问题 今日常规 单词 托福单词app今日任务完成， gre单词整理20个以上。 zhimind 本来打算写个背单词的，zhimind的功能应该暂时OK了，填内容。 健身 俯卧撑、深蹲 口语 完成今日 英语流利说的打卡任务，但20分钟太长了，减为10分钟。","tags":"日记","title":"2016.10.05日记"},{"url":"http://sndnyang.github.io/bu-neng-qiu-quan-ze-bei-dui-jiao-yu-de-yi-ge-kan-fa.html","text":"求全责备的思维误区 我是在写 概率论的独立性教程时， 一时没收住，写了一堆话， 而且从一个话题讲到另一个话题，也是废话多。 现在我在开发 zhimind 就是 希望能做到 明确、刻意地告诉大家要锻炼哪些思维能力，并提供练习方案。 和练肌肉一样，明确告诉每个动作练哪里，而且练习方案也会进步。一开始不知道健身动作的能否起作用，以后也可能有更有效的动作代替，就不要练这个健身动作了吗？ 很多人就说，不知道优秀思维怎么培养，不知道在线教育或某个学习方法能否锻炼思维能力、提高天赋，甚至觉得天赋是天生的没法提高， 所以什么学习方法、资源都不屑一顾，不感兴趣就不学不练了？ 这就是个很明显对在线教育或学习方法的思维误区，体育锻炼还各种试试，思维锻炼却求全责备？ 还有其他不少相似的场景， 比如翻译英文技术博客或学习视频， 说翻译不好还是小事， 虽然我可能也郁闷，但我确实翻译得不好， 最起码我也不会反驳说\"你行你上\"。 真正恶心的是两种，其实是一种： 翻译得不好还不如不翻译 翻译干嘛， 直接看原版（基本上英文的）就得了 完全可以认为这是一种非常不尊重别人的行为，相比之下， 未经授权就转载起码还是别人认可你的文章。 尤其是第2种人，有本事看到是翻译的就找原文去啊， 有本事别看翻译的啊，跟看翻译版的读者说看原版就算了， 我是翻译者，这样说是多不尊重我的劳动啊。","tags":"随笔","title":"不能求全责备--对教育的一个看法"},{"url":"http://sndnyang.github.io/comment-on-pedagogy.html","text":"求全责备的思维误区 我是在写 概率论的独立性教程时， 一时没收住，写了一堆话， 而且从一个话题讲到另一个话题，也是废话多。 MIT 6.041x Introduction to Probability 这个是别人分享的视频，请看unit 2 的 lec3 的0、1、2三个视频。 或者中国大学MOOC上看那个，或随便找一本书。 我觉得以上（水平甩我甩出一光年）的问题在于，他们用的是证明的方法来给出概念，但证明是知道起点和终点的，他们等于是知道了独立性的结论、定义，才能有意识地沿着某条路走到终点。 但实际上， 新知识、新概念、新定义是不知道的， 不知道终点、不知道方向，怎么可能去探索？一是可能的路子有无数条，穷举都穷举不过来，怎么试？二是就算到终点也不知道停下来，很可能又走远了。 为什么这么说 用证明式的教学方法教概念，证明、推导能力也没锻炼多少，反正例子一个比一个简单，相反的，概念定义里的抽象能力，以及提出想法不断改进的探索习惯， 这两项的锻炼就非常少了。 为什么讨厌概念和定义，一来死背很恶心，二来我们自己没有这种抽象的能力和锻炼，不了解如何从感性认识上升到理性认识，把直观感受如何抽象成严谨的表达。同理可用于归纳。演绎缺的又是其他东西。 这些概念都是以前的科学家们提出来的，他们估计可能是无意识间锻炼了抽象的能力，也可能有刻意锻炼的。 继续吐槽 现在开发 zhimind 就是 希望能做到 明确、刻意地告诉大家要锻炼哪些思维能力，并提供练习方案。","tags":"随笔","title":"对一些教学方法的吐槽"},{"url":"http://sndnyang.github.io/2016-10-04-diary.html","text":"星期二 天气：晴 标题： 好好学点深刻的 啥都没有， 昨天 做了点数学题、 数据挖掘题（瞎选的），没看视频。 口语练了点， 单词背了点。 今日常规 单词 托福单词APP完成 zhimind 界面大调整","tags":"日记","title":"2016.10.04日记"},{"url":"http://sndnyang.github.io/2016-10-03-diary.html","text":"星期一 天气：晴 标题： 畏难做什么， 没有绝境 IT行业，何愁没工作，怕什么， 153+4.0算什么， 一个多月，绝对搞定！ 日记写得不错， 虽然深刻体会到写书的不易， 真废话多啊， 这还是完全没有图片的情况。 今日常规任务 数学， 主要是做笔记了， 其实没有学新内容。 单词， OK 口语， 英语流利说简单练了点， 有几句能上90分了，但整个口音还是明显的。 zhimind， 更新了少量内容，调整界面，公式的mathpreview位置， 标准答案的检查等等。","tags":"日记","title":"2016.10.03日记"},{"url":"http://sndnyang.github.io/Conditioning-Probability.html","text":"说明 本文参考以下文献, 比如说例子、习题，我怎么编？ MIT的MOOC 概率论-不确定的科学，在学堂在线开课版 链接 国防科技大学的 概率论与数理统计， 在中国大学mooc网站 链接 在下文笔较烂， 恐贻笑大方。 不过因为是markdown 写的， 有不足， 改进比较方便， 所以欢迎提出意见及建议，找出问题， 谢谢。 假设读者已经熟悉基本的概率概念、 概率几大公理， 掌握古典概率计算方法。 概率有条件 有M个人要抽N张入场券，若某人第k个抽，但在此之前已知前k-1个均未抽到入场券，问此时他抽到的概率是多少？ 与不知道前k-1人的状态时的概率相比， 是否有变化？ 毫无疑问是有变化的， 因为前k-1人是有可能抽中入场券的，现在都知道他们没抽中了，竞争者已经减少了，概率当然发生了变化。 上面这个例子貌似不够接地气， 换一个例子。 一个节目安排一人在街上随机搭讪行人， 这时搭讪成功的概率无疑是比较低的。但若已知搭讪者是大明星（特别是你喜欢的）， 此时成功概率肯定有飞跃。 所以，一言不合就问问题: 我们下一步要干嘛？ 继续举例 归纳 推导 演绎 建模 submit 条件概率 所以我们来建个模， 抽象化， 把事件找出来。 本例中有哪两个事件? 搭讪成功与否 搭讪者性别 被搭讪者性别 搭讪者是否明星 被搭讪者是否喜欢搭讪者 submit 定义符号 把搭讪成功叫事件A，明星搭讪叫事件B。 所以，最开始的是P(A)，搭讪成功概率， 之后加个条件， 是否明星搭讪B。 这就是条件概率的最直观定义了， 要定义下符号， 要去求出的搭讪成功A概率最重要，放前面， 是明星放后面，中间用个|分隔， 记成 $$P(A|B)$$ 就是 在明星搭讪的条件下，搭讪成功的概率。 条件概率无处不在 这个例子想想让人激动， 近距离接触明星（娱乐、学术、商业各领域）多激动啊。但我觉得还是不能体现出条件概率的意义与无处不在。 因为现实中用到的 多数的概率都是条件概率， 比如掷骰子和扔硬币也有条件，条件是它们是均匀的，没有出老千，只是教材里不是教大家骗钱，所以这个概率明显是1。 又比如天气预报， 存在什么样的条件概率呢？ 概念总结 对了， 比如预测的天气A和实际天气B间存在条件概率。 还有两种 在预测某某天气A的条件下， 实际天气B的概率。P(B|A) 在实际天气B的条件下， 天气预报预测是A的概率。 P(A|B) 通过这么几个例子， 希望大家体会到条件概率的意义， 它们无处不在，不过好像真没什么存在感，不刻意分析，不会意识到它们的存在。 所以， 条件概率就是在原有信息、概率基础上（搭讪成功率很低）， 得到新的信息、知识（搭讪的人是大名人，或男士看到美女、女士看到帅哥），得到的新的概率。 复习， 已知事件A和事件B 问已知事件A的条件下，B的概率是 (P(?)写完整) submit 条件概率公式 因为文本对公式表达的麻烦， 想看视频可以看： 国防大学概率与统计 第二周第五讲的第一个视频 MIT概率论 第二单元章节2(Lec 2)的第2个视频 以上需要注册加选课，所以没法直接给链接。找公开课的话，得定位到分钟、秒。 文本这部分， 我尽力而为。 开始 拿最开始的抽奖来做例子， 有m个人要抽n张入场券，若某人第k个抽，但在此之前已知前k-1个均未抽到入场券，问此时他抽到的概率是多少？ 与不知道前k-1人的状态时的概率相比， 是否有变化？ 由于一定原因，数学表达式全部使用小写字母 好习惯， 先抽象化、 数学化建模。 设A:\"第k个人抽到入场券\"，B：\"前k-1个人均未抽到入场券\" 不要偷懒，求P(A)= 预览: submit 题不要停 条件概率定义之前说过了， 现在就到你尝试的时候了， 目前还是古典概率。 求P(A|B)= 预览: submit 没完不要停 还没求完呢，事件B也是可求的， 对吧？ 因为表达式的复杂度问题， 定义写法， 组合M中取N的可能数 $$C_m&#94;n = {m \\choose n}$$ 请写成 (C(m,n)), M和N的组合函数C 大写。 为了公式显示正确， 需要写成 (分子)/(分母) 求P(B)= 预览: submit 推导分析 教材没有把这一切当宝贝，毫不重视地把结论扔给我们，这肯定不好。现实中， 这一切都藏得很深， 都是宝贝， 所以我希望能锻炼这种找宝贝(探索)的习惯、 思维和能力。 刚才通过古典概率算出了 $P(A)=\\frac{n}{m}$和$P(A|B)=\\frac{n}{m-k+1}$，还有个$$P(B)=\\frac{{m-k+1 \\choose n}}{{m \\choose n}}$$。 这三个式子看起来毫无关系，我们要如何探索、如何试呢？ 没有头绪， 瞎试、瞎组合， 各种情况都试一遍， 砍掉解释不通的、 复杂过头的。 3个式子选2个进行组合，有几种可能 1 2 3 4 5 6 submit 从简单开始 A和B事件组合， 不外乎且和或嘛， 这里直觉觉得或or不出来， 只考虑且and。 所以我们要 考虑事件$P(A \\cap B)$ 。 同时我们有 P(A)*P(B)，不好写， 不写了，就是这么任性。 $P(A \\cap B)$ 可以算出来吧？因为这种表达式的化简问题，请化简成两个组合数相除，不要几个相加后再除。 老样子，组合M中取N的可能数 $C_m&#94;n$ 请写成 C(m,n), M和N的组合函数C 大写。 P(AandB)= 预览: submit 找到答案了吗？ $P(A \\cap B)$==P(A)*P(B)? 对 错 submit 继续试 不好意思， $P(A \\cap B) \\ne P(A)*P(B)$，继续试， 这回试谁和谁的组合呢？ A B A|B submit 成功 先分析， A和A|B， 一个是A的概率， 另一个是已知B发生的条件下A发生的概率， 都是A的概率， 明显凑不上。 PASS. 只剩下 B和A|B， 先看结果 $$P(A \\cap B) = \\frac{{m-k \\choose n-1}}{{m \\choose n}} = \\frac{n}{m-k+1} * \\frac{{m-k+1 \\choose n}}{{m \\choose n}} = P(A|B)* P(B) $$ (应该是个排列组合定理或很好的推导练习机会，但没计划写) 再从语义上分析， P(A|B) 是已知B发生的条件下A发生的概率, P(B)则是B发生的概率， 所以没错， 这俩相乘就是$P(A \\cap B)$。 所以我们就得到了结论： $$P(A \\cap B) = P(A|B) P(B) 或 P(A|B) = \\frac{P(A \\cap B)}{P(B)}, P(B) > 0 $$ 这么简单、显而易见的公式， 我显然扯了这么一大段， 是不是太罗嗦了？ 是 否 submit 概率公理 这部分一般显然了， 条件概率也是概率，那些基本的概率公理照样成立， 证明过程再写我要疯了~~~找理由、编理由很难的！ 特别是很多事都有默认， 平时根本没想到， 没错就忽视，错了再改， 所以根本没理由。 恭喜 这就是我写的条件概率内容， 已经很罗嗦了， 觉得想继续的， 请看下一篇 乘法法则","tags":"数学","title":"条件概率"},{"url":"http://sndnyang.github.io/Conditioning-Probability-Multiplication-Rule.html","text":"说明 本文参考以下文献, 比如说例子、习题，我怎么编？ MIT的MOOC 概率论-不确定的科学，在学堂在线开课版 链接 国防科技大学的 概率论与数理统计， 在中国大学mooc网站 链接 在下文笔较烂， 恐贻笑大方。 不过因为是markdown 写的， 有不足， 改进比较方便， 所以欢迎提出意见及建议，找出问题， 谢谢。 假设读者已经熟悉基本的概率概念、 概率几大公理， 掌握古典概率计算方法。 乘法法则 从上一篇 条件概率 里，我们已知$P(A \\cap B) = P(A|B) P(B)$， 很明显换一下条件也成立 $P(A \\cap B) = P(B|A) P(A)$， 说难听点， 也就是把符号换了下~~~当然事件条件反过来也行，就像 预报条件下的实际天气概率，和实际天气条件下预报的概率。 这个乘法法则已经是有意义了，一般人算算两个事件间的概率还好，3个事件累，4个事件烦了，5个以上摔笔了。 来看例题， 来自国防科大概率论与数理统计 第二周第五讲第三个视频 例题 在无放回的情况下， 求从r只红球和b只蓝球的袋中摸出两个红球的概率。 罗嗦一点， 用排列组合，r只红球选两个的目标方案数 除以 r+b只球里选两个的总方案数。 $$\\frac{C_{r}&#94;2}{C_{r+b}&#94;2} = \\frac{r(r-1)}{(r+b)(r+b-1)}$$ 就这样嘛， 不就得了？ 能用就好了呗， 不好意思， 条件概率的乘法法则 就希望避免掉计算组合数。 那怎么办？ 不想看这么罗嗦的东西了，关掉 跟条件概率关联上 分两次求概率后相乘 submit 哪有条件概率 不就两次取球吗， 第一次是 $r/(r+b)$的概率， 第二次不就是 $(r-1)/(r+b-1)$ 吗， 两个相乘就得了。 这不常识吗？也跟条件概率有关系? 有 没有 submit 条件概率没有存在感 对吧， 还真有， 第二次不就是基于第一次成功的条件嘛。 所以说， 条件概率特别是乘法法则可能真是太没有存在感了， 更像一种常识。 目前可能有点简单，本来的常识就是 看山是山看水是水，强行引入条件概率的思考方式，变成了看山不是山，看水也不是水，整个人做题时就怀疑人生，觉得题目怎么绕成这样， 到最后习惯了，又到了 看山还是山，看水还是水。但是， 你人已经不一样了。 上面这段只是吹的， 条件概率有没有那么夸张，我并不知道。 下一步呢？ 分析事件的独立性 归纳乘法法则 submit 推广 我们肯定不满足于此，n=1,2都是特例， 一定会想着归纳、 推广到任意n，不管是费马大定理还是1元n次方程求根公式， 只不过是有或是没有，证不证得出来。 没结果也要有过程， 要有这颗探索的心。 还是不放回的取球问题， 之前只是取两个， 推广到取任意n(n<=r)个球是红球的概率（原题更一般化）。 用排列组合，也行，不介绍。 n次取球， 也简单， 计算不难， 要把它写出来， 这个表达式要判断规则太多， 就直接我写了。 $$P(R_1R_2...R_n) = P(R_1)*P(R_2|R_1)...P(R_n|R_1R_2..R_{k-1})$$ 一环套一环，第k次取是前面第k-1次相乘的结果。 用类似的示意图(MIT概率论截的图) 乘法法则树 所以这个是个很明显的结果， 能直观理解——我也没找到啥证明， 就这样，完整的乘法法则的学习就告一段落了。 恭喜 这就是我写的乘法法则内容，感谢阅读。下一篇? 还没写呢 独立性","tags":"数学","title":"条件概率乘法法则"},{"url":"http://sndnyang.github.io/independence-foundation.html","text":"说明 本文参考以下文献, 比如说例子、习题，我怎么编？ MIT的MOOC 概率论-不确定的科学，在学堂在线开课版 链接 国防科技大学的 概率论与数理统计， 在中国大学mooc网站 链接 在下文笔较烂， 恐贻笑大方。 不过因为是markdown 写的， 有不足， 改进比较方便， 所以欢迎提出意见及建议，找出问题， 谢谢。 假设读者已经熟悉基本的概率概念、 概率几大公理， 掌握古典概率计算方法。 独立性 在以前章节 已经学了 条件概率 和 乘法法则 学了 没学 submit 概念引入 学了条件概率、 乘法法则， 有什么感觉吗？ 估计也没什么感觉， 很直观嘛， 现在穿越回去， 我也能当数学家——不对， 还得加强下英文、法文或德文。 而且条件概率包打天下， 无所不能吗？ 中国乒乓球队夺不夺冠跟国足出不出线 存在条件概率关系吗？是个中国人都知道，没什么关系。 这样我们就需要一个新的概念，事件的独立性，直观含义很明显了，接下来就是抽象成数学概念， 用数学来表达， 并介绍一下应用等等。 常见讲解 像国防科大基于生男孩生女孩概率这种计算简单，得出结论。好处是简单，坏处是太简单， 容易让人觉得，复杂情况会不会出错。 MIT沿着一棵概率生成树把条件概率、乘法法则、全概率公式、贝叶斯法则复习一遍，最后得出结论。 好处是复习了， 但说这种就融会贯通了？这例子也不是十分复杂， 靠这就融会贯通，简直超神了。 其他教材估计也差不多， 类似。甚至连例子都没有，直接给结论的 MIT 6.041x Introduction to Probability 这个是别人分享的视频，请看unit 2 的 lec3 的0、1、2三个视频。 或者中国大学MOOC上看那个，或随便找一本书。 你觉得他们的讲法很好吗(就这个概念引入而言)? MIT当然是完美的 久经考验的讲法当然很好 不好 垃圾，渣渣 submit 试着去抽象 我觉得以上两家（水平甩我甩出一光年）的问题在于，他们用的是证明的方法来给出概念，但证明是知道起点和终点的，他们等于是知道了独立性的结论、定义，才能有意识地沿着某条路走到终点。 现在我们不知道结论，不知道定义，没有一丝头绪，不能用证明的方法来搞这个。 我们要从直观思维提炼出严谨的表达， 从一个简单的抽象开始， 然后不断精炼。既锻炼对事物进行抽象的能力，又培养不断探索、不断改进的习惯。因为科学理论一开始基本是不完备的，后面慢慢改进，不要习惯上觉得出错是丢脸的事。 这个过程可以叫 科学方法(scientific method) 好像也要系统方法(systematic method) 链接见 scientific method steps 抽象第一步 某某那个酷，控制不住自己说广告词。 从直观开始，独立性是——两个事件A和B没什么关系，A、B发生的概率互不影响。 什么意思呢？不影响 -> 不变化 -> P(A) 或 P(B) 不变 P(A)相对谁不变？ 预览: submit 有假设了 我们就得到一个结论, 两个事件独立，意味着 P(A) = P(A|B)。 当然P(B) = P(B|A) 也对。 反向不成立的情况存不存在？就算存在也超纲了。 有假设下一步干嘛？ submit 例子验证 验证 P(A) = P(A|B)，省略， 请看上面的课程， 反正就一两个例子， 也没特别的证明过程。 猜猜完了吗？ 完了 没有 submit 完了？ P(A) = P(A|B) 挺好的，但会不会有什么问题？ P(B|A)=P(B)一定成立? 不够直观 不够简单 submit 再归纳、更一般化 没错， P(A) = P(A|B) 总会让人怀疑它只表示了\"A独立于B\"，那 P(B|A)=P(B)么？ 独立性要的是AB互相独立（默认）。 所以式子里的A和B要同等的地位。 然后我也编出来了。 唯一可以变形的公式是 $P(A \\cap B) = P(B) * P(A|B)$ 代入，得一个标准的独立性定义： 若$P(A \\cap B) = P(B) * P(A)$, 则称事件A，B独立(independent)。 推广 若A与B独立， 则A与B逆， A逆与B， A逆与B逆都独立。 推广到n个事件 也一样。 n事件独立 猜猜完了吗？ 完了 没有 submit 互斥与独立 首先， 先来个问题， 两互斥事件 独立吗？ 独立 不独立 submit 分析 目前没能实现用户完成多步推导的 指导功能。要在大文本框里写很多行， 各行去判断——哎哟喂。 就根据独立性的定义， 不外乎算两个， P(AB) 和 P(A)*P(B) 互斥事件P(AB) = 0, P(A) P(B) 两个都是概率事件P>0, 所以 P(A) P(B)>0。 当然大家应该能理解，只是需要这么明确地说出来，不然直观认识是很可能模糊的。再来解释下。 从概率上说， 俩事件互斥其实就是一种信息， 知道了A或B的成立，就知道B或A 一定不会成立。 我们又可以看到条件概率的意义， 知道新的信息后， 概率发生变化，也可以说是观点发生变化。 猜猜完了吗？ 完了 没有 submit 恭喜 独立性的基础部分大概就这么多。后面还有： 条件独立 多事件独立 应用分析事例","tags":"数学","title":"独立性基本概念"},{"url":"http://sndnyang.github.io/2016-10-02-diary.html","text":"星期天 天气：阴 标题： 坚持写好zhimind笔记 不能认真、 深入地学习永远不会进步， 总是各种畏难。等着天上掉馅饼？ 只会天上掉陷阱。 看英文课程挺好的， 最起码可以练听力啊， 口语什么的还是要专门练。写作也是。 今日常规任务 数学， 又做了两道题， 没怎么看视频 机器学习， 看了下 udacity的机器学习课程， 也是各种觉得简单，却不认真看、 分析。","tags":"日记","title":"2016.10.02日记"},{"url":"http://sndnyang.github.io/probability-foudation-concepts.html","text":"说明 本文参考以下文献： MIT的MOOC 概率论-不确定的科学，在学堂在线开课版 链接 在下文笔较烂， 恐贻笑大方。 不过因为是markdown 写的， 有不足， 改进比较方便， 所以欢迎提出意见及建议，找出问题， 谢谢。 前提条件 假设读者起码熟悉初高中的数学知识， 知道函数、映射这些基本概念。 所以， 函数是什么？ 大概就是把一个空间按某个法则映射到另一空间， 实例点就是把一个矩阵（向量，数）映射到另一个矩阵（向量，数）。 函数的几个要素 输入 输出 映射规则 定义域 值域 submit 概率课程总览 显然，概率模型也可以表现成函数映射关系。存在以下三种对应关系： 输入： 各种离散或连续的，一元或多元的变量。 输出： 各种情况下的输出概率likelihood及 可信度 belief 映射规则： 如概率公理及其推论 所以， 概率课的大纲往往按照总分结构，先抽象后具体，先给抽象的总体性概念，接着是理论基石--条件概率、概率公理， 中间掺杂点约等于基础组合数学的古典概率， 主体总分是对各种具体概率分布类型的介绍。 配上统计课， 统计课则是把概率课学到各种概率分布应用到具体数据上进行分析，求出实际的概率分布参数及参数的可信程度。 基本概念 对照函数的三个要素，我们来看看有什么基本概念需要我们先行了解。 首先来看输入部分，输入的是各种变量，所以貌似没有特别的概念，具体内容将在以后的章节中进行讲述。 输出部分，我们知道函数的输出有值域，对应到概率上就是可能是输出结果的集合， 另外， 因为是概率，所以每个输出值还要对应上一个概率值。 我们可以把 所有可能的输出的集合 叫做 样本空间(sample space)，虽然说样本这两个字习惯上跟输入的关系更密切，但这样的翻译已经约定俗成，就这样吧。 输出样本空间 元素互斥性 mutually exclusive collectively exhaustive 应该是指完全列举， 所有可能事件的集合才是样本空间，哪怕无数可能性缺了一个也不是。实际输出也必然属于样本空间。 此处应有样本空间例子 映射规则--概率公理 正如实数连续统之于数学分析， 欧几里德4+1公理之于那各种几何， 概率同样也打造了坚实的公理基础， 精简有用。 先声明，公理只是映射规则的基础、基石， 并不是映射规则只有这么点。 因为我们不需要求样本空间的概率（就是1）， 一般来说是求他的子集的概率，所以我们把样本空间的一个子集称作 事件(event)。 求事件A的概率记为 P(A). 接下来给出概率的这几条公理： 非负性： 事件发生概率大于等于0。 P(A)>=0 (P(A)<=1) 标准性： 样本空间的概率为一。 $P(\\omega)=1$ 加法公理： $if A \\cap B = \\emptyset, then P(A\\cup B) = P(A) + P(B)$ 最典型例子就是掷一个骰子，不同点数概率相加就是这条公理的体现。 另外$if A \\cap B = \\emptyset$, 我们把A、B叫做互斥事件(disjoint events), 一开始需要和A\\B独立区分开来。 一些推论及性质 由以上三条公理， 我们可以推导出以下： P(A) <= 1 P(空集) = 0 P(A) + P(A的补集) = 1 $P(A \\cup B \\cup C) = P(A)+P(B)+P(C)$, 对k个互斥事件都成立 $if A \\subset B, then P(A) <= P(B)$ $P(A\\cup B) = P(A) + P(B) - P(A\\cap B)$ $P(A \\cup B)$ <= P(A) + P(B) $P(A\\cup B \\cup C) = P(A) + P(A&#94;c\\cap B) + P(A&#94;c \\ cap B&#94;c \\cap C, A&#94;c B&#94;c 是指补集complement$ 以上推论可以自行推导， 因为还是比较直观， 理解记清能用到就行。 连续样本空间概率计算 例如平面上选取一点或一直线、 直线上选一点， 因为都有无数个样本，所以概率 这块非常直观， 样本空间对应总面积是1，概率100%， 那么事件所占的长度、 面积、 体积比例就是概率了。 需要有个前提条件是满足均匀分布， 单位长度、 面积、体积的概率都相等。不是射箭比赛的分数， 中心分值高。 可数相加公理 之前的加法公理是在有限集上的， 无限集要扩展成新公理——不是定理，没有逻辑推导过程。 可数相加公理： 如果$A_1,A_2,A_3...$是 一个无限的互斥事件序列，则有：$$P(A_1 \\cup A_2 \\cup A_3 ...) = P(A_1)+P(A_2)+P(A_3)+...$$ 然后扯了一堆 序列代表事件是可排序的，这个公理只能适用于事件序列， 像平面里的点或线并不存在顺序关系， 该公理不适用。所以不能把点（线）的概率相加（一堆0相加等于0）得到平面的概率， 所以 平面上的点（线）是不可数的。最后是面积是可数的，满足可数可加性。 还提了下这块是测度论的内容， 在概率论范围内不需要考虑这种事。 恭喜 我笔记做得不认真， 太过敷衍， 推导不到位。 惭愧惭愧。","tags":"数学","title":"概率基础概念"},{"url":"http://sndnyang.github.io/2016-10-01-diary.html","text":"星期六 天气：阴 标题： 不要瞎抱期望 反正我也没创业的想法， 有什么创意做出来就是了，一点点完善，期望太高虽然也好，但还是要看自己能力的 今日常规任务 单词 搞定 中文阅读朗读 一篇 英文阅读朗读 雅思三篇阅读，没有遇到匹配题，基本只错2个，可以考虑GRE阅读了。 机器学习 学堂在线的数据挖掘看了点，偏入门，对深层原理没作深入分析，题目就更少了。 听力 好难好难 ~~~","tags":"日记","title":"2016.10.01日记"},{"url":"http://sndnyang.github.io/2016-09-30-diary.html","text":"星期五 天气：阴 标题： 心不静 今天常规 除了搞了点单词， 做了点数学题， 一事无成。","tags":"日记","title":"2016.09.30日记"},{"url":"http://sndnyang.github.io/2016-09-29-diary.html","text":"星期四 天气：阴 标题： 把握重点 该练习写作文了！zhimind 花了很多时间了。 今天常规 单词 10：05 听力 剑雅4 TEST2 整套模考，掉到5.5， 确实有点难， 听写第4部分疯了， 语料库听写 zhimind 给数学表达式部分进行了一定的改进， 包括界面和功能。 矩阵运算式子就用去空格后相等。 但如果有异常的话， 还是要基于 edx和sympy做个开源项目出来。 健身 俯卧撑3组？","tags":"日记","title":"2016.09.29日记"},{"url":"http://sndnyang.github.io/data-preprocessing-in-dm-xuetangx.html","text":"为什么要数据预处理？ 真实的数据非常脏（混乱、复杂） 不完整的 incomplete 噪音 noisy 不一致 inconsistent 冗余 redundant Others Data types imbalanced datasets 主要内容 数据清洗 数据缺失 离群点检测 重复检测 数据转换和采样 数据标准化 特征选择与主成分分析 数据描述与可视化分析 数据清洗 数据缺失 数据缺失的 可能原因： 设备故障 无数据 不适用——如体检男女项目不同 不同的类型： 完全随机缺失 条件随机缺失--概率跟某些属性有关， 比如女生相对不愿透露体重 非随机缺失 处理方式 忽略 手工补充 自动填充--全局参数、 均值或中值、 最可能的值 练习 数据缺失python练习(未实现) 离群点outlier 一般思路， LOF(local outliner factor)， 点与若干相邻点的相邻距离， 与 相邻点自己的相邻距离相比较 离群点检测python练习(未实现) 重复数据 duplicate 使用滑动窗口技术， 前提是相似、重复的数据是相邻存储的 需要一定的人工分析 重复检测python练习(未实现) 类型转换 转换工作主要有： 类型转换(type conversion)，如离散和连续值 正则化(normalization)， 使用相同的scale 采样(sampling) 类型 连续 离散 序数(ornial)，如点评的普通、好、非常好 纯名词， 标签。方法比如 One-Hot Encoding One-Hot编码,又称为一位有效编码。 字符串 采样 统计是获取数据成本高。 数据太多， 减少数据 调整分布， 不平衡数据的重新采样 聚集 aggregation change of scale: 城市聚集到省， 日期聚集成月份 更多稳定性， 更少变化 不平衡数据集 使用新的评估方式: $$F-measure = \\frac{2 \\times Precission \\times Recall} {Precision + Recall}$$ $ Precision = \\frac{TP}{TP+FP}, Recall= \\frac{TP}{TP+FN}$ 其他方法 over-sampling boundary-sampling 标准化 使用相同的scale比例尺， 避免特征上的不平衡。 主要方案 最小最大标准化（0-1标准化）： $v'=\\frac{v-min}{max - min}$ z-score标准化（针对正态分布无上下界数据）:$v'=\\frac{v-\\mu}{\\sigma}$ 数据描述与可视化 描述方法 算术平均值 mean 中位数 median 某模式：如次数、频率最高的值 方差（变化率） 相关度 皮尔森pearson相关系数： $$r_{A,B} = \\frac{\\sum(AB)-n\\hat{A}\\hat{B}}{(n-1)\\sigma_A\\sigma_B}$$ pearson chi-square test: $$\\chi&#94;2 = \\sum\\frac{(Observed - Expected)&#94;2}{Expected}$$ 可视化 可以推荐课程， coursera可视化 虽然好像太理论？ 特征选择 属性太多，也要\"采样\" 那么什么是好的属性？ 介绍熵(Entropy)、 信息增益(Information Gain)等 决策树也用的技术 [决策树(待编写)]((http://sndnyang.github.io/404.html) 用于评估特征 特征的搜索 确定性： 穷举exhaustive 分支限界branch and bound 启发式： top K 序列前向选择sequentail forward selection，已知k个最优， 试k+1最优 序列反向选择sequentail backward selection 特征提取feature extraction 主要是 主成分分析 principal component analysis 主成分分析 基本就是线性代数的 特征值、特征向量[特征值(待编写)]((http://sndnyang.github.io/404.html)求解， 好像奇异值分解[奇异值(待编写)]((http://sndnyang.github.io/404.html)也行。 线性判别分析 linear Discriminant Analysis","tags":"机器学习","title":"数据预处理"},{"url":"http://sndnyang.github.io/2016-09-28-diary.html","text":"星期三 天气：阴 标题： 时间过得快， 学习要进步 学的内容多， 要做的东西也很多~~~数学不容易， 机器学习不容易， 英语不容易， 开发也不容易。 这几项一起学， 确实挑战。 今天常规 单词 8：15 听力 14：46 剑雅4，test1 section4 第一次一篇全对！！！听写也还好，不过各种略读确实难认。 英文阅读 剑雅5 test 1 section 2。接昨天，做过， 13题错2， 一个算是看错题（社会生物学研究的目的，我以为是这个实验的目的），一个还是看错题（一些学生是否是耶鲁大学学生，我以为是所有了） 英语 口语训练 16：10 走遍美国+网易口语大师 中文阅读朗读 一篇 话题写作 中文 话题写作 中文 数学 计算机基础 机器学习 音乐 9：05 中原大学 音乐基础训练. 四个基本元素， 节奏、旋律、和声和音色， 主要是和声了解了音程和和弦的基本概念。 zhimind android对思维导图样式的处理完成！！！22:18 健身","tags":"日记","title":"2016.09.28日记"},{"url":"http://sndnyang.github.io/2016-09-27-diary.html","text":"星期二 天气：阴 标题： 猜得很准， 很失败 GRE成绩出来了， 作文确实是2分， 基本可以肯定要重考了。 怎么准备， 真是~~~没办法， 还是找个工作蹭点工资吧。 今天常规 单词 8：55 中文阅读朗读 23：22 《假如今天是我生命中的最后一天》， 这种文章略鸡汤。 《匆匆》，多年后的再次阅读 英文阅读朗读 22：36 13题对9，之前做过一遍，NOT GIVEN类错最多。 学到一个是非题的顺序原则~~~也对。照顺序，在上下题间找同义句，完全没提到是not given，另外是同义和反义判断。 数学 20：22 热身 机器学习 今天没有学习， 为了工作， 得赶紧学机器学习和深度学习， 练代码了。 音乐 23：31 音乐导聆 李叔同 送别—— 原作曲哪位现在就忘了。 还是 一壶浊酒尽余欢， 壶字比瓢字好听，又比觚字好用， 虽然喝一壶酒确实太多了。 zhimind 17:21 尝试了下html来搞android，继续卡在样式上了。 健身 20：40 书包当哑铃， 另外深蹲","tags":"日记","title":"2016.09.27日记"},{"url":"http://sndnyang.github.io/20160926ri-ji.html","text":"星期一 天气：晴 标题： 千里之行 今天8点出门， 直到17点才到， 足足9个小时。 8点10分出门， 路上有点堵， 9点10几到， 花了一个小时。 提前了一个半小时到机场， 本来应该10点50起飞的， 拖了10几20分钟， 飞行时间2小时， 下午1点出头到贵阳， 3点的班车， 再有2个小时， 终于在5点到家了。 1个小时的公交车， 2个小时的候机时间， 2个小时的飞行时间，2个小时的班车等待时间， 2个小时班车行驶+县城公交步行时间。 其实已经很便捷了， 但我还是嫌麻烦， 比如候机有点久，没带大件行李，直接自助取票就可以安检，时间上再精确点就能节约点。另外，如果不是住在小县城、班车太少，就不用等2个小时了。 今天常规 英文听力 21:18 单词 10:15 日记总结 23:13","tags":"日记","title":"2016.09.26日记"},{"url":"http://sndnyang.github.io/20160925ri-ji.html","text":"星期天 天气：晴 标题： 千里之行，始于足下 今天常规 英文听力 完成 17:23 音乐 23:42 音乐导聆第一章第一节，古琴与士, 了解古琴的发展历史 练字 执行中 单词 20:43 完成 健身 21:00 完成， 腹肌锻炼，仰卧起坐、卷腹 日记总结 23:43 图书馆借了本薄书， 学堂在线出毛病了， PC上很难打开， 翻墙反而能打开。","tags":"日记","title":"2016.09.25日记"},{"url":"http://sndnyang.github.io/liu-xue-ying-yu-kao-shi-gan-xiang.html","text":"昨天上午12点， 雅思笔试结束。 出来，主要感想： 终于考完了！ 考得太差了！ 准备太不充分了！ 估计要二战了！ 钱啊！ 钱啊！ GRE和雅思成绩结果及预测： GRE V:149, Q: 168, 总分:317, AW(预测):2.5或3 雅思预测 口语:5或5.5， 听力：6,6.5，阅读：6.5,7， 写作：5.5,6。 但需要 6.5的均分 不知道在网站上写的 153, 155, 4.0 和 6.5雅思 是不是固定死的， 不然我两个都是一定要重新考了。一般来说应该留有余地， 那GRE我的aw如果有3， 估计也不用再考了。雅思仍然不清楚，没有估分经验， 万一总分才5.5那肯定不行， 但6分不知道可不可以。 备考感想 以下才是正文。 在8月中旬差不多8月20号，突然发现尽快考出成绩，也许还能申请春季入学。于是一开始是报了9月4号的GRE 和 准备报9月10号的雅思。纠结几天后，发现不可能， 把GRE推迟到9月17号， 雅思定到9月24号。 总之， GRE的准备时间大概1个月， 雅思多一周。 然而现实给了自己狠狠一耳光——终究没有认真学习的习惯，仍然在经常在无所事事中浪费时间。 GRE备考状态： 刷单词 前期用 再要你命三千 刷词， 同时也在看托福词汇。刷词就是纯粹地看一下，记一下，速度估计是1分钟1个list 10个单词， 10分钟就1个单元100个单词， 一个小时多看500、600单词， 当然这是巅峰速度。 记忆质量毫无疑问的差，但估计也能对不少词混个眼熟。 但还是有不少日子没有背单词或看得极少，严格来说，我只刷了两遍 3000，后来因为对部分单词（六分之一左右）了解已够， 不想多看， 在收藏夹 和 正常刷词间徘徊，几乎停止刷单词。 后期因为记忆效率和质量问题，改采用联想记忆法，一度效果不错，但剩余时间不多， 联想不易， 又试图从 词根、词缀（比如看了英语词根与单词的说文解字）协助记忆， 这番纠结下来， 没时间了。 总结： 3000+magoosh 已经很精简了， 背GRE 有点忽略了相对基础的词汇， 在GRE考试中选择短语有点模糊而错误， 背的效果也不好。 有时间准备， 一定是基于词根词缀（未来扩展性强） 辅以 联想记忆法， 比如词根及非派生词， 只能用联想记忆法了。 所以，其实也计划把 3000 和 magoosh 用词根词缀和联想编写好后，做个APP。一直想做个背单词的应用， 主要是受限于素材（例句等）。 verbal 基本上只是做点模考，熟悉了下题型， 没有做超过8套题。没有看过任何技巧总结，因为我觉得看懂就是看懂，单词、短语知道是什么意思，选词填空题就基本没问题，这部分最主要的还是单词和短语看不懂，句子意思因结构和语法不理解的在少数。 当然， 填空和阅读都涉及阅读速度、理解速度。习惯性回跳重读、精力不集中， 非常影响速度。 但这个平时积累、练习就好， 并不需要刻意针对考试。 而阅读的考试技巧不能说是无稽之谈，但我真得读得又快又好的话，不需要技巧这种套路，临时分析都行。能自己从分析中显式地总结技巧、 套路是最有效果、最出类拔萃的。 数学 虽然从来没满分过， 但也从来不觉得有什么问题， 除了看花眼， 以及不想检查。 作文 作文有题库。 但我从头到尾， 一篇都没写过， 提纲也没看过， 等于是裸考。 实际考试中，遇到的issue题目正好在一本书上看到。不过书上是强调学生对题目分析上的错误（群众对官方、当局的质疑，能提高福利），应该没有附范文。 就算有范文，我也没背，不能抄， 还是裸考。 几乎没有看过题库， 提纲一篇都没写过， 完整作文更是没有写。 说到底就是浪费了太多时间。 雅思备考感想 雅思考试就是GRE考试的下一周， 挺紧张的。 本来差点计划9月10号考的， 但一开始计划是回家呆着，然后到时间去广州考GRE，然后回厦门就等着雅思， 所以推到24号。结果又不回家了~~~成绩出来太晚了，因为国庆，要到10月21号才出。 早知道， 我就10号考了， 早一点就早一点练习而已，也没有多出多少练习时间。 听力 托福都是选择题，没听到都可以逻辑或蒙个选项。 雅思的填单词类的题目就不要指望了， 没听到就放弃吧。 所以我第一次模考 听力4.0分， 阅读7.0。 训练大概就是在网上（比如考满分）去听写。 考满分网有好的地方，完整剑雅的素材（沪江听写就比较乱）， 速度可调， 在1.0凑合的情况下，我直接听 1.5 倍速。反正他一旦略读太多， 0.8我也听不出来， 各种that和介词听不到。缺点就是，比如一句话有多少个词， 他就摆好多少个框，而且一填完就知道正确与否，就容易拿词试框， 忽略整体。 另外它上面的语料库——主要是练习听力单词——很有意义， 也方便使用， 但有些单词发音不准，另外没有上下文单纯听单词发音，会多犯很多错误。 阅读 同GRE，没怎么准备， 模考而已，第一次模考，听力4分，阅读7分， 后面两次也都没涨分，倒是看了下所谓head题， 并不是我以为的段落总结、中心思想， 只是找到段落里同义句而已。 写作 也是基本没准备， 简单地看了下其他范文，了解下结构而已。 实际考试里， 小作文还行， 大作文字数好像不够。 口语 简单地练了下语音， 口语话题也有题库， 做好准备的话，也应该在考前把所有话题做个提纲甚至打个底稿。 然而我也没有准备， 只是考前（考场外面等待的一两个小时）看了下高频话题， 发现 有趣的公共场所、 首次尝试的某种食物、 运动等都不会~~~ 实际口语考试中， 各种错误百出就不说了， part1还问到了 celebrity，可是我没听出来，不确认是不是名人。 等我出来一看题库， 还真有celebrity， 本来我准备了part2 被问到 peole I admire的。 总结 没有好好准备， 前一个月一边烦躁，一边在浪费时间， 估计躲不了二战再花3000多——加路费更高。 之后的练习计划： 口语： 走遍美国 听力： 考满分，每天先做一个section的题，再听写。再加上听点英文课程。 作文： 每天就一个话题，限时写作。写完修改。 阅读： 只能平时看点英文文章，注意速度。也许看完多回忆就好了，不需要找专门有阅读题的书籍来做练习。","tags":"日记","title":"留学英语考试感想"},{"url":"http://sndnyang.github.io/20160924ri-ji.html","text":"星期六 天气：晴 标题： 终于摆脱火急火燎的状态了！！！ 今天完成的事 雅思笔试，从8月中突然意识到，要赶紧考GRE和托福后， 今天终于结束了首战， 虽然很有可能要二战，但最起码心里有底了。 剑雅10 - Test4 - 第3部分， 模考， 没有听写 magoosh 1000 补充中文释义 完毕--可能有更完善版，但我没找，其他的释义不全也没有说明词性。 学堂在线-MIT概率导论第一单元的第一小块作业题--有一个有点疑问，好吧， 刚才脑子不正常，搞错了， 没有问题。","tags":"日记","title":"2016.09.24日记"},{"url":"http://sndnyang.github.io/ying-yu-ci-gen-yu-dan-ci-du-shu-bi-ji.html","text":"构词形态变化 字母增加 重读闭音节的词根 添加后缀时， 尾辅音字母需要重写， 因而增加一个辅音字母。 例如： hap+-en-> happen v.发生 refer+-al-> referral a.参考的 古典语词根与后缀或另一词根构成一级派生词时， 增加连接字母。 如希腊词根之后多添加-o-，拉丁词根后多添加-i- helic+ o + pter -> helicopter anthrop+ o + logy -> anthropology sent+ i + ment -> sentiment cert+ i + fy -> certify 为保持位于某些位于词尾的重读词根中元音字母读长音， 在词根后添加无语义的构词后缀-e， 使之成为 开音节。 例如： in + clud + e -> include pro + mot + e -> promote 字母的脱落 主要是词素尾字母的脱落： 在添加后缀时， 某些词根的尾字母 -e脱落， 以保持词根原本的音值。 write+er=write create+ive=creative 有些以辅音字母结尾的前缀与以相同的辅音字母开头的词根结合时， 前缀尾字母脱落。常见的脱落字母是 -s。 trans+spir+e = transpire trans + ship = tranship 字母的变更 指在构词过程中， 字母或字母组合变成另外的字母或组合： 零级派生情况下， 词根内部元音字母变化， 使单词的词类发生转化。 full -> fill gold -> gild food -> feed 一级派生时， 某些前缀的尾辅音字母受词根首字母的同化， 变得与词根首字母相同： ad + fect = affect com + rect = co.rrect dis + fer = dif.fer ex + fect = ef.fect 二级派生时， 某些原始派生词的词根尾字母在添加 ion, ive, ible等后缀时会发生变化： absorb + ion = absorption produce + ive = production respond + ible = resonsible 二级派生时， 以辅音字母+y 结尾的单词在添加派生后缀时， 字母y变为i： dry + ly = drily happy + ness = happiness 连字符的使用 太明显， 略 用在派生前缀后， 避免该二次派生词与同形的外来借词（原生词）相混淆： re-cover - recover re-create - recreate re-sort - resort 词义及派生词义 隐性派生词： 字面含义与实用意义之间有一定距离， 但可以通过领会造词意图， 理解词义。 派生词本义发生了变化， 扩大、缩小、扬升、 贬降、 比喻或引申 不但包含含义， 还需要一些背景知识——其实举的例子也简单， 真正怕的还是一词多义， 形容词和动词。 词性 粘附词根来自 古典语基本单词的 词干 部分 古典语基本单词的词性 是粘附词根属性的依据 与之等义的英语或汉语单词的词性则可作为判断这种属性的标志。 源及形 同源异形根 拉丁动词词根， 数量多是两个， 往往分别来自不定式词干和动名词词干。 语音形体变化： 尾辅音向 t 过渡 t,d 变成 s 必要的添音与语音同化 词根尾辅音字母 b->pt p -> pt g -> ct c -> ct v -> (u)t m -> mpt x, sc -> (c)t 其他尾辅音字母 -> t t, d -> s 异源同形根","tags":"其他","title":"英语词根与单词 读书笔记"},{"url":"http://sndnyang.github.io/Locally-weighted-regression.html","text":"说明 本文参考以下文献： Andrew Ng在斯坦福的cs229视频和讲义， cs229 在下文笔较烂， 恐贻笑大方。 不过因为是markdown 写的， 有不足， 改进比较方便， 所以欢迎提出意见及建议，找出问题， 谢谢。 导读 经过了： 1. 基础 2. 标准方程 3. 概率解释 三篇教程（以后补练习）， 希望能对线性回归有个还算深入的认识。 假设样本空间确实是线性的， 那线性回归自然当仁不让， 大展身手。但我们也知道， 现实中，很多模型是非线性的。那要么发展出非线性模型（比如svm的核和神经网络），要么想办法把它看成是线性的。 人工神经网络整体可表达非线性的，但单个神经元用sigmoid函数一般还是线性的。 比如这张图, 我随机弄的点， 实际是啥模型不知道： 怎么把它看成线性的呢？ 分段线性函数 太明显了 submit 局部线性 大概就是这样的 这是理想情况， 如果是一个高斯曲线、 二次曲线肯定没那么漂亮的拟合效果。 接下来， 看这小节的题目——局部线性， 线性肯定熟悉了， 关键是怎么局部？ 拿上图来说， 人眼在这图上可以看出三段直线， 计算机怎么知道是三段， 然后每段用上哪些数据点， 就是我们试图解决的问题。 要有局部性， 最显然的， 你要预测某个点x的值， 就找x 附近的点， 把它们找出来， 然后做线性回归， 这肯定是一种方法。 速度上也可以用一些结构来优化效率——Ng在视频里提到的 KD树的局部线性方法 我猜应该就是这个方向。 但这种方法有问题。 x附近这个范围难定 不在范围内的点完全无用 x附近可能没点 x附近点也不线性 submit 局部的强调 那我们就说， 既要抓重点（局部）， 又不能把次要的（远点）全部忽略点。那怎么办？ submit 加权重 加权 这种思想、 方法应该也是经常用的。 别的不说， 线性回归方程的系数就是不同特征$x_i$的一个权重。 还有老师为了让学生能及格、不挂科， 会在期末考、 期中考、 平时作业、 考勤等项目上调整权重。 那很明显了， 要给 每个点 一个权重， M个点， 就是一个M的权重向量， 一般用w(weight)表示, 但我这里为了不写$\\theta$(theta), 线性模型里就用过w了。我们这里用 p 代表权重。 那我们的损失函数 J(w)就可以写成： $$ J(w) = \\sum&#94;m_{i=1}p&#94;i(h(x&#94;i) - y&#94;i)&#94;2 $$ 这个式子不容易表达成纯矩阵、向量乘法~~~ 那怎么求 p 呢？ 用什么方法把 p 和 w 一起求出来？也许有， 但不是本文研究的。 本文里的p 是先行计算出来， 而不是w这种给定模型后，用数据拟合出来。 想想 p 有什么特点， 我们要预测 x 的值，所以离x越近的值的权重就会越大， 越远就越小， 但这里我们认为始终大于0. 也就是x点或0点值最高， 然后往两边递减， 这样的特点你能想到什么相似的东西吗？ 钟型曲线 是的， 像正态分布或钟型曲线。 我们也知道正态分布的概率密度函数为： 平均值 $\\mu$ 是什么？ 就是我们要预测的 x 。 方差 $\\sigma$ 是什么？ 没说， 简单点的话就是1， 也可以根据实际估计一个值。 用字母t来表示。 最后还有前面的这个系数， 不需要了， 一个指数已经够了。 这样我们就得到一个权重的表达式： $$ 点 i 的权重 p&#94;{(i)} = \\exp \\large(-\\frac{(x&#94;i - x)&#94;2}{2t&#94;2}\\large) $$ 最后的最后要说的是： 这是一种权重的计算方式， 有其他的也可以用。 虽然我是从正态分布转过来的， 但 Ng 特意强调这个跟正态分布没有关系， 就是长得像。 特点 局部加权回归每次预测时， 因为要先知道预测的中心点，才能有附近、权重， 所以每次预测新的x值时， 都要重新计算。 有其他方法解决这个问题， 但不是本文讲解内容了。 总结 复杂的非线性模型可能可以考虑采用局部线性回归来拟合， 让相近点权重大，远点权重小。就是把每一个局部数据都看作是线性的。 再用， 用钟型曲线的函数来生成所有点相对x的权重。 恭喜 局部加权线性回归的主要内容就是这些， 谢谢您的阅读。","tags":"机器学习","title":"局部加权回归"},{"url":"http://sndnyang.github.io/probabilistic-Community-Role-model.html","text":"摘要 submit 再来一遍， 不权威定义：社区发现大概就是发现高密度的点群， 群内互连较多；连到群外的较少 submit 之前社区发现和role detection（作用检测）两个领域各玩各的， 本篇论文就是把这两个结合起来， 以一种统一的模型结合，来同时处理这两个任务， 并且要做到更好的效果。 基本观点 节点可能属于多个社区，一个点有没有边连到另一个点要视它所属的社区而定。each node has a distribution over the communities. 节点的属性可以把节点分成多类，可视为节点所承担的角色。each node has a distribution over roles。 每个点会有一些动作， 跟所属社区和角色有关。 所以我们必须考虑the distributions that the node has over both communities and roles. 主要贡献 写了三个 incorporate various elements of a social network into a unified probabilistic generative framework. then design a method to estimate the parameters of the model. use our model to generate a synthetic network with the learned parameters, and verify the superiority of our model, 这也能算？做实验不算创新就算贡献了？ apply the model to two problems|behavior prediction and community detection|verifying its versatility and effectiveness 这条和第二条不都是做实验吗？ 正文 定义 社区community： Each community has a multinomial distribution over all pairs (v, u), denoted as $\\zeta$ 社区上的点分布node distribution over communities？: Each node has a multinomial distribution over communities, which is denoted as $\\phi$ 角色role: 每个点可能有多个角色设定。 每个角色都有一组分布的参数。 我们这里用 高斯分布。 角色上的点分布 nodes distribution over Roles: Each node has a multinomial distribution over roles, which is denoted as $\\theta$ 动作action 动作多种，比如转发和关注。 y 表示 action 社区角色对 community-role pair the distribution of community-role pairs over actions, denoted as $\\rho$ 因为actioin y只有两种结果，所以，使用伯努利分布。 模型描述 regard a node as a random mixture over communities 网络中边的生成过程描述 如下： define each role as a distribution over attributes and each node is a random mixture over roles The generative process of all nodes ： The generative process of the actions: 时间复杂度略 推断及参数估计 看不懂。 使用 Gibbs 采样和 EM 算法，两个都不认识。 一大堆公式也不知道是什么意思。 得去补补课再回来补完报告了。 总结 这篇A和之后的一篇B挺像的（B引用了A），B的作者里有没有A 呢？ 总结不出来什么内容。 这里一个分布， 那里一个分布， 我就没思考为什么用这个、那个分布。推断部分的两个算法都不认识。 最后的实验也没看。还是要看实验才行， 实验看多了， 才能估计模型、方法有没有效果吧。","tags":"研究","title":"社交网络的概率社区角色模型"},{"url":"http://sndnyang.github.io/linear-regression-probability.html","text":"说明 本文参考以下内容： Andrew Ng在斯坦福的cs229， cs229 在下文笔较烂， 恐贻笑大方。 不过因为是markdown 写的， 有不足， 改进比较方便， 所以欢迎提出意见及建议，找出问题， 谢谢。 导读 线性回归基础 介绍了线性回归基础的三大要素： 模型， y=h(x)=w x+b， 为求方便，$b=w_0 x_0$ 策略。 损失函数 $J(w) = (h(x) - y)&#94;2$ 算法。 梯度下降法（批量和随机）， 对w向量的各个分量求偏导，迭代。 这篇文章内容会比较少， 毕竟整体风格就是碎片化阅读， 想认真看书、文章的话，有很多经典的书， 我还差远了。 回归正题。 这里我们要严格地分析所采用的策略，解释为什么用最小二乘法，也就是 损失函数$J(w) = (h(x) - y)&#94;2$ 为什么是可行有效的。 本文将介绍一种概率的解释方法， 存在其他解释（比如几何的）。 引入误差 首先， 假设确实是线性模型，$y=w*x$。 在理想的实验环境下，这个模型也许就是完美的。但我们知道，理想和现实是有差距的， 也就是我们还要加上一个误差项error。所以得： $$ y = w*x + \\varepsilon $$ 这里的误差有几种可能来源 没有考虑到的特征 随机噪声random noise 就像子女身高跟父母身高关系一样， 难免有误差， 这里不难理解。 那我们也知道，误差也是有规律的，比如预测身高应该是1米8，实际身高可能是1米79，1米8，1米81，但低于1米6的概率和高于2米的概率就很小。 那直觉上， 你觉得误差满足什么分布吗？不满足请填\"不满足\"， 否则请写是什么什么分布，比如均匀分布。 答： submit 误差的分布 没错， 就是正态分布，这主要涉及到一个概率统计上的概念， 你觉得是哪个？ 中心极限定理 大数定律 方差分析 期望和方差 submit 数学表达 所以数学式子有： $$ \\varepsilon \\sim \\mathcal{N}(0, \\sigma&#94;2) \\ 概率密度 P(\\varepsilon) = \\frac{1}{\\sqrt{2}\\pi\\sigma}exp(-\\frac{\\varepsilon&#94;2}{2\\sigma&#94;2}) $$ 因为 $\\varepsilon = y-w*x$, 而数据集方差不变。 请回答： $$y|x;w \\sim \\mathcal{N}(?, \\sigma&#94;2)$$ N( ,...)? submit 似然值 已知$y|x;w \\sim \\mathcal{N}(w*x, \\sigma&#94;2)$，则单个数据$(x&#94;i,y&#94;i)$的概率表达式为： $$ P(y&#94;i|x&#94;i;w) = \\frac{1}{\\sqrt{2}\\pi\\sigma}exp(-\\frac{(y&#94;i - x&#94;iw)&#94;2}{2\\sigma&#94;2}) $$ 不要在意乘号、矩阵向量相乘这些细节, 有时候是 $w*x$, 有时候$Xw$, 有时候$x&#94;iw$， 我也很乱的。 那所有数据(X, y)的似然值（概率）呢？ 跟单个概率什么关系？ 所有概率之和 所有概率之积 submit 表达式 所以， 完整的w似然值(likelihood)为： $$ \\begin{aligned} L(w) & = P(\\vec{y} | X; w) \\ & = \\prod&#94;m_{i=1} P(y&#94;i| X&#94;i; w) \\ & = \\prod&#94;m_{i=1}\\frac{1}{\\sqrt{2}\\pi\\sigma}exp(-\\frac{(y&#94;i - X&#94;iw)&#94;2}{2\\sigma&#94;2}) \\end{aligned} $$ 其实已经可以看到曙光了， 似然值自然是越大越好， 但看这个式子， 求积太复杂了， 肯定要化简。 怎么化简? submit 对数化 概率的值本来就很小， 再相乘就太小了。 所以， 遇到概率时， 经常使用到 取对数 的方法。 所以就有： $$ \\begin{aligned} \\mathcal{l}(w) & = log L(w) \\ 化积为和： \\ &= \\sum&#94;m_{i=1}log(\\frac{1}{\\sqrt{2}\\pi\\sigma}exp(-\\frac{(y&#94;i - X&#94;iw)&#94;2}{2\\sigma&#94;2})) \\ 展开 \\ & = m*log \\frac{1}{\\sqrt{2}\\pi\\sigma} + \\sum&#94;m_{i=1}-\\frac{(y&#94;i - X&#94;iw)&#94;2}{2\\sigma&#94;2} \\end{aligned} $$ 因为$\\sigma$跟数据样本有关，跟$w$无关，所以第一项常数可忽略， 后一项的分母也是常数忽略。 所以对 l(w)求最大化似然值就等价于最小二乘法的最小化。 $$ \\max_w \\sum&#94;m_{i=1}-(y&#94;i - X&#94;iw)&#94;2 = \\min_w \\space (y-Xw)&#94;2 $$ 至此，我们给出了最小二乘法的一个概率解释。 总结 希望通过本文的介绍， 你能了解如何通过对误差的概率分析，得到似然值的表达式，再取对数化简，最终得到和最小二乘法一样的最优化目标。 恭喜 恭喜你看完本文， 完结撒花！！！","tags":"机器学习","title":"线性回归概率解释"},{"url":"http://sndnyang.github.io/logistic-regression-foundation.html","text":"说明 本文参考以下内容： Andrew Ng在斯坦福的cs229， cs229 在下文笔较烂， 恐贻笑大方。 不过因为是markdown 写的， 有不足， 改进比较方便， 所以欢迎提出意见及建议，找出问题， 谢谢。 导读 线性回归基础 介绍了线性回归基础的三大要素： 模型， y=h(x)=w x+b， 为求方便，$b=w_0 x_0$ 策略。 损失函数 $J(w) = (h(x) - y)&#94;2$ 算法。 梯度下降法（批量和随机）， 对w向量的各个分量求偏导，迭代。 这篇文章会基于线性回归的基础， 介绍第一个分类方程--逻辑回归 logistic regression. 之前说回归和分类是两类问题， 而现在要讲的逻辑回归却主要用于解决分类问题， 虽然它确实是回归方法，输出值是连续非离散的。 模型 关于线性回归为什么不能用于分类--这个问题我就不讨论了。 分类问题，特别是二元分类问题--只有两类--是很明显的。如图： 首先， 我们要明确一点，虽然不再是线性回归了，但线性这个还是没跑的。也就是说我们要用一条 直线 来划分两类数据，更复杂的非线性分类方法以后会有。 还是老问题，现实中的数据用线性来拟合最简单， 而且也不知道它实际上会符合什么样的数学模型， 所以线性模型大概也够了， 不要自己给自己找不痛快了。 深度学习是深度学习，不一样。 建模时， 比如上图， 决定用直线$y=w*x$来划分。 那接下来就是用数学来表示这条直线了。 分类定义 先对分类问题进行定义。 分类， y值肯定是离散的， 这例子里就两类，那就可以写成 $y \\in [0, 1] 或 [-1, 1]$， 习惯上取前者[0,1]。 这例子， x的参数有几个呢？ 分类超平面 肯定是2个， x轴和y轴啊， y已经有定义？ 那就$x_1轴和x_2轴$。 2维平面上的分类， 所以用1维的直线来分类， 如果是3维空间中的分类，则用2维平面来划分。 如果是维度为n的高维空间， 就要用n-1维的超平面来分类， 这就是 分类超平面 。 这里的直线假设是$w_1x_1 + w_2x_2 + w_0 = wx = 0$。 因为，y值跟我们习惯上的 y(即$x_2$)轴没什么关系。真要画图表示的话， y值要画成第三条轴，而且只有0、1两个值。 那这条直线跟y值 怎么关联的？ 很明显， 直线左边和右边对应两个类别。 换数学一点就是： $$ y&#94;i = \\left{ \\begin{aligned} 0 & & w&#94;Tx&#94;i < 0 \\ 1 & & w&#94;Tx&#94;i \\ge 0 \\end{aligned} \\right. $$ 所以我们要从$w&#94;Tx&#94;i$的实数值映射到${0, 1}$上， 也就是加个函数变换， 就是 $$g(w&#94;Tx&#94;i)$$ 毫无疑问， 上面那个<0,>=0可以作为一个g函数。如果基于这个， 那就是 感知器模型算法(Perceptron)。 它和逻辑回归非常像， 策略和算法部分也一致， 虽然还没讲到。现在继续讲逻辑回归。 G函数 我们要把点到分类直线的远近考虑到，可以认为是类似权重之类的。那起码有两种方案： 与0比较大小之外，再添加距离作权重——这个算是svm的方式， 请看 svm基础 g函数映射时，把值分开。 第2种方案是本文的重点。因为y取0或1，所以就要满足： x=0时居中， 比如标准0.5。 负无穷时趋近于0 正无穷时趋近于1。 大概这意思， 而且趋近不能太慢，刚从0值变化时，要有个比较剧烈的变化。 可选函数应该有很多， 比较常用的是 sigmoid函数： $$g(z)=\\frac{1}{1+e&#94;{-z}}$$ 曲线如图： 模型总结 综上所述， 我们得到了我们的模型： $$h(x) = g(w&#94;Tx) = \\frac{1}{1+e&#94;{w&#94;Tx}}$$ 策略 基于模型， 我们来考虑策略、损失函数了。 用最小二乘法的$(h(x) - y)&#94;2$ 怎么样呢？很少有讨论--PASS。 这里直接上概率式子（具体推导请用误差正态分布代入h(x)自行推导）： $$ \\begin{aligned} &P(y=1|x;w) = h(x)\\ &P(y=0|x;w) = 1-h(x) \\ &所以：\\ &P(y|x;w) = h(x)&#94;y (1-h(x))&#94;{(1-y)} \\end{aligned} $$ 又看到了复杂的乘法及指数，所以求个对数得： $$ \\begin{aligned} \\mathcal{l}(w) & = \\sum y&#94;i \\log{h(x&#94;i)} + (1-y&#94;i)\\log{(1-h(x&#94;i))} \\ & = y&#94;T log(h(Xw)) + (1-y&#94;T)log(1-h(Xw)) \\end{aligned} $$ 简单粗暴 算法 继续求最优化的算法， 这里是求似然值的最大值， 不是之前的最小。 梯度下降法通用，所以是： $$ w = w + \\alpha \\nabla_w l(w) $$ 因为求最大， 所以梯度方向是求最大，使用+加法，反方向是求最小，才用减法。 这个求导过程也请自行尝试。结果为（希望没有写错）： $$ \\nabla l(w) = y&#94;T (y - h(x)) $$ 总结 本篇从线性回归的线性模型开始，一步步介绍 分类超平面的引入($w&#94;Tx=0$) 根据点和超平面的关系，需要对$w&#94;Tx$进行转换。 引入 sigmoid函数 （分类问题经常用到，非常常用） 逻辑回归的策略--似然值表达式 使用梯度下降处理 恭喜 恭喜您读完本节内容， 完结撒花！！！","tags":"机器学习","title":"逻辑回归基础"},{"url":"http://sndnyang.github.io/linear-regression-normal-equation.html","text":"说明 本文参考以下内容： Andrew Ng在斯坦福的cs229， cs229 Berkeley分校在Edx的Scalable Machine Learning，原课已关闭，改头换面重出江湖。 在下文笔较烂， 恐贻笑大方。 不过因为是markdown 写的， 有不足， 改进比较方便， 所以欢迎提出意见及建议，找出问题， 谢谢。 导读 上一篇文章 介绍了线性回归基础的三大要素： 模型， y=h(x)=w x+b， 为求方便，$b=w_0 x_0$ 策略。 损失函数 $J(w) = (h(x) - y)&#94;2$ 算法。 梯度下降法（批量和随机）， 对w向量的各个分量求偏导，迭代。 这篇文章内容会比较少， 毕竟整体风格就是碎片化阅读， 想认真看书、文章的话，有很多经典的书， 我还差远了。 回归正题。 这里我们讲的还是算法部分（起算法作用），所以模型和策略还是原样。 还记得这题不？ 你觉得应该怎么做? 不知道 从非常小到非常大全部试 从一个随机位置开始 公式变换找最小位置 submit 标准方程 之前，在讲梯度下降法时， 强行PASS公式变换。但我们都知道，在高中初学抛物线方程时，叫配项还是配方，把式子转成$y=a(x-t)&#94;2+b$的形式，最低点就在x=t处。 我们现在要给J(w)进行配方吗？已经是这个形式了， 搞不定。那怎么办？想想最低点有什么特点、 性质？（用填空更好，但这判断正误太难了） 最低点最小 一阶导数为0 二阶导数为常数 不知道 submit 求导 高中学导数或大学学微分会知道，二次函数的极值点的一阶导数为0，这是已知条件， 看来可以用了。 不过， 和一元二次方程就只有一个最小值不一样的是， 现在是矩阵、 向量的求导， 是否有多个一阶导数为0的极小值呢？ 应该没有吧~~~我也不知道 矩阵和向量 直接对 $J(w) = (h(x) - y)&#94;2$求导吗？我写的是简化版函数式，求和和上下标都省了。 已知， 训练集 X是个 $m n$维的矩阵， 或者$m (n+1)$的矩阵(把b看作0项的话）。 y 是 $m*1$维的向量。 $$h(x)=\\sum \\limits&#94;n_{i=1}w_i*x_i$$ 所以, J(w)要写得更漂亮点，使用一些先进的符号，不要用求和、 上下标之类的，档次太低。 使用哪方面的符号来改写J(w)? 微积分 数学分析 矩阵 概率论 不知道 submit h(x)的矩阵化 先定义， 使用小写的w 直接代表参数向量， 大写的X 代表数据集X， y还是y, &#94;T代表矩阵转置。 从内到外， 先改h(x), 就是 w 这个$n 1$维的系数向量 和 X 这个$m n$维的矩阵相乘得到一个 $m*1$维的预测向量？ 所以h(x)改写成 ？ submit 平方和的矩阵化 -y好说， J(w)求平方后再求和呢？ 已知 h(x)-y 是个向量。 一个向量v的平方和就是？ v和v的叉乘 v和v的内积 我也编不出来了 submit 矩阵化结果 当然， 内积是个符号， 还要再换个形式。 $n 1$维的向量v和v的内积， 就等于 $1 n$维的$v&#94;T$和v的乘积$v&#94;Tv$。 请代入之前的结果， 写出完整表达式 submit 简化版求解 Ng的视频和讲义用到了矩阵的迹及一堆定理， 太复杂。 Bekeley里呢， 就非常简单了。 先从简单的讲起。 已知 $J(w) = (Xw - y)&#94;2$ 还是复合函数求导： $$ \\frac{df}{dw}(w) = 2 (Xw-y) \\frac{df}{dw}(Xw-y) $$ 关键就是后面一部分的结果， 虽然也简单，但我觉得这种显性地思考过比潜意识觉得对好一点。 先来个问题，$\\frac{df}{dw}(w)$ 这个应该是什么结果？ w是个$n*1$维向量。 一个实数 n*n维矩阵 n*1维向量 submit 对向量求导 某函数对向量求导、偏导（表达没错吧？）就是这个函数对每个分量都求一下， 结果也就是一个向量。 现在， 1. $\\frac{df}{dw}(w)$ 的结果会是$n 1$的向量。 2. Xw-y 是$m 1$维的向量， 因为是m行的数据 $\\frac{df}{dw}(Xw - y)$中， y对w来说是常数，微分就没了， Xw是个$m 1$维向量，对$n 1$维的w求导，那对w的n个分量,都有m个结果， 所以是 $n*m$维。 那$\\frac{df}{dw}(Xw - y)$的结果应该是什么？ submit 求导的结果 $\\frac{df}{dw}(Xw - y)$是$n*m$维的，又加上Xw是w的一次函数，所以，符合$X&#94;T$的特征，那结果没跑了吧。 维数上好理解，但正好是$X&#94;T$还是蛮神奇的，我也不知道怎么解释。线性代数就没学好，这部分已经到矩阵论了吧。 三个部分都有了， 根据矩阵乘法要求， 调一下顺序得： $$ \\begin{aligned} \\frac{df}{dw}(w) & = 2X&#94;T(Xw-y) \\ & = 2(X&#94;TXw - X&#94;Ty) \\end{aligned} $$ 下一步呢？ 继续化简 令结果=0 submit 求w 请写出最终结果， 求逆请写&#94;(-1), 转置 仍是&#94;T, 仍然不要用乘号* w= submit 总结 简化版我也写得很啰嗦， 实际上不解释矩阵求导、向量求导的话， 从$J(w)=(Xw-y)&#94;2$到结果只要三五步而已。 严谨版证明 从$J(w)=(Xw-y)&#94;2=(Xw-y)&#94;T(Xw-y)$开始，展开得 $$J(w) = w&#94;T X&#94;T Xw - w&#94;T X&#94;T y - y&#94;T Xw + y&#94;T y$$ Ng在这里直接引入迹 tr(A)的概念及一堆结论。 如果函数 $f:\\mathbb{R}&#94;{m \\times n} \\mapsto \\mathbb{R}$, 也就是$m*n$维矩阵求成一个实数， 定义： $$ \\nabla_Af(A) = \\left[ \\begin{matrix} \\frac{\\partial{f}}{\\partial{A_{11}}} & \\cdots & \\frac{\\partial{f}}{\\partial{A_{1n}}} \\ \\vdots & \\ddots & \\vdots \\ \\frac{\\partial{f}}{\\partial{A_{m1}}} & \\cdots & \\frac{\\partial{f}}{\\partial{A_{mn}}} \\ \\end{matrix} \\right] $$ 迹trace的定义: 前提： $A \\in \\mathbb{R}&#94;{n \\times n}$, 即A是方阵， 则 $$tr A = \\sum&#94;n_{i=1}A_{ii}$$ 因为要求比较丰富的线性代数、矩阵论知识， 不可能在这里补充， 就想到哪里讲哪里了。 所以， J(w)的结果是个什么东西？ 一个实数 一个向量 一个矩阵 一个方阵 submit 对方阵求导 接下来的证明， 涉及几个定理： 实数的迹 等于实数本身, $tr \\alpha = \\alpha$。可以对J(w)求迹， 进而再使用迹的其他定理 $tr AB = tr BA$ 和 $tr ABC = tr CAB = tr BCA$, 给矩阵相乘顺序作调整。 $tr A = tr A&#94;T$ 最重要的是： 如果 $f:\\mathbb{R}&#94;{m \\times n} \\mapsto \\mathbb{R}, 且 f(A) = tr AB$， 则有： $\\nabla_A tr AB = B&#94;T$ $\\nabla_A tr ABA&#94;T C = CAB + C&#94;T A B&#94;T$ 这几个定理的证明我不太清楚哪里有， 好像不是线性代数里的内容。 接下来， 就是对展开式的进一步变换了。 $$J(w) = w&#94;T X&#94;T Xw - w&#94;T X&#94;T y - y&#94;T Xw + y&#94;T y \\ = tr (w&#94;T X&#94;T Xw - w&#94;T X&#94;T y - y&#94;T Xw + y&#94;T y)$$ 第四项 $y&#94;T y$求导肯定为0。 第2、3项互为转置，求导结果相等，只看其中的$y&#94;T Xw$，把$y&#94;T X$看作是B, 所以这两项求导结果就是$2*(y&#94;T X)&#94;T = 2 X&#94;T y$ 第一项最复杂， 跟$tr ABA&#94;TC$最相似，有$A和A&#94;T$。来问题了, 我们把$X&#94;T X$看作是B呢还是C呢？ B C submit 迹求导 把$X&#94;T X$看成C， 那现在的顺序是 $tr w&#94;T C w$, 要把顺序从ABC改成CAB， 变成$tr(w w&#94;T X&#94;T X)$ 所以$A, A&#94;T$之间的B 就可以用 单位矩阵来表示。 这又是个种技巧，也许就叫 单位元 技巧。 然后代入结果， 就得到： $$ \\nabla_w tr(w&#94;T X&#94;T X w) = X&#94;T Xw + (X&#94;T X)&#94;T w \\ = 2 X&#94;T Xw $$ $(X&#94;T X)&#94;T=X&#94;T X$ 是叫什么来着？ 所以综合一下得到： $$ \\nabla_w J(w) = 2X&#94;T Xw - 2 X&#94;T y $$ 令上式等于0， 求得： $$w = (X&#94;T X)&#94;{-1} X&#94;T y$$ 恭喜 线性回归的标准方程 Normal Equations 内容就是这么多， 又叫 closed form 什么的。 其实就是二次函数只有极小（大）值点的导数为0。所以对J(w)求个导， 令结果=0， 就得到极值点了。 这是个理论吧， 实践中用这个的应该比较少。","tags":"机器学习","title":"线性回归标准方程"},{"url":"http://sndnyang.github.io/linear-regression-foundation.html","text":"说明 本文参考以下文献： Andrew Ng在斯坦福的cs229 讲义， cs229 在下文笔较烂， 恐贻笑大方。 不过因为是markdown 写的， 有不足， 改进比较方便， 所以欢迎提出意见及建议，找出问题， 谢谢。 导读 关于机器学习的概念、定义，我写不明白。 监督学习、无监督学习的区别， 分类问题和回归问题的区别也写不好，所以这里假设读者对这些概念都有一定了解。 本节要讲的内容就是线性回归及其求解。 一言不合就问问题。 同学你知道什么是线性回归吗？ 知道 不知道 submit 是什么？ 线性回归是什么？ 线性简单，代表模型、理论函数是 y=w*x+b, 这里的x可以是一个变量，也可以是向量。 回归是什么？建议找本统计学的书来看看， 比如陈希孺《概率论与数理统计》第258页开始， 介绍回归分析的基本概念。 我们假设了模型，但模型的参数未知， 回归分析就是去分析、求出参数的值， 并达到某个效果。 和分类问题相比， 分类是输出\"类别\"，离散的值， 回归输出的是连续的值。 为什么？ 为什么是线性回归， 首先线性肯定比非线性简单， 其次跟线性代数、 统计课程能连上。 但为什么线性模型足够有效，超过我的知识水平了。 总览 机器学习有三要素（李航《统计学习方法》统计学习方法的三要素）： 模型 策略 算法 所以，本文内容相应的就是： 模型--线性模型， y=w*x+b 策略--确定模型（参数）选择的准则（评价方法），即损失\\误差函数。用策略一词，是有点不习惯。 算法--求解最优模型的过程。 模型 前面说过多回了， 线性回归， 线性方程 y=h(x)=$\\sum \\lim_{i=0} w_i*x_i$, 这里把b看作是 $w_0 * x_0$ 这里采用这种标记法，另有一种常用的是$Y=\\theta&#94;TX$。只是符号问题。 接下来， 我们就要考虑求最优模型（参数）。 策略 为了求解最优模型， 需要采取一定的策略。机器学习里的策略大概就是指模型选择的准则。 对线性回归来说，我们考虑让预测值和实际值之间误差最小作为标准，选取损失函数。 篇幅限制及原课程的安排， 所以， 推导具体的损失函数请看 概率解释 是否已看过概率解释部分？ 看了 没有 submit 损失函数 上结论， 我们选择评估误差的标准为 $(h(x) - y)&#94;2$, h(x)=w*x是预测值，y是实际值。平方强调了误差越大，影响越大。 这是单个数据点的误差， 所以整个数据集的误差就是： $$ J(w)=\\sum&#94;m_{i=1}(h(x&#94;i) - y&#94;i)&#94;2 $$ 于是， 任务就变成了对上式求最小化（误差最小），并求出最小是h(x)的参数w。 $$ \\min\\limits_w\\sum&#94;m_{i=1}(h(x&#94;i) - y&#94;i)&#94;2 $$ 算法 如果了解、熟悉梯度下降法的同学已经可以不用看了。直接点下一段到结束吧！ $x&#94;2$平方函数有什么特点呢？一个开口向上的抛物线， 对吧？所以它确实存在最小值。而损失函数也会存在最小值， 这个我也不会证，但应该没有疑义吧？ 在训练集数据x,y都已知的情况， 损失函数的自变量是否只有一个w了？损失函数的值会随着w的变化而变化。 我们假设一个一维w对应的损失函数如图： 人眼能估计最小值在哪个位置对吧， 先说明下，我写的函数是 $y=3 x&#94;2-2 x+5$。所以最小值不是在x=0。 你觉得应该怎么做? 不知道 从非常小到非常大全部试 从一个随机位置开始 公式变换找最小位置 submit 从随机状态开始 暴力搜索地从非常小到非常大地尝试肯定是不可取的，又慢又试不完。PASS。 公式变换是可行的， 但它是标准方程或闭型部分的内容， 请看 下一篇 。PASS。 只能从某个随机值开始，以前学的算法多数是确定性算法， 貌似只有快速排序里可选地提及随机选择。 而现实中， 类似的搜索性问题多半是从随机初始状态开始的。这可能算是个很重要的 随机化 思路。 我们现在随机选择了x=5这一点（实际可能喜欢用0向量）， 在上图的红点有个X。这里是个坡，我们放个小球，会自然地顺着坡滚下去。那计算机程序怎么办呢？一维的w在图上还只有左右两个方向，二维的w加上y值形成3D图，每个点都是360度连续空间 怎么选？ 随机选 采样求平均 求坡度 求最小 不知道 求反函数 submit 坡度和梯度 坡度是什么呢？ 极值 导数 反函数 指数 对数 二阶导数 submit 导数 很明显了， 我们要求导！！！ 一维的w，也就是只有一个变量， 求导肯定对的，非左即右。 那2维、多维的w呢？拿2维来说，就相当于x轴和y轴，要同时在两个变量上求导~~~所以， 分别对两个变量求偏导。并可以类推到更高维。 高数、线代没学好，这里强词夺理、囫囵吞枣。 求偏导 终于， 绕过来了， 需要要对J（w）函数求偏导，找出当前点的\"滚动\"方向。写作$ \\frac{\\partial}{\\partial w}J(w)$。 其中,忽略求和符号后，$J(w)=(h(x) - y)&#94;2$。一个简单的复合函数。 本以为，终于可以有填空题了。结果发现失败了。 复合函数求导h(x)=f(g(x)),则h'(x)= ?不需要*符号 submit 继续求导 复合函数的求导法则知道了， 来推导本例子。 $$ \\begin{aligned} \\frac{\\partial}{\\partial w_i}J(w) & = \\frac{\\partial}{\\partial w}(h(x) - y)&#94;2 \\ & = 2*(h(x)-y) * \\frac{\\partial}{\\partial w_i}(h(x) - y) \\ \\end{aligned} $$ 想实现读者自行推导过程， 辅助提醒， 但公式判断怎么做？搜都不知道搜什么。强行中断一下。 已知 h(x) = w*x, 等于向量w和x的内积 那 $\\frac{\\partial}{\\partial w_i}(h(x) - y)$化简结果是？ w向量的i分量 x的i分量 x的和 w的和 submit 求导结果 所以结果为： $$ \\frac{\\partial}{\\partial w_i}J(w) = 2 (h(x)-y) x_i $$ 求偏导给出了方向， 所以， 要沿着方向。那么， 把w看作点的位置，怎么滚？ w-偏导值 偏导值替换w 二者相乘 二者相加 submit 滚多远？ 所以 w是当前坐标， 要减掉偏导得到的结果（沿着梯度方向滚）， 才能使得J(w)变小。 我们能沿着这个方向没没完没了地滚下去吗？ 要知道哪怕只滚动一厘米、 一毫米， 新的位置的偏导值（向量）都有可能发生变化。 所以要怎么样？ 给偏导值向量乘上个小系数 无聊 乘上个大系数 偏导值改成倒数 submit 什么时候结束 $\\alpha$的取值太大太小都有问题，这是工程实践上的问题，跳过。 综上所述， 我们有 $w=w-\\alpha \\frac{\\partial}{\\partial w}J(w)$， 作为每次迭代对w的更新。 迭代总要有个停。大概可能有以下这些方案： 迭代指定次数 两次迭代的J(w)值之差足够小——这个好像比较常用。 偏导值奇迹般地等于0 算法部分总结 以上就是线性回归求解的算法，即梯度下降法 大致步骤为： 已知一个待求最优（最小或最大）的一元、多元函数 给自变量选取一个随机的起始值 对自变量的各个分量求偏导 根据偏导的方向（值）来适当更新自变量 迭代3、4步直到满足你设定的收敛或其他条件 梯度下降实践 梯度下降法在更新w上可以采取不同的策略。 从之前的公式， 我们得到完整用于整个训练集的公式为（原先的2是常数，跟alpha合并即可）： $$ w_i = w_i - \\alpha \\sum&#94;m_{j=1}(h(x&#94;j)-y&#94;j)*x&#94;j_i $$ 这个式子每次都要把整个训练集X求个和， 所以叫 批量梯度下降 Batch Gradient Descent 在数据量比较大的时候就会很慢。 那相应的就是不批量策略。比如stochastic随机梯度下降也叫增量梯度下降。 简单来说， 就是不求和，不停扫描。 Repeat { for j=1 to m { $w_i = w_i - alpha * (h(x&#94;j) - y&#94;j) * x&#94;j_i$ (for all i) } } 恭喜 线性回归的主要部分就是这些， 谢谢您的参与。","tags":"机器学习","title":"线性回归基础"},{"url":"http://sndnyang.github.io/multilayer-edge-mixture-model.html","text":"摘要 本文针对多层的网络数据--可能主要是对它进行社区发现--做了以下工作： 提出多层边混合模型(multilayer edge mixture model MEMM)，从边组合的视角（观点），探索一种通用的社区结构评估器 展示了从MEMM可以推导出多层颗粒度(multilayer modularity)和随机块模型(stochastic blockmodel) 探索以MMEM的形式，对特定形式的社区结构评估器进行分解。这样可以发现评估器的新的解释。 社区发现的说明： 多层图（多层图）的说明： 本文使用的是柱式多层图， 各层点相同。 导论 MEMM(multiply edge mixture model)： 是基于边（的贡献）的线性组合。 超模型(hyper model)，能为现存的多层社区结构评估器提供新的解释。 帮助推导出新的质量函数。 正文 基础 Reichardt and Bornholdt提出的 边反馈方案(rewarding scheme of edges), 应用于单层网络 奖励社区内存在的边 惩罚社区内不存在的边 惩罚社区间存在的边 奖励社区间不存在的边 Reichardt and White 提出的 role model， 假设只允许一些社区对之间存在边，这些社区定义为\"亲密社区 intimate communities\" 奖励亲密社区间的边 奖励非亲密社区间不存在的边 以上两种方案的相同点： 引入超参数，根据社区定义确定参数值，得到社区结构评估器。 该评估器代表了被发现社区结构的质量，进而得到最优社区分配。 这样的模型命名为 超模型（hyper model） MEMM 在边反馈方案的基础上： 添加层间边的情况 添加边的概率 如图： s和r代表层, node is代表i点在s层 A, C, v 矩阵分别表示 层内邻接、 层间邻接和社区标签 abcdefgh是超参数，混合系数 P是两点同属一社区的概率， 支持模糊划分 fuzzy partition. lambda(w)函数， 大于w，返回1， 否则 -1 hyper parameter的设定 策略： 固定值 网络结构的某个特定函数？什么意思 可行做法： 根据边的贡献比例， 选择固定值（贡献比例怎么求？） 网络变更后，重算参数 推导 之后的工作是从MEMM来推导： 模块性(Modularity) SBM stochastic blockmodel 随机块模型 这部分可以理解成拟合、 泰勒展示、 逼近、 近似什么的。 通过设定超参数 hyper parameters 的值就可以做到， 所以MEMM是个更加通用的模型 具体表达涉及 模块性、SBM的定义， 就不在这里展开了。（也还没看） 分解评估器 不太理解这块和上一块推导有多少区别。 模块性和随机块模型是否也算是评估器。 看起来没太多区别。 意义在于： decomposition of an evaluator reveals the preference of the evaluator 但evaluator是什么和如何decompose 都没讲好的感觉 实验部分 内容很多~~~ 总结 复杂，长 语言也比较复杂~~~介绍部分大段文本看着眼花缭乱 吐槽下举了个电话网络例子，感觉这个例子一点都不好","tags":"CS","title":"多层边混合模型"},{"url":"http://sndnyang.github.io/Generative-Feature-Selection.html","text":"摘要 现在的网络数据（社交网络或信息网络），不仅有链接信息， 还会有一些或噪声或重要的高维内容，比如点的属性。 也就是说，不单纯是个图数据，每个点还有若干个特性、属性。 那些对网络数据进行机器学习的任务，比如社区发现和链接预测，如果能够利用上这些附加的、额外的信息，想必会有一定的效果。 但这些点的属性或特征往往是高维度数据，对效率上影响比较大。 所以， 本文的目标就两个zhi： 降维 就是对点的特征进行选取 (feature selection) 再根据前人工作中的两个问题， 选取特征时没考虑数据的网络、图特性（存在边、连接）。 网络数据没有标准的标签（label)，一般不适合监督学习的方法。但却用聚类等方法强行给标签，强行使用监督学习，准确性很差。 所以本文提出的 特征选取方法 ： 非监督的 用生成模型（generative model)来整合连接和内容信息（即边和属性） 导论 和摘要差不多， 重复不提。 主要思想： 从点的所有特征中，选取重要的、关键的特征，记为 预特征 （oracle features，叫圣特征、神谕特征都不好吧） 对于边来说 边不是边~~~而是两个点在预特征上的亲密度（就相似度）来决定两个点是不是该相连。 首先，非预特征对边没有影响。就像自然语言处理里肯定要把停用词过滤掉， 这种数据没有作用。 其次， 想说什么来着？忘了 对于点的属性特征来说 首先， 很多属性也是没用、没影响的，只占空间不干活。 其次， 还有很多的属性或特征 本质上是重复的。就像线性方程组消元，发现一些方程可能是另外方程的线性组合。 所以，原本比较大的点属性集合就可以用比较小的 预特征 集合来代替（还可以存在映射关系） 结论 就可以用点的预特征集合 来同时表示连接和内容信息。 这样，就得到了一个模型， 类似于线性回归的那个线性方程的模型， 后面会写模型具体的形式 然后，再去定义这个模型对应的： 策略。 即损失函数，要优化的目标。 算法。 如何进行优化。 *模型、策略、算法，机器学习三要素，《统计学习方法》的描述方法 题外话 生成图模型，generative graph model， 比如AGM 都是先定义模型、模型表达式，再定义策略、损失函数，最后才用算法来求出模型解空间里 最优的每个参数 正文 定义 Attributed Network 属性网络： G = (V; E; X)， X是n个点的D维属性、特征向量 s 是D维01向量， 1代表该位置是预特征， 0则否 diag(s) 则是把s 拉成对角矩阵， 后面计算用得到--光点积不够用 目标是求出 预特征 集合 对连接信息建模 如之前所说： probability of a link is determined by the oracle affinity between two nodes 所以要定义 Oracle Affinity Oracle Affinity dot product of oracle features of two nodes $$ a_{ij}=x&#94;T_i diag(s) x_j $$ 边生成概率定义 用某个转换函数（比如sigmoid）将上面的 $a_ij$转成个概率值 概率值再用伯努利分布来决定边是生成还是不生成， 如下： $$ p_{ij}=F_g(a_{ij}) \\ E_{ij} \\sim Bernoulli(p_{ij}) $$ 整个图的概率 从定义上， 在给定 预特征 oracle features时， 整个网络的概率为： $$ P(G|s) = \\prod_{(i,j)\\in E}p_{ij}\\cdot \\prod_{(i,j)\\notin E}(1-p_{ij}) $$ 最终式 把$a_{ij},F_g$代入上式， 求个-log, 化简得： $\\cal{L_G}=xxxx$ 省略~~~ 对内容建模 关键就是得到一个 预特征向量 和 原特征向量的映射关系。 所以可以是：（也可以使用和边相似的建模） $$ \\mu_i = F_c(diag(s)x_i) \\ x_i \\sim \\cal{N}(\\mu_i, \\sigma&#94;2\\bf{I}_D) $$ 为简便， Fc就等于左乘个 D行D列的投影矩阵 $\\bf{W}$ 使用以上模型的话， 那对内容的最大似然值优化 其实也就等效于 最小化误差平方和， 再加个控制项得： $$ \\cal{L}_C=||X&#94;Tdiag(s)W - X&#94;T||&#94;2_F + \\beta||W||&#94;2_F $$ 二者结合 就是 $$ \\min \\limits_{s,b,W} \\cal{L}_G + \\cal{L}_C $$ 限制条件有， s向量里取值0或1，长度D，向量和=d 对齐还没掌握~~~ 优化 出于优化难度，改写优化式： $$ \\begin{eqnarray} & \\min \\limits_{s,b,W} & \\cal{L}_G + \\cal{L}_C + \\lambda||s||_1 \\ & s.t. & 0 \\leq s_p \\leq 1, \\forall p=1,...,D \\end{eqnarray} $$ 优化过程并不特殊， 貌似和SVM的优化过程SMO是一样的。 先固定W， 优化方程的s和b 再固定s和b， 优化方程的W 偏导方程就不写了。 总结 本文定义、 概念描述准确，逻辑清晰，没有太复杂的东西。 总结不太会写， 还不敢评价论文质量。 不过本文的实验部分写得比较简略~~~别的很多论文都3、5页的。本文将将一页出头。","tags":"CS","title":"Unsupervised Feature Selection on Networks: A Generative View"},{"url":"http://sndnyang.github.io/Community-Role-Detection.html","text":"摘要 submit 再来一遍， 不权威定义：社区发现大概就是发现高密度的点群， 群内互连较多；连到群外的较少 submit 之前社区发现和role detection（作用检测）两个领域各玩各的， 本篇论文就是把这两个结合起来， 以一种统一的模型结合，来同时处理这两个任务， 并且要做到更好的效果。 基本观点 正确地划分出社区， 能促进对点的功能、角色的检测。 正确地识别点的功能，可以得到更好的网络模型，社区发现也就更好。 例子 如图： 两个群、 3种角色 主要贡献 写了三个 Studying a novel problem of integrating community and role detection in information networks. 这也能算 :( Proposing a unified probabilistic generative model that defines the link generation probability based on both community and role labels of nodes, and a Gibbs sampling based inference algorithm is proposed. 在三个人工数据和两个真实数据上实验， 证明了方法对社区/功能检测 和 链接预测上的有效性-- 这也能算？ 应该就第二条， 结合了社区、功能检测思想，提出了一个统一（通用）概率生成模型来定义链接生成概率， 并使用Gibbs采样算法（别人的）。 正文 除掉实验和公式， 内容不多。 没看过概率图模型的内容， 我估计以下步骤是概率图模型的内容 就像MMDS(Mining Massive Datasets)课程或书里写的， 社区发现分两步： 定义一个生成网络的生成概率模型 generative model 再根据某种统计推断方法， 确定生成模型里的各个参数 因为本文使用的统计推断算法几乎就是别人原版吧， 所以也就是在生成模型上有点新意——可能也就各种凑合。 定义生成模型 原模型 基于 Mixed Membership Stochastic Blockmodel (MMSB) 4 MMSB的连接生成过程（边生成）如下： For each node i: - Draw a group membership distribution vector πi ～ Dirichlet(α) For each node pair (i, j): - Draw node i's latent group Zij ～ Multinomial(πi) - Draw node j's latent group Zji ～ Multinomial(πj) - Draw the link Eij ～ Bernoulli(SZij ;Zji) S: group-group interaction probability matrix 改进 Mixed Membership Community and Role (MMCR) Model 大概就是给点多加一个role功能的向量 总结 本篇或许是比较新颖地把原本不相关的社区发现和功能检测进行了关联--假设他们是第一个吧。 但给我感觉比较水， 重复地讲那简单的思想原理——不过其他论文也经常重复当强调 对整个过程用了种理所当然的描述——the collapsed Gibbs sampling algorithm is straightforward。不得不去看Gibbs这个算法的论文。 伪代码也没有 实验数据非常小。 百来个点， 效果好了， 性能没实验， 复杂度说是平方 感觉英文写作水平不是很好，不知道是真的， 还是我英文差。","tags":"研究","title":"社区及角色发现"},{"url":"http://sndnyang.github.io/Communities-Ground-Truth.html","text":"摘要 社区发现的挑战 社区定义的多样性、不确定性 a plethora of definitions of a community 算法困难， NP-hard intractability of algorithms 评估困难，缺少可靠的参考 lack of a reliable gold-standard ground-truth 本文工作 定义 ground-truth 根据230个大规模的实际网络数据， 包括社交、协作、信息网络 这些数据里的点有明确的归属关系，属于 进而可以用这些信息来定义参考（ground-truth） 社区的可靠、可信的标记标签 量化评估 量化评估网络社区的不同结构化定义， 评估敏感度、 稳定性和性能 导论 贡献 A possible solution would be to find a reliable definition of explicitly labeled gold-standard ground-truth communities 贡献： 对230个大规模的社交、信息网络，用一种可靠的方法，定义了参考（ground-truth）社区 基于ground-truth，量化评估了网络社区的13种常用的结构化定义， 并检验了它们的稳定性和敏感性 扩展了局部谱聚类算法， 得到一种无参社区发现方法， 可支持百万点级别的网络检测 数据集评估 社区参考标准 社交网络的社区， 基于特定主题的分组 购物网络， 基于层次化组织的货物分类 科研合作网络， 基于相同的出版组织（会议等） 好处 与社区发现工作的隐含前提一致： 社区内的成员会有相同的功能或性质 这是网络的组织原则 其他方案 用成员点对点的相似度， 以属性来评估社区 来源：Y.-Y. Ahn, J. P. Bagrow, and S. Lehmann Link communities reveal multi-scale complexity in networks 文章链接 数据预处理 无权重无向静态图。 组里成员可能无外连， 视作独立的社区 参考社区可内嵌、 可重叠 社区评分函数 思想： 给定一个社区评分函数， 把它发现的高分点集视作社区 社区内点的连通度高， 社区间连通度低。 分类 基于内部连通性 内部边密度 内部边数 平均度数 度数中位数上比例 三角参与比例 基于外部连通性 扩展 割比例 cut ratio 内外部连通性结合 传导性 conductance 标准割 normalized cut 最大出度分数 平均出度分数 Flake出度分数 基于网络模型 模块性 因公式麻烦， 请看图片 实验结果 计算每个标准社区的的这13种分数 计算一个相关矩阵 correlation matrix 设定相关阈值， 比如 0.6 分成四组 评估社区评分函数 to develop an evaluation methodology for network community detection 社区效果度量 采用一种公理化方法（然而只有第一步）： 定义四个社区效果（goodness)度量 对\"好\"社区的直观定义进行规范形式化(formalize) 区别 社区评分函数量化了一个集合的\"社区相似度\" 效果试题量化社区的满意度(desirable property) 分类 对点集S 的效果度量g(S)有： 可分度Separability： g(S) = 内边数/外边数 密度Density: g(S) = 边数/（2点组合数） 凝聚度Cohesiveness: S诱导（生成）子图的最大导率(conductance) 聚类协同系数Clustering coefficient：没有准确描述，得看其他论文 实验设置 步骤： 有很多标准社区Si 对每个评分函数， 把标准社区按该方法的分数降序排列 计算前k个标准社区的效果度量值， 并计算累积平均 直观思路： 好的社区评分函数与效果度量完美相关（正或负） 则效果度量的平均值 应该 随k 单调 假设社区评分函数乱排序， 效果度量平均值将是k的固定函数 constant function 实验结果 社区评分函数稳定性 Robustness Community perturbation strategies 点交换： 随机选择一条边(u, v), $u \\in S, v \\notin S$， 对S 删除u，加上v 随机： 随机选择$u \\in S, v \\notin S$， 对S 删除u，加上v 扩展: 随机选择 $u \\in S, v \\notin S$， 对S 加上v 收缩： 随机选择 $u \\in S, v \\notin S$， 对S 删除u 量化差别 h(S, p)代表干扰后， 则 Z-score: E是期望， Var 是方差 高Z-score意味着什么？ 标准社区的期望分数比扰动后的低， 所以对f评估函数来说， 标准社区比扰动后好。 实验 基于种子点的社区发现 基本算法 给定： 图G、 种子节点s、 评分函数f 步骤： 用PageRank-Nibble， 从点s开始计算随机游走分数$r_u$ 根据$r_u$/d(u)，对点进行排序， d(u)是点u的度 对前k个点， 计算社区评分函数 f(Sk) 检测f(Sk)的极小值，发现一到多个社区 if 要发现一个社区 找到第一个局部最优的fk else 找到全部的局部最优 英文版描述： 个人总结 本文定义标准社区思想非常简洁。 新提出的社区发现方法， 里面极小值的使用不知道是什么原理。 实验部分长， 但真正做研究也确实应该这样认真地做实验分析， 不同的评估函数、衡量标准、 干扰等等。","tags":"CS","title":"Defining and Evaluating Network Communities based on Ground-truth"},{"url":"http://sndnyang.github.io/other.html","text":"数据库 db2 db2 变更alter db2 导出 db2 特殊查询 db2-jdbc db2 配置远程数据库 系统工具 端口监听 grep技巧 find_grep 命令行工具 tar压缩 date makefile vbox共享文件夹 awk-sed ctags+vim tag技巧 cscope技巧 shell shell文件.html shell技巧.html shell时间.html shell_string.html shell变量控制.html shell替换与插入-ru.html ksh ksh_auto_complete.html ksh判断.html ksh手册.html ksh数学运算.html ksh数组.html ksh语法.html ksh参数处理.html ksh自动补全.html profile设置 论文 FOCS总结 FOCS notebook 基于进化算法的社区发现综述 社区重叠综述 进化多目标方法在动态网络聚落检测中的应用-总结 MOOC 数据分析与统计推断项目翻译 生物信息学算法课程概述 编程语言 python python web框架html模板 java java乱码.html java-Comparator.html java错误.html java-Generic-Iterator.html","tags":"other","title":"其他"},{"url":"http://sndnyang.github.io/slides_set.html","text":"人工智能 alphago 数据挖掘 多层网络社区检测综述 快速重叠社区发现","tags":"研究","title":"幻灯片集合"},{"url":"http://sndnyang.github.io/sml-linear-regression.html","text":"什么是机器学习 定义： 从数据中获取到的经验，使某方面的表现更好(如更高的准确率、识别速率) 为什么使用机器学习 规则难定义或未知 程序编写复杂 从数据中学习中相对容易 使用情形示例： 无法人工编写出全部规则 (规则未可尽知) 无法准确定义 (规则难以言表) 快速决策 面向大量用户 所以 机器学习是编写复杂规则系统的一种替代方法","tags":"机器学习","title":"可扩展机器学习第三周-线性回归"},{"url":"http://sndnyang.github.io/union-find-optimize.html","text":"方法 1 带权重快速合并 好处: 优化 快速合并 ，避免树的层次过多 随时记录每棵树（子树）的大小 通过将较小子树的根挂在较大树的根下，来获得平衡 数据结构 比快速合并算法，增加一个大小为N的整型数组sz。 sz[i]代表以i为根的对象个数。 查找 与快速合并相同， return root(p) == root(q) 合并 将较小子树的根结点连接到较大子树的根结点 更新sz数组 public void union(int p, int q) { int i = root(p); int j = root(q); if (sz[i] < sz[j]) { id[i] = j; sz[j] += sz[i];} else { id[j] = i; sz[i] += sz[i]; } } 算法分析 运行时间: 查找: 与p和q的深度成正比 即 lg N 合并: 对给定的根，只花费常数时间 lg N 结点x的尝试最多为 lg N. 路径压缩 方法: 计算出p的根结点后， 将每个被检测到的结点都指向这个根结点 实现: 两次遍历: 循环中再增加一次处理， 将每个被检测到的结点的id指向上一层的根结点。 private int root(int i) { while (i != id[i]) { id[i] = id[id[i]]; i = id[i]; } return i; } 命题: 从空集开始，N个对象的任意M次操作，对数组的访问次数萍踪: <= c (N + M lg* N) lg N 其实就是 lg N 的再次求对数的样子。 In computer science, the iterated logarithm of n, written log n (usually read \"log star\"), is the number of times the logarithm function must be iteratively applied before the result is less than or equal to 1. 本算法理论上非线性复杂度， 实际上可以看作线性。","tags":"算法","title":"并查集优化"},{"url":"http://sndnyang.github.io/word2vec-2-mindmap.html","text":"译自： 原文链接 (没有找过作者， 随手就翻译了) 思维导图这一工具因其长于组织大量任务、材料信息，在头脑风暴、 计划和问题解决等领域得到广泛使用、一致好评。 对思路的可视整理有助于整个思考的过程， 并且模拟了我们人类思考时获取脑中知识的方式。 现今有很多工具可以帮助我们画出思维导图， 但还没有一个能自行生成的， \"生成\"是指从文本（语音）内容中提成。 为了做到这一点， 我花了最长的时间（至今快8个月了）， 研究如何结合文本挖掘和图论做成一个框架来生成思维导图（给定一段文本）。 当然， 第一个问题就是， 任意一段文字都不会只有那么一种可行的思维导图。 只是， 如果你要构建自己的思维导图， 有这么一个自动工具， 可能会给你更多的思路和洞见， 特别是头脑风暴时， 或帮你查缺补漏。 那我们先来看看一个思维导图的样式—— 两个关键点： 思维导图并不简单地是一棵树， 不只是递归地将主题划分成子主题。 它本质上更像图， 连接项在语义上是相关的。 正如‘夜晚'可能会让你想到‘白天'， 思维导图中， 意义相反的两个概念之间也很可能存在连接。 还有诸如使用图片强化概念等其他点。但这些并不是本文的主旨（我的设计师风格创造力糟透了）。 有备无患 ， Heres 这篇文章能帮助你熟悉构建和使用思维导图的过程。 在我上一篇博文 链接 中， 我描述了一种从文本生成Word2Vec模型的方法（使用维基的文章作为示例）。 在这里， 我将描述我使用的从 Word2Vec模型生成基本思维导图的方法。 第一步： 从文章中找出前n项 （就像我上一篇博文所说， 我只使用stemmed unigrams（一个词干的n-gram), 你可以自行采用更高阶的ngrams, 想来会更棘手（准确来说， 是当你生成n-gram的算法有效时） 这里的 n 是指思维导图中的节点数， 在我多次尝试之后， 50是个比较好的数字， 太小则信息少， 太大则噪音多。 欢迎尝试其他数字。 我使用了本文 链接 中写的 co-occurrence 方法， 列出文本的前 n 项词。 代码如下： def _get_param_matrices(vocabulary, sentence_terms): \"\"\" Returns ======= 1. Top 300(or lesser, if vocab is short) most frequent terms(list) 2. co-occurence matrix wrt the most frequent terms(dict) 3. Dict containing Pg of most-frequent terms(dict) 4. nw(no of terms affected) of each term(dict) \"\"\" #Figure out top n terms with respect to mere occurences n = min(300, len(vocabulary)) topterms = list(vocabulary.keys()) topterms.sort(key = lambda x: vocabulary[x], reverse = True) topterms = topterms[:n] #nw maps term to the number of terms it 'affects' #(sum of number of terms in all sentences it #appears in) nw = {} #Co-occurence values are wrt top terms only co_occur = {} #Initially, co-occurence matrix is empty for x in vocabulary: co_occur[x] = [0 for i in range(len(topterms))] #Iterate over list of all sentences' vocabulary dictionaries #Build the co-occurence matrix for sentence in sentence_terms: total_terms = sum(list(sentence.values())) #This list contains the indices of all terms from topterms, #that are present in this sentence top_indices = [] #Populate top_indices top_indices = [topterms.index(x) for x in sentence if x in topterms] #Update nw dict, and co-occurence matrix for term in sentence: nw[term] = nw.get(term, 0) + total_terms for index in top_indices: co_occur[term][index] += (sentence[term] * sentence[topterms[index]]) #Pg is just nw[term]/total vocabulary of text Pg = {} N = sum(list(vocabulary.values())) for x in topterms: Pg[x] = float(nw[x])/N return topterms, co_occur, Pg, nw def get_top_n_terms(vocabulary, sentence_terms, n=50): \"\"\" Returns the top 'n' terms from a block of text, in the form of a list, from most important to least. 'vocabulary' should be a dict mapping each term to the number of its occurences in the entire text. 'sentence_terms' should be an iterable of dicts, each denoting the vocabulary of the corresponding sentence. \"\"\" #First compute the matrices topterms, co_occur, Pg, nw = _get_param_matrices(vocabulary, sentence_terms) #This dict will map each term to its weightage with respect to the #document result = {} N = sum(list(vocabulary.values())) #Iterates over all terms in vocabulary for term in co_occur: term = str(term) org_term = str(term) for x in Pg: #expected_cooccur is the expected cooccurence of term with this #term, based on nw value of this and Pg value of the other expected_cooccur = nw[term] * Pg[x] #Result measures the difference(in no of terms) of expected #cooccurence and actual cooccurence result[org_term] = ((co_occur[term][topterms.index(x)] - expected_cooccur)**2/ float(expected_cooccur)) terms = list(result.keys()) terms.sort(key=lambda x: result[x], reverse=True) return terms[:n] get_top_n_terms 函数实现了这个功能， 我希望我写的 docstring 和 注释很好地解释了整个过程（结合起那篇论文）。 如果你有时间， 足够耐心， 你可以看到你Word2Vec模型里的整个词库（entire vocabulary）， 并找到你想加入到你的思维导图里的那些项。 这样做大概能得到最好的结果（就是太辛苦）。 第二步： 选定根节点 根结点是最能表达思维导图中心思想的。 相比起整个词库entire vocabulary， 选中的结点个数小上许多， 所以， 也许最好就是 手工选定根结点的项。 或者， 使用出现频率最高的（has the highest occurrence）。这一步也需要很多尝试（但数学科学能起什么作用吗） 第三步： 生成导图 这是至关重要的一步， 也是我花了最多时间的。 首先， 我需要定义一个项（term）的 情境向量（contextual vector） 假设， 本导图的根是‘电脑'， 连到另一个项‘硬件'， ‘硬件'再连‘键盘'， 那么， ‘键盘'的Word2Vec向量以 model[keyboard]的方式在Python/Gensim中获得。 定义这个向量为 $ v_{keyboard} $ 现在考虑构建过程。 因为你目前已经有了一些东西， 你再想到'键盘' 时， 其实已经处于'电脑'和 '硬件' 的情境（上下文）中。 所以你很难把 '键盘‘ 跟 '音乐‘ 联系起来（最起码不直接相关）。 可见， '键盘' 的contextual vector情境向量（定义为 $ v&#94;{'}_{keyboard} $ ) 一定会将方向偏向到 $ v 和 v_{hardware} $ (be biased in its direction towards). ---- 我们要计算 Word2Vec 模型的 cosine 相似度， 当然只跟方向有关。 从直觉上说， $ v_{hardware} 和 v&#94;{'}_{keyboard} $ 的影响应该大于的影响应该大于 v ， 也就是距离越远， 父节点的影响会越小。 为了考虑这个因素， 我再加入了一个 参数 情境递减因子 αα 。 数学表达如下： $$ v&#94;{'} {computer} = v $$ $$ v&#94;{'}{hardware} = (1-\\alpha)v + \\alpha v&#94;{'} $$ $$ v&#94;{'} = (1-\\alpha)v + \\alpha v&#94;{'}_{hardware} $$ 最后， 可以生成实际的导图了， 以下是我使用的算法（我希望行内注释能帮你理解我的工作） from scipy.spatial.distance import cosine from networkx import Graph def build_mind_map ( model , stemmer , root , nodes , alpha = 0.2 ): \"\"\" Returns the Mind-Map in the form of a NetworkX Graph instance. 'model' should be an instance of gensim.models.Word2Vec 'nodes' should be a list of terms, included in the vocabulary of 'model'. 'root' should be the node that is to be used as the root of the Mind Map graph. 'stemmer' should be an instance of StemmingHelper. \"\"\" #This will be the Mind-Map g = Graph () #Ensure that the every node is in the vocabulary of the Word2Vec #model, and that the root itself is included in the given nodes for node in nodes : if node not in model . vocab : raise ValueError ( node + \" not in model's vocabulary\" ) if root not in nodes : raise ValueError ( \"root not in nodes\" ) ##Containers for algorithm run #Initially, all nodes are unvisited unvisited_nodes = set ( nodes ) #Initially, no nodes are visited visited_nodes = set ([]) #The following will map visited node to its contextual vector visited_node_vectors = {} #Thw following will map unvisited nodes to (closest_distance, parent) #parent will obviously be a visited node node_distances = {} #Initialization with respect to root current_node = root visited_node_vectors [ root ] = model [ root ] unvisited_nodes . remove ( root ) visited_nodes . add ( root ) #Build the Mind-Map in n-1 iterations for i in range ( 1 , len ( nodes )): #For every unvisited node 'x' for x in unvisited_nodes : #Compute contextual distance between current node and x dist_from_current = cosine ( visited_node_vectors [ current_node ], model [ x ]) #Get the least contextual distance to x found until now distance = node_distances . get ( x , ( 100 , '' )) #If current node provides a shorter path to x, update x's #distance and parent information if distance [ 0 ] > dist_from_current : node_distances [ x ] = ( dist_from_current , current_node ) #Choose next 'current' as that unvisited node, which has the #lowest contextual distance from any of the visited nodes next_node = min ( unvisited_nodes , key = lambda x : node_distances [ x ][ 0 ]) ##Update all containers parent = node_distances [ next_node ][ 1 ] del node_distances [ next_node ] next_node_vect = (( 1 - alpha ) * model [ next_node ] + alpha * visited_node_vectors [ parent ]) visited_node_vectors [ next_node ] = next_node_vect unvisited_nodes . remove ( next_node ) visited_nodes . add ( next_node ) #Add the link between newly selected node and its parent(from the #visited nodes) to the NetworkX Graph instance g . add_edge ( stemmer . original_form ( parent ) . capitalize (), stemmer . original_form ( next_node ) . capitalize ()) #The new node becomes the current node for the next iteration current_node = next_node return g 备注： 我使用了 NetworkX 的简易图构建架构来完成了思维导图生成的核心任务（使之更易用于可视化）。 要计算 余弦距离， 我使用了 SciPy. 另外注意74和75行， 我使用了上篇博文所写的 StemmingHelper 类， 所以在思维导图中显示的是词干原始形式， 而不是词干。可以将StemmingHelper类直接当做参数 stemmer 传入。 所以， 如果你不需要词干处理， 那就把第4,74,75三行的代码干掉吧。 如果你仔细看过代码， 你会发现， 这看着很像 迪杰斯特拉的单点最短路径， 只是情境不同。 示例输出 原文链接 看着不错， 和我人工画的也挺像的。 其他 还有一些可尝试的东西。 比如加入 bi-grams 和 trigrams。 我相信能让 Word2Vec 模型更强大， 能对文本做出更好的释义。 导图中仍存在多余项， 但它给出了文本的（最？）短长度（相对其他文本挖掘任务来说）， 这种关键词提取算法（我在上文提到过的论文）似乎相当不错。 这段翻译有点不确认（There are some unnecessary terms in the Mind Maps, but given the short length of the texts (compared to most text mining tasks), the Keyword extraction algorithm in the paper I mentioned before, seems really good.） 此方法可用于头脑风暴， 从你选择的一点出发， 这代码框架会给出建议的可连项， 你再做出选择， 然后又可以得到新的推荐——就有点像思维导图助手。 无论怎样， 这都是篇长博文了， 谢谢你坚持着读完全文！（翻译也一样感谢您的阅读）。","tags":"自然语言处理","title":"基于Word2Vec生成基本的思维导图"},{"url":"http://sndnyang.github.io/zhimind-datastruct.html","text":"数据结构可视化 DataStructVis.js--基于vis.js制作的基本数据结构可视化库（散点图基于 echarts.js） 使用说明 全局变量满足， 见下面全局变量说明 选择div容器 -- var container = document.getElementById('datastruct'); 选择数据结构类型 -- 见后面列表 创建 variable = type(container, size) 如structure = LinkedList(container, 10); 全局变量说明 一开始没有进行分析设计， 还在使用多个全局变量， 没有设置成内部变量， 全局变量分别是： nodes nodeSet edges edgeSet data network 支持数据结构 示例见（不建议在这个网页上改大的数值--起码有向图挺卡的）： demo 链表 栈 队列 森林 -- 允许多棵树 树 -- 单棵树，分支不限 二叉树 -- 分支限制不超过2 左右子树目前不能确定， 只能用 空结点（无标签）来设定。 无向图 有向图 -- 目前暂不支持在自动生成图时 设定密度 矩阵 散列表 散点图（2D） 高级数据结构基本可显示成以上结构， 所以感觉不需要。 功能列表 随机初始化 -- 都有简单版本 解析数据 -- 都可解析对应格式数据， 并重新绘制 链表： 列表数据， 如 [1,2,3] 栈： 列表数据 队列： 列表数据 森林： json数据， 不能有环， 如 {1: [2,3,4], 2: [5], 3: [6], 7} 树： json数据， 只能有一个根。 二叉树： json数据， 子结点不能超过2个。 有向图： json数据 无向图： json数据 矩阵： 二维数组数据 散列表: json数据 散点图：二维点列表， 如： [点1, 点2, 点3]， 点格式： [[x], y, class], [x]本来是为了支持3维--TODO：改掉 更新数据 -- 少量有实现自己的更新数据接口： 链表: add 标签， swap 交换标签， remove 值， removeID 移除ID 栈： pop, push 队列： enqueue, dequeue 点标记 -- markNodes([id1, id2, id3, idn]), 将相应id的点修改成红色。 交换标签 -- 待全部实现 setData -- 数据重绘， 数据格式自定义","tags":"zhimind","title":"zhimind-数据结构可视化"},{"url":"http://sndnyang.github.io/zhimind-practice-manual.html","text":"练习使用说明 文本部分同 教程说明 zhimind教程 需要在和文本文件同一路径下有一个 answer.js 文件 answer.js定义 定义函数 initData(): 自定义界面样式， 并为某div 创建可视数据结构， 见 datastructvis 定义 procedure(v): v 是数据， 定义算法过程生成的 stepLog 列表。","tags":"zhimind","title":"zhimind练习开发"},{"url":"http://sndnyang.github.io/zhimind-manual.html","text":"教程和练习使用说明 未注册用户 直接使用， 但不会有记录。 注册用户 需要与思维导图进行关联， 才有进度记录--个人主页没有开发计划， 不知道开发什么，也没时间。 填空（不管带不带公式）题比较难正确，因为基本不是客观题，没有标准答案，但自然语言理解还没学习，没能力开发。 说明 在线可视化编程练习还没有建好框架，所以几乎还不能使用。 重点在于 教程 部分。 新建教程和练习 在个人主页点击 新建教程， 会打开在线编辑， 按markdown格式编辑， 可按 保存， 但暂时未提供临时保存功能，请注意。 在个人主页点击 新建练习， 在线编辑器没有做好，会要求输入 文本文件的链接（暂不支持平台新建，全部从外部读）， 后缀必须是 .mkd 或 .md 或在导图上添加属性时， 属性名填写 练习或教程， 链接输入文本文件的链接， 后缀必须是 .mkd 或 .md。 文件格式 主体部分与markdown 一致 新加题目格式： <div class= \"process\" ><input type= \"hidden\" class= \"answers\" value= \"a@b答案，多选题和多个填空的答案用@分开，单个填空要多个匹配用空格分隔\" /><input type= \"hidden\" class= \"comments\" value= \"提示1#提示2]\" /><button onclick= \"checkQuiz(this, 0)\" > submit </button><br/></div> %} 简单的问题建议写成一行，复杂问题可写成多行，但答案请写成一行，提示可多行，各部分应该不能混杂 formula 答案前加:冒号， 将使用去空格完全匹配的比较方法， 主要是针对 线性代数等可能不要化简的式子。 答案和提示的复杂情况 待实现 示例 zhimind教程示例即在线编辑器 示例 单行模式 <div class= \"process\" ><span><p> 请选择 </p><input type= \"radio\" class= \"quiz\" name= \"quiz\" value= \"a\" > a </input><br/><input type= \"radio\" class= \"quiz\" name= \"quiz\" value= \"b\" > b </input><br/><input type= \"radio\" class= \"quiz\" name= \"quiz\" value= \"c\" > c </input><br/><input type= \"radio\" class= \"quiz\" name= \"quiz\" value= \"d\" > d </input><br/></span><br/><input type= \"hidden\" class= \"answers\" value= \"d\" /><input type= \"hidden\" class= \"comments\" value= \"随便#}单选\" /><button onclick= \"checkQuiz(this, 1)\" > submit </button><br/></div> <div class= \"process\" ><span><p> 请选择 </p><input type= \"checkbox\" class= \"quiz\" name= \"quiz\" value= \"a\" > a </input><br/><input type= \"checkbox\" class= \"quiz\" name= \"quiz\" value= \"b\" > b </input><br/><input type= \"checkbox\" class= \"quiz\" name= \"quiz\" value= \"c\" > c </input><br/><input type= \"checkbox\" class= \"quiz\" name= \"quiz\" value= \"d\" > d </input><br/></span><br/><input type= \"hidden\" class= \"answers\" value= \"d\" /><input type= \"hidden\" class= \"comments\" value= \"随便#}多选\" /><button onclick= \"checkQuiz(this, 2)\" > submit </button><br/></div> <div class= \"process\" ><span><p> 请填空: <input type= \"text\" class= \"quiz\" /> 是有意义的, <input type= \"text\" class= \"quiz\" /> 也是有意义的 </p></span><br/><input type= \"hidden\" class= \"answers\" value= \"教育@数学\" /><input type= \"hidden\" class= \"comments\" value= \"随便写点什么#不想写也可以#}填空\" /><button onclick= \"checkQuiz(this, 3)\" > submit </button><br/></div> <div class= \"process\" ><span><p> 请填空:公式 <input type= \"text\" class= \"quiz formula\" onchange= \"Preview.Update(this)\" /> 预览: submit <div class=\"process\"><span><p><input type=\"text\" class=\"quiz formula\" onchange=\"Preview.Update(this)\"/> 预览: submit 多行模式 且 选项或匹配提示 #提示匹配模式 : 提示的内容 < div class = \"process\" >< span >< p > 你觉得评估标准应该是什么？ </ p >< input type = \"checkbox\" class = \"quiz\" name = \"quiz\" value = \"w范数值越小越好\" > w 范数值越小越好 </ input >< br />< input type = \"checkbox\" class = \"quiz\" name = \"quiz\" value = \"超平面到所有点的距离之和越大越好\" > 超平面到所有点的距离之和越大越好 </ input >< br />< input type = \"checkbox\" class = \"quiz\" name = \"quiz\" value = \"超平面到最近点的距离越大越好\" > 超平面到最近点的距离越大越好 </ input >< br />< input type = \"checkbox\" class = \"quiz\" name = \"quiz\" value = \"w范数越大越好\" > w 范数越大越好 </ input >< br /></ span >< br />< input type = \"hidden\" class = \"answers\" value = \"超平面到最近点的距离越大越好\" />< input type = \"hidden\" class = \"comments\" value = \"w范数:10x+8=0是否等价于5x+4=0?,所有点:考虑极端情况，一条线离两个类的几个点非常近，离其他点非常远，效果并不好\" />< button onclick = \"checkQuiz(this, 6)\" > submit </ button >< br /></ div > 多步推导， process 类型， 答案格式：每空 @ 答案 [ : 前提步骤 ][ : 公式 ( mathjax ) 或图片 ] < div class = \"process\" >< input type = \"hidden\" class = \"answers\" value = \"a@b:`x+y`@c:a,b:$w&#94;Tx+b=0$@d:b:![x](http://blog.pluskid.org/wp-content/uploads/2010/09/Hyper-Plane.png)@e:d:`x/y`@Q.E.D.:e,c\" />< input type = \"hidden\" class = \"comments\" value = \"请把w作为系数写在x前面&#10; #但维度是否对上，没对上要如何对上？#偏置项都是kx+b哪见过kx-b了#答案w&#94;Tx + b = 0&#10; \" />< button onclick = \"checkQuiz(this, 7)\" > submit </ button >< br /></ div > 嵌套模式 在提示部分递归嵌套问题 -- 目前只支持 text 和 radio 嵌套，其他感觉没必要，能显示但判断不了对错。 < div class = \"process\" >< span >< p > 你觉得评估标准应该是什么？ </ p >< input type = \"checkbox\" class = \"quiz\" name = \"quiz\" value = \"w范数值越小越好\" > w 范数值越小越好 </ input >< br />< input type = \"checkbox\" class = \"quiz\" name = \"quiz\" value = \"超平面到所有点的距离之和越大越好\" > 超平面到所有点的距离之和越大越好 </ input >< br />< input type = \"checkbox\" class = \"quiz\" name = \"quiz\" value = \"超平面到最近点的距离越大越好\" > 超平面到最近点的距离越大越好 </ input >< br />< input type = \"checkbox\" class = \"quiz\" name = \"quiz\" value = \"w范数越大越好\" > w 范数越大越好 </ input >< br /></ span >< br />< input type = \"hidden\" class = \"answers\" value = \"超平面到最近点的距离越大越好@2\" />< input type = \"hidden\" class = \"comments\" value = \"w范数:10x+8=0是否等价于5x+4=0？#所有点:{%text|考虑极端情况_@这个#:&#10; {%radio|这两个情况哪个好？&amp;这个&amp;那个&#10; @那个#这个:这怎么聊下去呢?%}%}&#10; \" />< button onclick = \"checkQuiz(this, 8)\" > submit </ button >< br /></ div > 其他说明 填空和公式题 题干必填 ， 用 下划线 _ 代表一个空 填空使用关键字查找方式， 多关键字逻辑关系使用 & 表示且, | 表示或， 支持一层括号，默认 或 优先级高于 且（与顺序无关），所以有时候需要括号调整顺序。 示例： 请填空: 是有意义的, 也是有意义的 submit 思维导图使用步骤 注册登录 个人主页点创建新思维导图 或 首页直接点新导图 编辑导图（存在bug， 未解决） 鼠标在点上悬停时， 会显示四个小图标（有bug,可能被另一个tooltip挡住）， 分别为： 插入子结点 删除当前结点 修改结点名称 添加新属性（即外链） 快捷键方法。 鼠标选中一点后 快捷键insert插入子结点 enter 插入同级结点（根结点不能enter） delete 删除该结点 保存到json文件。 未注册不能保存云上， 仍可以导出 json 格式文件。 查找他人的导图。 点击导航栏上的 推荐。 在思维导图列表下点击一个打开导图（目前只有几个）， 点击存档可直接保存。可到个人主页查看。 感想 目标是能智能地批改 数学、 物理等客观题作业， 并智能地给以思路提示。 但还没有想法。需要自然语言理解。 选择题简单， 但提示不智能——选择题还算好了。 填空题难准确。提示也很死。 公式比较都比较困难， sympy 的 simplify_logic 凑合用， 但只认普通函数， 线性代数或方程, 如 $ w&#94;Tx+b=0 $， 带 = 号， 线性代数转置 &#94;T 都不对。 理想境界 就是对多步的计算题 和 证明题进行智能批改、 提示。 更新日志 2016-11-14 前后台大改造， 多步推导 及 问题嵌套都支持， 界面优化。 准备开始 自然语言部分。 提示更智能。 2016-10-18 改造，支持多行模式——但提示部分json格式是否已经可行还不确认。 决定使用#key:value1-values2#key2:value3-value4的格式，但主要还是看需求。 2016-10-5 界面小调整， 操作有区别 公式添加 : 作分类， 有需要还可以在:前加类别","tags":"zhimind","title":"zhimind使用说明"},{"url":"http://sndnyang.github.io/total-probability.html","text":"问题引入 已知色盲基因由X染色体携带，且若男性的X染色体有此基因则 男性患色盲，女性则要两个X染色体均有此基因才患 色盲，而两个X是否有色盲基因是独立的。 若色盲基因出现概率为0.08。又设男女婴出生比为110:100。问一新生儿有色盲的概率是多少？ 我们来分析这个问题。 设：\"新生儿有色盲\"为事件A，","tags":"数学","title":"全概率公式"},{"url":"http://sndnyang.github.io/svm-1-max-margin.html","text":"本文参考以下文献： pluskid支持向量机系列博文，特别是图片引用 pluskid-svm Andrew Ng在斯坦福的cs229讲义， cs229 李航《统计学习方法》 邓乃扬、田英杰《支持向量机》 学堂在线 袁博 数据挖掘：理论与算法 文笔不好，专业水平也很次，若有意见或建议，欢迎通过下方微博或邮箱联系。 我原则上会从什么开始？ 问题 定义 故事 历史 submit 导读 本文假设读者已经了解机器学习领域里的 分类 和 回归 的概念， 若不懂， 请参考 机器学习类型 支持向量机support vector machine(SVM)有着很长很长的故事和历史， 学堂在线的 数据挖掘：理论与算法 有介绍。 先下结论， SVM是效果最好、 现成可用、学术工业都常用的分类方法之一。 所以， 让我们从线性二分类这个问题开始， 慢慢引出SVM。 下面直接给出分类问题的一种数学符号定义。 分类问题定义 先假设数据线性可分。 符号定义： 数据集 $X = [x_1,...,x_m], 其中 x_i$是一个n维列向量。(我觉得 n行m列有点别扭，习惯于$X$ 是 $m \\times n$维矩阵。) 对应类别 $y$, 用 1，-1 或 0，1 代表不同的分类。 分类问题就是要在 n维的数据空间中把各类数据分隔开，比如找出一个超平面，或构造出一棵 决策树#todo 。 这里是线性分类问题， 则假设要找的 该超平面方程为 f(x) = 0, 那么定义： 超平面上的点都有 f(x) = 0 对 f(x) < 0的点， 有 y = -1 对 f(x) > 0的点， 有 y = 1 用于提神的第一个问题, 我现在在介绍什么分类问题？ 多类 二类 svm 贝叶斯 逻辑 一次 二次 submit 线性分类器 再假设该超平面的权重系数向量w为 n维列向量 ， bias偏置项为 b， x为$n\\times m$维矩阵。 用于醒脑的第二个问题， 这个超平面的方程(f(x)=0)表示为 预览: submit 注意，矩阵相乘不要乘号，且维数要对上 分类结果-超平面 所以超平面方程可表示为： $w&#94;Tx + b = 0， 即 f(x) = w&#94;Tx + b$ 举个栗子， 超平面在二维空间中就是一条直线, 如图所示(直接从pluskid博客上引用过来)： 要求出这一条直线（超平面）， 我们有 诸如 感知机 和 逻辑回归 等方法， 都可以找到这么一个超平面。 下一步你觉得应该考虑什么，请自由发挥。 submit 无数个超平面 以上图为例， 有无数条直线可以把红蓝点正确地划分开。 比如用逻辑回归， 不同的初始值或学习速率都可能得到不同的超平面、权重向量和偏置项。 既然有无数种可能， 那么只要条件允许（时间、 金钱等）， 我们就要精益求精， 做到最好！要找到最好的一个超平面。 问题就来了， 哪个超平面是最好的呢？ 要选出最好的，肯定要知道如何评估， 评估标准是什么。 你觉得评估标准应该是什么？（在都正确地划分了数据集的前提下） w范数值越小越好 超平面到所有点的距离之和越大越好 超平面到最近点的距离越大越好 w范数越大越好 submit 评估超平面-距离 范数没用， 在线性分类里， w和b 乘上相同的常数，仍是同一个超平面， 一个直观例子， 10x+8 = 0 等价于 5x+4=0， 对应同一条线。 所有点距离之和 计算量可能确实大了点， 可能还有其他原因，不是本文讨论的内容。 目前来看，超平面到最近点的距离是最合适的方案。至于会不会有人找出其他方法~~~这不知道， 但一切皆有可能嘛。 显然，这个距离越大越好， 所以我们要做的是什么？ submit 目标记心中 没错， 找出最近点距离最大的超平面。 很自然的问题来了，谁不知道啊，但怎么找啊，还想找消失的宝藏、沉船呢，怎么找啊。 第一个方案， 暴力、蛮力搜索整个假设空间， 无限大， 肯定不行。 方案二，能不能在感知机或逻辑回归找到一个超平面的基础上进行优化？ 能 不能 submit 能做到吗？ 来分析一下这个过程： 用其他机器学习方法学到一个超平面 想办法调整、优化超平面， 要找到最近点、求出距离，达到最优后，结束，否则转2. 那就有两个问题， 一是如何调整超平面（参数），二是什么时候达到最优，如何判断出来。编不下去了， 只是感觉和线性svm还是有点共通之处。 我们还是先来继续讲第三种方案，数学推导方案:(, 一般教材里svm都是这样玩的。 就回归形式化、数学化道路， 根据 机器学习三要素#todo ，我们又来分析 svm的模型、策略和算法啦。 第一步是做什么？ 建模 策略 算法 submit 模型数学化开始 我们说超平面到最近点距离最大， 这里距离应该是什么？ f(x)的绝对值 点到超平面的垂线距离 不知道 编不出来选项了 submit 几何间隔 样本点(x, y)到超平面$w&#94;Tx+b=0$的垂线距离 定义为 几何间隔(geometrical margin ) 。 关于 $f(x) = w&#94;Tx+b=0$的超平面P 和 点z 的几何距离的计算或者说通用式， 建议不熟悉、 没印象的读者动手推导一下。 先上结论： 点到超平面的几何距离(可能有误， 所以说要动手推导一下) $$ \\gamma = \\frac{w&#94;Tx+b}{\\|w\\|}=\\frac{f(x)}{\\|w\\|} $$ 因为有好几种数学符号的写法， 李航的《统计学习方法》和Andrew Ng的机器学习讲义是用w*x+b, 用了$||w||$ 没有考虑b，有些人的符号使用的是 $\\theta&#94;T X = 0$, 有时候我就纠结于 b 的影响。 另外 |w| 指w的模，一般就是$||w||_2$, w的2范数。 继续?还是发现了什么问题? submit 有问题？ 本式有个问题， 不能做为距离使用， 请指出 |w|未定义 |w|是范数，范数有很多种 可能为负值 跟类别y无关 太简单 推导太复杂 submit 上式存在负值的可能， 距离是不可能是负值的， 所以要取个绝对值， 等价于乘以 y ， 因为 f(x)小于0时, y 正好等于 -1。 改写式子， 仍使用 gamma 记号（很快符号就要混乱了） $$ 几何距离: \\gamma = |\\gamma| = y\\gamma = y \\frac{w&#94;Tx+b}{\\|w\\|}=\\frac{y f(x)}{\\|w\\|} \\tag{1} $$ 所以， 单点到超平面距离知道怎么求了， 下一步呢？ 随机找一点的几何距离 求全部点的距离之和 找最远点 找最近点 求平均值 求平均值后找最接近平均值的一个点 submit 训练集的几何间隔 已知某个点到超平面的几何间隔 $$ \\gamma = y \\frac{w&#94;Tx+b}{\\|w\\|}=\\frac{y f(x)}{\\|w\\|} $$ 我们知道， 要找的是超平面的最近点， 这个如何描写成数学表达式， 应该有头绪吧？ 有，会做 完全不理解 程序找最小会，数学式子是什么鬼 submit 定义 最近点到其超平面的几何距离， 就是所有点到该超平面的几何距离的最小值， 定义为该平面关于数据集的几何间隔(geometrical margin ) 所以有 几何间隔: $$ \\tilde{\\gamma} = \\min_{i=1,...,m} \\gamma&#94;{(i)} \\tag{2} $$ 同时我们还可以知道支持向量的概念。 如图所示： 函数间隔 一般教材里会同时介绍 函数间隔 functional margin。 但我目前觉得它没用。 直接上 函数间隔 数学式 $$\\hat{\\gamma}=y(w&#94;Tx+b)=yf(x) = \\tilde{\\gamma} ||w|| \\tag{3}$$ 继续， 已知训练集的几何间隔， $$ \\tilde{\\gamma} = \\min_{i=1,...,m} \\gamma&#94;{(i)} $$ 那下一步呢? 不知道 梯度下降法 求最大化 蒙特卡罗模拟 酱油 submit 最大间隔 还记得我们要做什么吧？ 找出到相应最近点距离最大的超平面， 也就是几何间隔最大的 超平面。重复的简单抽象问题不再提。 如下： $$ \\begin{align} &\\max_{w,b} \\tilde{\\gamma} \\ & \\begin{array} &s.t. &\\frac{y&#94;{(i)}(w&#94;Tx&#94;{(i)} + b)}{||w||} ≥ \\tilde{\\gamma}, &i=1,\\ldots,m\\ \\end{array} \\end{align} $$ 即： $$ \\begin{align} &\\max_{w,b} \\min_{i=1,...,m} \\frac{y&#94;{(i)}(w&#94;Tx&#94;{(i)}+b)}{\\|w\\|} \\end{align} $$ 为美观着想，求最近点部分，即求最小的min符号及i标号先省略掉，希望不要影响理解，主要是为了我写latex表达式轻松点。 $$ \\begin{align} &\\max_{w,b} \\frac{y(w&#94;Tx+b)}{\\|w\\|} \\ & \\begin{array} &s.t. &y&#94;{(i)}(w&#94;Tx&#94;{(i)} + b) ≥ \\tilde{\\gamma}||w||, &i=1,\\ldots,m\\ \\end{array} \\end{align} \\tag{4} $$ 作者注： 因为是涉及多个来源，符号不尽相同，中英文，概念上可能也有点问题。 一帆风顺的旅途到此结束 到(4)式子， 应该是已经得到一个待优化的问题。 那我们还能再进一步优化吗？ 发现、分析、探索未知世界时， 是没有具体的方向的， 但掌握一些指导性的原则，培养一些好的思维方法和习惯， 积累丰富的知识储备， 当我们遇到问题时， 就很可能有一种\"直觉\"指引着你去解决。 没有标准答案， 如果你要来优化，会怎么做？哪个地方可以简化。写思路。 你的想法是？ submit 优化 我们已经发现了问题——求最大间隔的超平面， 整体来看，也是个最优化的问题，所以也许最优化的方法、原则可以应用于此。 来看这个式子 $$ \\max_{w,b} \\frac{y(w&#94;Tx+b)}{\\|w\\|}=\\frac{\\hat{\\gamma}}{||w||} $$ 我书读得少，大概有两个最优化的原则（对错不知）： 最优化需要求导，y 就是给它取正，取正 = 取绝对值。 那么平方也能取正， 小本子上记下， 来个求平方？但式子里已经存在除法，再求平方更复杂了， PASS。 最优化经常采取 固定部分 的方法， 所以来看： $w ||w||$ 和 $b$ 是未知待求，待优化的， 扔掉就没事做了，应该不能固定。 那上面的这个 $\\hat{\\gamma}$ 是不是能做点文章呢? 看来只剩$\\hat{\\gamma}=y(w&#94;Tx+b) $可以一试了， y解释过，不能求平方， 那只剩$w&#94;Tx+b$， 它是个线性函数， 对吧？ 它有什么性质，能帮助化简、固定呢？ 不知道 乘常数不变性 最简最低次性 可线性组合 submit 前面有提过相关的例子。 乘常数不变 $$ \\begin{align} &\\max_{w,b} \\frac{y(w&#94;Tx+b)}{\\|w\\|} \\ & \\begin{array} &s.t. &y&#94;{(i)}(w&#94;Tx&#94;{(i)} + b) ≥ \\tilde{\\gamma}||w||, &i=1,\\ldots,m\\ \\end{array} \\end{align} $$ 给$\\hat{\\gamma}=y(w&#94;Tx+b) 的 w和b$乘上个常数， 对目标函数优化的结果有影响吗？ 肯定没影响。为什么？ 最近点还是最近点 几何间隔不变 函数间隔不变 超平面不变 不知道 submit 还有呢？ 不等式约束怎么样？ 也是不影响，为什么？ 最近点还是最近点 几何间隔不变 函数间隔不变 超平面不变 编不出来了 submit 结论 所以我们就知道， w和b乘常数： 对目标函数的优化没影响， 这个比较直观， 因为方程变化后， 最近点还是最近点， 几何间隔（最近点到超平面的几何距离 ）$\\tilde{\\gamma}$不变。 不等式约束不变化， 没影响。 几何间隔$\\tilde{\\gamma}$不变， 左式变化的比例， 会等比地影响到右边式子的$||w||$。 师出有名 那我们就开始搞坏事了~~~ $$ \\begin{align} &\\max_{w,b} \\frac{y(w&#94;Tx+b)}{\\|w\\|} = \\frac{\\hat{\\gamma}}{||w||} \\ & \\begin{array} &s.t. &y&#94;{(i)}(w&#94;Tx&#94;{(i)} + b) ≥ \\tilde{\\gamma}||w||, &i=1,\\ldots,m\\ \\end{array} \\end{align} \\tag{5} $$ 就要去给它变一下。 怎么变？ 令函数间隔（分子）等于0 令函数间隔等于1 除以函数间隔 除以||w|| submit 理由 必须搞清楚每一步， 李航的《统计学习方法》写： $ \\hat { \\gamma} $ 的取值不影响最优化问题的解。假设将w和b按比例改变为 lw和lb， 函数间隔变成 $ l \\hat { \\gamma} $ ，函数间隔 的这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响。这样，就可以取 $ \\hat { \\gamma}=1 $ 代入上面的最优化问题里。 pluskid 写的是 固定变量， 固定的方式有两种： 1. 固定 $\\|w\\|$ 2. 固定 $\\hat{\\gamma}$ ， 出于方便推导和优化的目的，选择第二种 Ng的描述为： Recall our earlier discussion that we can add an arbitrary scaling constraint on w and b without changing anything. This is the key idea we'll use now. We will introduce the scaling constraint that the functional margin of w, b with respect to the training set must be 1: $$\\hat{\\gamma} = 1.$$ Since multiplying w and b by some constant results in the functional margin being multiplied by that same constant, this is indeed a scaling constraint, and can be satisfied by rescaling w, b. Plugging this into our problem above, and noting that maximizing $$\\hat{\\gamma}/||w|| = 1/||w||$$ 还有直接说把margin两边线定义成 +1 -1的， 如图： 他们说的都没错，但里面省了不少步骤，就不那么让人信服了。 比较好的解释 来自邓乃扬《支持向量机》，还是上图的margin的两条边线， 因为超平面是 $w&#94;Tx+b=0$， 所以，这两条线（到超平面距离相等）分别是 $w&#94;Tx+b=k 和 w&#94;Tx+b=-k$。 为了把k和-k置成+1和-1，显然原w和b要分别除以一个k，得到$w&#94;Tx+b=1 和 w&#94;Tx+b=-1$， 两条线之间的距离就是\"间隔\"，等于$\\frac{2}{\\|w\\|}$。 另外不管是从图上还是数学式上，我们都知道 $k = \\hat{\\gamma}$，也就是函数间隔。 所以，(5)可以给$\\hat{\\gamma}$除上它自己（函数间隔），也就等效于令它等于1。 结果是一样的，但需要了解背后的原理，之前说w,b可以乘常数，但不验证下$\\hat{\\gamma}$， 怎么知道它能当常数用呢？ 继续 令 $\\hat{\\gamma}=1$ 则我们的目标函数化为： $$ \\begin{align} &\\max_{w,b} \\frac{1}{\\|w\\|} \\ & \\begin{array} & s.t., y_i(w&#94;Tx_i+b)\\geq 1, i=1,\\ldots,m\\ \\end{array} \\end{align} $$ 通过求解这个问题，我们就可以找到一个 margin 最大的 classifier ，如下图所示，中间的红色线条是 Optimal Hyper Plane ，另外两条线到红线的距离都是等于 $\\tilde{\\gamma}$ 的： 倒数变形 到上一步时， 我们得到了一个优化问题， 优化一个 1/||w|| , 一个倒数， 不好。 显然： $\\max \\frac{1}{||w||}$等价于$\\min ||w||$ 有什么问题吗? 没问题 不成立 不能求导 不知道 值不连续 submit 等效变形 对，线性函数的怎么求导？优化不求导，怎么搞得了。那就改成平方，还是显然，对求最优化问题没影响， 另外为了求导结果漂亮一点， 再加上一个常系数， 最终结果如下： $$ \\begin{align} &\\min_{w,b} \\frac{1}{2}\\|w\\|&#94;2 \\ & \\begin{array} & s.t., y_i(w&#94;Tx_i+b)\\geq 1, i=1,\\ldots,m\\ \\end{array} \\end{align} $$ 最终我们得到了一个凸优化问题，或者更具体地说，它是一个二次优化问题——目标函数是二次的，约束条件是线性的。这个问题可以用任何现成的 QP (Quadratic Programming) 的优化包进行求解——具体求解不是机器学习课程的教学内容。 所以，学完了吗？ 结束，撒花 想太多 不知道 submit 总结 回忆一下， 我们整个过程做了哪些事。 从二元线性分类开始， 我们明确目标是求超平面， 分类器都是这么干的。 超平面有很多个， 我们就要有对超平面的评估标准。 评估标准就要计算距离， 我们就用上了几何距离。 有了几何距离， 我们就有最近点的几何距离， 就是几何间隔。 知道了几何间隔， 我们就要求出最大的几何间隔 及 对应的超平面， 绕了一圈， 又绕回到第1条。 优化公式里的 $\\hat{\\gamma}=y(w&#94;Tx+b)$ 有着特殊的性质， 取值对最优化问题的解不影响。 所以我们把它设为1了。 最后再把倒数形式的$\\max \\frac{1}{||w||}$ 转成 $\\min \\frac{1}{2}\\|w\\|&#94;2$ 完结撒花！ 恭喜！！！ 一路艰辛， SVM 这才完成了最简单的部分， 后面理论 的推导更加复杂， 做好心理准备吧！ 下一节 支持向量机的对偶形式#todo 如果您觉得这种学习方式有帮助的话， 呃， 那就好~~~","tags":"机器学习","title":"支持向量机系列之最大间隔"},{"url":"http://sndnyang.github.io/svm-2-dual.html","text":"免责说明 本文参考以下文献： pluskid支持向量机系列博文，特别是图片引用 pluskid-svm Andrew Ng在斯坦福的cs229讲义， cs229 jerrylead 博文 jerrylead-svm , pluskid 的公式显示出问题， 看着头大 在下文笔较烂， 恐贻笑大方。 不过因为是markdown 写的， 有不足， 改进比较方便， 所以欢迎提出意见及建议，找出问题， 谢谢。 导读 在上一篇 svm介绍 中， 我们已经得到了如下一个公式： $$ \\begin{align} &\\min_{w,b} \\frac{1}{2}\\|w\\|&#94;2 \\ & \\begin{array} & s.t., y_i(w&#94;Tx_i+b)\\geq 1, i=1,\\ldots,m\\ \\end{array} \\end{align} $$ 这个式子或者说优化问题 大家都说是有现成的优化方法来求解， 所以完成了一部分内容。 既然已经能用了， 为什么还要继续讲下去， 肯定是有更优的方案。 这也是明摆的事。 如果说上面的多数内容是普通读者(会来学习svm或机器学习的人)能顺利理解每一步公式的话， 后面的内容对于很多人就困难得多了， 特别是对高数没学好的（比如我）来说。 我现在强行凑文章， 也不合适， 还是得先学好数学了再说。","tags":"机器学习","title":"支持向量机系列二之对偶化"},{"url":"http://sndnyang.github.io/conditional-probability.html","text":"问题引入 讲概念、 讲定义都是太抽象的， 还是从问题开始， 先有个直观的印象。 例一 抽奖 有M个人要抽N张入场券，若某人第k个抽，但在此之前已知前k-1个均未抽到入场券，问此时他抽到的概率是多少， 与不知道前k-1人的状态时的概率相比， 是否有变化？ 设 A：\"第k个人抽到入场券\" B：\"前k-1个人均未抽到入场券\" 已知在不考虑B的情况下， P(A) = N/M submit 直观分析 原题\"B已经发生的情况下， A的概率\"里包含了几个新的事件， 需要计算概率， 分别是 AB A|B B|A B AorB submit 计算概率 因为本人水平有限， 暂未实现多个填空的问题 所以请自行在草稿纸上计算出 P(B) 和 P(AB) 再进行下一步——如果直接点下一步， 也行 相关 根据 P(AB) 和 P(B), 可知 P(AB)/P(B) = N / (M-k+1) = P(A*) 这样， 我们就发现 P(A*) = P(AB)/P(B) 这是巧合吗？ 例二 人口调查 暂略 例三 肿瘤 暂略 too 总结 根据前面的例子， 可知此时A发生的概率已经有了新的意义。 我们想想， 叫什么？ 在已知B发生的条件下， A发生的概率 有了新定义， 就要引入一个新的记法， 记做 P(A|B) |后面的B 是已知条件 条件概率定义 设A，B为事件，P(B)>0，定义 P(A|B) = P(AB)/P(B) 称为是B发生条件下A发生的概率（conditional probability of the event A given the event B has occurred） 验证条件概率性质 略 乘法公式 虽然最好的方法是从问题来引出 我们需要乘法公式， 但我找不出来例子就算了， 乘法公式太简单了 由条件概率公式变形， 得到乘法公式： P(AB) = P(A|B)P(B) 意义： 计算积事件的概率， 通常可避免计算组合数 独立性即条件概率成立的条件 问题引出 设一个家庭生男孩、女孩是等可能的。 考察任一两个孩子家庭，分别求\"老二是女孩\"的概率和在\"老大是男孩\"的条件下\"老二是女孩\"的概率 一道常识题， 都是50% 设A为\"老二是女孩\",B为\"老大是男孩\" 则 S = {(bb),(bg),(gb),(gg)} A = {(bg),(gg)} B = {(bb),(bg)} AB = {(bg)} 条件概率 P(A|B) = 1/2 结论 上例中条件概率与无条件概率是一样的，说明\"老大是男孩\"这一事件对\"老二是女孩\"这一事件的概率没有影响，或者说这两个事件是独立的。 独立性定义 一般地，若P(A)=P(A|B)，或等价地若P(AB)=P(A)P(B)，称事件A，B独立(independent). 独立性判断 呃， 经验~~~ 多事件独立 事件 $A_1,...,A_n $相互独立，指下列 $2&#94;n-n-1$个等式均成立 公式略——任意2到n个事件的组合","tags":"数学","title":"条件概率"},{"url":"http://sndnyang.github.io/prob-measure.html","text":"定义 学派 频率派 贝叶斯派 公理化整合 计算概率 频率说： 在系列重复随机试验中， 考察随机事件发生的频率（稳定值） 样本信息 主观说（贝叶斯派）：根据以往的资料或经验， 形成的关于随机事件可能性的印象 先验信息 等可能说（贝叶斯派）：基本事件的发生没有偏向性，都是等可能的 无信息 公理化定义 概率：定义在S（样本空间）的事件族上的实值函数 P， 满足： 非负， $ P(A) \\geq 0 , \\forall A \\in S $ 规范， 必然事件概率为1 P(S) = 1 可列可加， 互斥事件之和的概率 = 各自概率之和 性质 不可能事件概率为零 $ P(\\varnothing) = 0 $ 有限可加性 单调性 事件概率小于等于 1 对立事件 加法公式 推广","tags":"数学","title":"概率-度量可能性"},{"url":"http://sndnyang.github.io/prob-stat-intro.html","text":"概率直观定义 概率就是一个事情发生的可能性。 但这个感性的认识并不严格， 不能准确地描述 概率定义严格化 研究对象 什么问题（现象、 事件）有可能性？ 这类问题才是概率要研究的内容 很明显， 简单来分， 自然现象可分为两类 确定性现象， 没概率的事 不确定现象(可能) 无规律现象 随机现象：大量重复实验存在统计规律性的现象，概率研究对象 数理统计 研究对象 研究对象： 数据 收集 整理 分析 推断或预测 概率与统计 数理统计为概率论面向实际问题提供联系桥梁。 概率论 为 数理统计方法 的合理性提供理论证明。","tags":"数学","title":"概统介绍"},{"url":"http://sndnyang.github.io/quick-sort.html","text":"欢迎在zhimind上学习 本教程将尽我所能，指导您理解、练习并掌握快速排序算法。虽然目前仅限于使用javascript, 但算法对其他语言是通用的， 具体的语言不会影响您的理解。 左侧是javascript输入控制台，现在先告诉我们您的名字, 请输入 setName('您的昵称') 另外，有一些有用的命令: 输入 help() 来查看帮助吧！ 排序问题 现实中非常常见的一个问题就是， 给定一堆乱序的数字或名称， 给它们排好序。 之前可能已经学过以下几种排序算法: 冒泡排序 选择排序 插入排序 归并排序 堆排序 我们现在又要用新的方法实现了, 它就是 快速排序 。 在开始正题之前， 先来查看待排序数组 v[] 的数据 ， 请输入 print_list() 分治思想 不同于选择或冒泡排序逐个处理的暴力搜索思想， 我 们现在来考虑分治的思想 submit 划分的方式 从中间位置划分，分后治之的是归并排序。 如果不指定位置划分， 有其他方法划分否？ 另外能否边分边治？ 还有哪些可能的划分方式? 随机 值大小 位置 不知道 submit 怎么选值呢? 不按位置，那我们只能按实际值的大小来划分。 那怎么选划分值呢？ 随机一个 第一个 最后一个 中间那一个 平均值 中位数 不知道 submit 准备划分 既然已经知道了划分的k值， 那么现在就来划分数组了。 目标就是让小于k的数放到左边， 大于等于k的数在右边，k在中间 想法是简单，可这不是废话吗？排好序的就有这性质啊。 那乱序的数组不排序怎么做呢？ 分析初始和目标状态 关于初始状态，只知道 k在数组第一个位置 k左边没有数，为空 右集合是一堆乱序数。 反过来看目标状态，是不是也是三个集合： k自己是一个集合 比k小的数组成一个集合 不比k小的数组成一个集合（除k） 正好对应上 既然如此， 就定义两个空集合（k本身是一个集合）， 分别叫做 left, right. 请写代码 left=[],right=[] 遍历划分 很简单， 我们只要遍历一遍数组v[]，将里面的元素按比k值的大小结果，相应地添加到left和right 请实现， 得到left, right结果 请从第二位开始从左往右遍历， 否则顺序不匹配， 也过不了 递归解决子问题 目前求得三个集合， k, 比k小， 不比k小。 然后要递归处理子问题， 首先， 当问题规模 <= 1时， 不需要再划分， 直接返回该数组-- 作为递归出口 其次， 子问题处理完毕后， 三块集合要连接， 才能向更上层返回。 请基于前面几步， 编写一个递归函数quicksort， 并调用它来处理数组v[] 快速排序优化， 请待后续分解 以上部分是最容易实现的快速排序方法， 但有浪费一些空间， 教科书上最常见的不需要这部分空间, 会有一些小陷阱， 我暂时写不下去了--打算先写点别的 如何求解 所以现在问题就是 如何求 i(index), 就是k的目标位置 从初始状态开始， k 在第一个位置，而js数组下标从0开始， 所以 i = 0 目标位置可以由什么直接决定? 不比k小的个数 比k大的个数 比k小的个数 和k相等的个数 submit 那请找出比 k 小的值的个数 so easy! 请在得到该值后（直接数个数也行），建议用循环比较（文本框问题全部写成一行)，并保存到变量里， 输入 变量或值 哪里不对？ 看起来位置是知道了， 但感觉哪里不对？ 哪里都不对 知道k位置，还不知道左右集合大小 只知道k位置，其他值没说 有哪里不对 submit 能否在计算 k 位置的同时， 把集合划分好？ 之前你是怎么统计 <k 的值的个数的？ 还记得吧？ 从左往右扫描时（或从右往左），假设不扫描k所在的第一位 是不是 看到一个小的数，就加1呢？ 如果求的是 >= k 的个数， 也是类似的， 不是吗？ 做成动画的话， 是不是 k 的位置往右移动了一格呢？ 那有什么问题吗？ 原位置留的坑怎么办？ 不知道看到哪里了？ 新位置的值怎么办？ 这些都不是问题 submit 向右移动到新位置 新位置上的值怎么办？ 只用下标记位置 用临时变量保存起来 submit 原位置怎么办？ 留空 k 我也不知道 随便找个小于k的数填上 正看到的那个小于k的数来填上 submit 顺利？ 上一步有什么问题？ 想想上上一步 新位置上的值是怎么处理的？ 有问题？我不知道 位置只是下标,原位置的实际值会被覆盖 k的位置可能会乱掉 submit 如何避免覆盖？ 当然是 两个位置的值进行交换 用临时变量保存 submit 回顾一下？ 那我们脑海里的动画似乎可以顺利进行了 选择好 k 的值——先假设是数组的第一个数， 当前位置是下标0 数组从左往右扫描时（或从右往左）， 遍历时怎么处理？ 看到一个小于k的数，k的位置向右走一格， 原位置和扫描到的位置进行交换(即小于k的值被移到了k的左边，而在原位置上的数是之前扫描过的不小于k的数，交换后仍然保持在k的右边)。 否则继续遍历， 直到遍历完整个数组。 好， 那我们开始走一遍 先选一个 k 请用代码选出当前数组的第一个值,及下标 i=0;k = ??? ，请不要直接填写具体值 循环遍历 假设从左往右扫描， 都说了边分边治，为什么不呢？难不成还等回来再处理？ 那现在的问题就是， 在我们遍历时， 能不能顺便把小的扔在一起，不小于k的扔在一起？ 假设现在左右两边集合都是空的，遍历时小的放到数组左端， 不小 的加到数组右端， 省略若干字和一步 但想想，这两个集合是互斥的，不重叠的，我把左边的搞好了，k就知道了， 右边自然就剩下并满足了，为什么要小也判断一下，大也判断一下 所以只需要在遍历时， 发现小的就放到数组左端， 像是去排队一样 , 一开始队伍长度为0， 所以我们定义一个变量 var set_length=0 , 也可以用队首队尾概念定为-1，问题都不大 现在请用代码实现 集合长度 变化过程 遍历；逐个处理；（这个暂时没想好 怎么算通过测试， 请随意输入） . 是不是只要知道k 在目标状态时的位置， 并保证左边的都小于k, 右边的都大于等 于k， 就知道左右集合大小了呢， 至于它们内部需要有序吗？我们不管。 分后才治之？ 正确实现 所以整个过程就是， 当遍历到某个值v[i]时， if (v[i]小于k) {v[set_length] = v[i]; set_length++;} 是不是有什么问题呢——对吧， v[set_length] 被覆盖了， 原来的 值就这么随意的get out了吗？所以， 我们不能简单地覆盖， 而是应该交换两个位置的值 下面就请正确实现 整个划分过程吧， 一定要正确地划分！不然不让 过哦， 用很多算法书上的 while 循环或一些算法书上的for循环遍历都 可以，只要满足k 左边的值都小于k,右边的值都不小于k, 请使用已定义 变量 l 或 k 或 global_l, global_k， 代码写成一行 请将 k 目标状态所在位置的值保存在 变量 global_l 中。 参考答案: l = 1; for(var j = 1; j &lt v.length; j++){ // 这个for 循环要写成 一行 if (v[j] &lt k){ swap(v, j, l); l++; } } swap(v, l-1, 0) global_l = l-1; 可以像归并排序那样分完再治吗？ 如果没学过归并排序， 以下几节不用细看 正如之前提到的， 在学习分治算法时， 老师通常会讲 归并排序和快速排序。 如果有学过 归并排序 , 应该有印象， 最常规的归并排序是先不断地递归划分成子问题，直到不可再划分， 等到返回时， 再将子问题进行合并。 所以是先分后治， 分完再治。 那快速排序可以这样吗？我们每层都可以求出划分的位置，是否可以 先递归求子问题， 再返回求解主问题？ 可以 不可以 submit 撒花庆祝！ Congratulations, {{ firstName }}, 你完成了快速排序的学习（中间 少了很多步还没写呢）。 如果觉得有意义, 但没完全掌握或希望复习的话， 用 reset() 重新学习吧！","tags":"算法","title":"快速排序"},{"url":"http://sndnyang.github.io/random-event.html","text":"随机试验 对随机现象的观测 样本和样本空间 试验的每一种结果就是一个样本s 所有可能的结果 就是样本空间S 随机事件 样本空间的子集 事件运算 类集合运算 省略","tags":"数学","title":"随机事件"},{"url":"http://sndnyang.github.io/tu-jie-shu-xue-xue-xi-zhi-ju-zhen.html","text":"matrix 矩阵的英文叫 Matrix， 原意是\"母体， 基质\"， 作者认为西方原意是指为了形成一个整体而填充进去的填充物。 所以 $$ \\begin{pmatrix} 1&2 \\ 3&4 \\end{pmatrix} $$ 在西方人的眼中很可能是数字填充在了括号的空间里。 用黑客帝国来理解呢？ 其实它的英文名是 matrix, 直译应该是母体的意思， 矩阵是个母体， 里面的数字就是啥？ 其实矩阵的译名应该不错， 矩形不一定对， 但目前只见过矩形的。 阵也是个整体， 个体站在需要的位置上。 原文这段主要在吐槽 日文里的行列译法， 而且应该是跟 他们的队列、 排队 相重了。 还有当年日本文字是竖写的， 行、列的意思与现在相反。 矩阵的出现 矩阵公认是根据方程组发明的。 比如一个方程组 $$ \\begin{equation} \\left{ \\begin{aligned} x+5y+2z=9 \\ 4x+6y+z=12 \\ 9x+3y+3z = 6 \\end{aligned} \\right. \\end{equation} $$ 如果方程式再增多， 写起来就非常烦琐了。 于是数学家经过抽象（也就是偷懒）， 发明了 Matrix 概念。 怎么抽象？ 提取系数 提取未知变量 submit 抽象 没错， 系数和变量分开装， 先是变成了这样(latex公式不太会写)： $$ \\begin{equation} \\begin{aligned} (1 \\ 5 \\ 2) \\ (4 \\ 6 \\ 1) \\ (9 \\ 3 \\ 3) \\end{aligned} \\times (x \\ y \\ z) = ? \\end{equation} $$ 再把所有数字放进一个括号里， 就成一个整体 A 了， $$ A=\\begin{pmatrix} 1&5&2 \\ 4&6&1 \\ 9&3&3 \\end{pmatrix} $$ 问题就成了求方程: $$ A*(x \\ y \\ z) = ? $$ 你可能会想:\"A的这些数字之间没联系， 怎么看成整体？\" 所以这就是抽象的威力了， 数学家不只是偷懒， 他们还给矩阵定义了很多严密的规则（运算规则）， 使得大家都认可了这些规则。 所以， 学数学的过程 其实也包括锻炼抽象思维能力， 甚至于具体知识用不上，就只剩下抽象、 逻辑等思维能力了， 不然可以说高等数学真的完全没用。 矩阵的现实意义 之前提到matrix 的原意， 但数学上matrix 矩阵 到底算个什么东西呢？ 教科书上喜欢的定义是： $$ 由 m \\times n 个数组成的 m 行 n列的数表 ， 称为一个 m行n列的矩阵，或 m \\times n 矩阵 $$ 只抽取了数字， 对数字背后潜藏的意义无动于衷。 最关键的是定义加减乘除（貌似很少说除）规则后， 还让你手算~~~ 这样的计算， 只知方法， 不知意义， 就很可能使学生越来越讨厌数学。 原文从销售例子介绍了一堆， 但我觉得 他说的\"看到矩阵， 在脑中就应该浮现出原来的数据表\" 不太现实。 矩阵和数据表 中间还有个方程组这一步， 甚至高观点里矩阵相乘是什么线性变换， 跟现实的市场数据就套不上了。 所以决定直接跳到乘法运算分析这部分(最后面加减也无视了)。 乘法运算为什么是行乘列？ 我们已经得到了一个这样的式子 $$ \\begin{equation} \\begin{pmatrix} 1&5&2 \\ 4&6&1 \\ 9&3&3 \\end{pmatrix} \\times \\begin{pmatrix} x & y & z \\end{pmatrix} = ? \\end{equation} $$ 不对， 课本上是这样的啊 $$ \\begin{equation} \\begin{pmatrix} 1&5&2 \\ 4&6&1 \\ 9&3&3 \\end{pmatrix} \\begin{pmatrix} x \\ y \\ z \\end{pmatrix} \\end{equation} $$ 为什么要竖着排？ 就是这样定义的 节约空间 好玩 不走异常路 不知道 submit 解释 现在解释这个 可能没什么意义， 纯粹科普。 但不好意思的是， 原文里 这段的例子不好打~~~放弃了。 大概意思就是， 如果矩阵乘法使用行乘行的话（不考虑列乘行这种更违背习惯的）， 假设左矩阵（被乘数矩阵？）固定了 那右矩阵再扩展的话， 得纵向延伸也就是继续加新的行， 但计算的结果（矩阵）却是横向延伸，加新的列。 延伸方向不一样。 乘数矩阵添加了第i行， 计算结果矩阵却添加了第i列。 所以改成行乘列的话， 乘数和计算结果矩阵延伸方向就一致， 第j列对应第j列。 符合从左向右看算式的习惯。 不好意思， 内容就这么多~~~ 作者其实在讨论最基本的东西， 可以说 -- 这本书的内容不看也没问题 他重点不是把各种高深的理论解释一遍， 让你全部弄懂。 作者希望的是\"把日常生活和抽象世界（数学）紧紧联系起来， 就能理解数学\"。所以应该是通过他写的这几个例子， 有这方面的意识， 懂得怎么去联系。 说起来， 这话也没错， 但对于有些知识数学知识来说， 很难跟日常生活联系起来。 比如抽象代数群环域， 据说 刚提出时， 连大数学家欧拉都无法理解， 欧拉公式都有点跟现实世界联系不起来了， 何况欧拉看不懂的。 联系的应该是他另外地方说的模板， 也就是包括日常生活经验以及以往的数学知识。 没有模板， 全新的内容学起来就会很痛苦。","tags":"线性代数","title":"图解数学学习之矩阵"},{"url":"http://sndnyang.github.io/alphago-papers.html","text":"参考文献 原文:Mastering the Game of Go with Deep Neural Networks and Tree Search alphago-原理 郑宇,张钧波 alphago-分析 田渊栋 开源代码 alphago-code 战绩 对欧洲冠军 Fan Hui formal games were 1 hour main time plus 3 periods of 30 seconds byoyomi informal games were 3 periods of 30 seconds byoyomi 对李世石 2016.3.9 alpha胜 2016.3.10 alpha胜 2016.3.12 alpha胜 2016.3.13 李世石胜 2016.3.15 alpha胜 期待与柯洁的对战 李世石重下战书 基本定义 棋盘状态 动作 action a : action 动作 暴搜不可行！！！ 组合爆炸！！！ 围棋的可能状态数大约在 250&#94;150 国际象棋 35&#94;80 策略 减少搜索树的深度， 即先行评估位置， 避免深度递归 减少搜索树的广度， 即减少可能动作分支 方法 步骤 用棋谱数据 训练 监督学习策略(走棋)网络, 同时训练快速走子策略 对策略网络进行强化学习， 程序自我对弈 训练一个价值网络， 预测胜率 用MCTS 结合策略网络和价值网络 组成 链接：http://zhuanlan.zhihu.com/yuandong/20607684 来源：知乎 走棋网络（Policy Network），给定当前局面，预测/采样下一步的走棋。 快速走子（Fast rollout），目标和1一样，但在适当牺牲走棋质量的条件下，速度要比1快1000倍。 估值网络（Value Network），给定当前局面，估计是白胜还是黑胜。 蒙特卡罗树搜索（Monte Carlo Tree Search，MCTS)，把以上这三个部分连起来，形成一个完整的系统。 一. 策略网络学习 监督学习 训练集数据： KGS 专业棋手(5-9段)的棋谱， 大概16万局棋， 3千万种棋盘状态 学习到一个预测模型 g 状态S 预测模型 g:S->p(a|S) 概率 p(a|S) 概率最大的动作 a 模型学习算法 深度学习： Convolutional Neural Network (CNN)， 卷积神经网络 围棋对局势的评估很难建模， 抽象 CNN正好擅长抽象 另用线性模型训练快速策略 随机梯度下降 预测 输入： 棋盘状态S 输出： 所有合法动作a 的概率分布 二. 策略网络强化 对弈激励 当前版本的策略网络 与 随机的一个版本 胜 z_t = +1, 负= -1, 未结束=0 瓶颈 强化学习 存在理论瓶颈， 而且应该是被证明了， 没记。 三. 价值网络强化 步骤 输入状态S, 经过 普通策略网络 生成前 U-1步 随机采样 决定 第U步 增强策略网络 完成剩下博弈 胜负作为输出 得到价值网络， 判断该盘面的输赢概率 reduce 搜索空间的方案 减少搜索树的深度， 价值网络 value network 减少搜索树的广度， 策略网络 policy network 四. monte-carlo 树搜索 步骤 选择， 用策略网络剪枝 扩展 评估， 使用价值网络 回溯 树的组成 对 每条边(状态s, 动作a) 动作值 action value Q(s, a) 访问次数 visit count N(s, a) 先验概率 prior probability P(s, a), 初始化为 策略网络值 p(a|s) 图示： 描述 根据 ， 找到叶子结点 用策略网络计算所有可能下一步的概率， 逐个进行3 用价值网络和快速走子策略评估","tags":"研究","title":"alphago-总结"},{"url":"http://sndnyang.github.io/classical-music-week1.html","text":"4. 音乐传播过程-声波和耳朵 长波低音高， 短波高音高 空气（外耳） - 内耳 - 电化学信号 - basilar membrane（耳蜗基底膜） - 纤毛感应特定声波 - 大脑 - primary auditory nerve - auditory cortex(大脑皮层) in temporal lobe 其他相关： 1. prefrontal cortex(lobe) -- where am i in this piece 2. motor cortex parietal lobe -- movement 3. hippocampus - memory 4. limbic system, amygdala -- emotion 5. ... 6. why we like what we like. 对音乐模板、模式(template)的预测方式 nurture -- 文化培养 nature 7. 西方音乐的语法syntax leading tone -- pull toward the home pitch or tonic large leap -- reversed by following pitch harmony -- must fit with the melody chord progression -- harmonies usually return home to tonic 8. 音乐的本质 nature overtone series 乐器震动出声时， 其实不只一个音， 只是听觉系统让我们只听到最低音， 因为它声音最大 这个最低音振幅最大， 定为 fundamental tone , 其他音 overtones 后面几个音和前面做和音","tags":"音乐","title":"古典音乐导论-第一周"},{"url":"http://sndnyang.github.io/multi_layer_cd_suver.html","text":"摘要 Community Detection in Multi-Layer Graphs: A Survey 本文介绍多层网络下聚落检测问题， 并对相应算法做综述 导论 - 过 背景知识 群落/社区 community densely-connected components/subgraph relative/similar 比如： 同校、 同系、 同班是一个 community 同一个社团、 公司 同一领域的研究 多层网络模型 关系的不同方面就可以表达成多个独立图组成的多层图， 里面的每个、每层图就代表了一个方面 比如： 同学关系 微信好友 微博好友 单层图定义 a weighted graph (V,w) V is a set of vertices w is a set of edge weights: (V × V ) → [0,1]. 点映射(图层之间) node mapping is a function from a graph layer L1 = (V1 ,w1 ) to another graph layer L2 = (V2 ,w2 ) V1 × V2 → [0,1]. For each u ∈ V 1 , the set C(u) = {v ∈ V2 |f(u,v) > 0} is the set of V2 vertices corresponding to u. 对于一个facebook上的账号（个人）： twitter 上没有账号 twitter 上只有一个号--pillar(柱型) multi-layer graph twitter 上多个号 多层图定义 a tuple MLN = (L1 ,...,Ll ,IM) where Li = (Vi ,wi ),i ∈ 1,...,l are graph layers IM (Identity Mapping) is an l ×l matrix of node mappings, with $ IM_{i,j} : V_i ×V_j → [0,1] $ 例： 症状 疾病名 细菌、病毒或基因 信息网络定义： 是个有向图， 存在\"点\"到\"点类别\" 的函数映射 及 \"边\"到\"关系(边)类别\"的函数映射 异构信息网络 Heterogeneous Information Networks 点的类别 或 关系的类别个数大于1的信息网络。 异构信息网络与多层网络模型 等价 但强调不太相同 heterogeneous information networks emphasize heterogeneous types of entities connected by different relationships 主要方法 分类 聚类扩展 cluster expansion 矩阵分解 matrix factorization 统一距离？ unified distance 基于概率模型 model based 模式挖掘 pattern mining 图合并 graph merging 特点 多数只支持两层图 一层是图的原始拓扑结构信息 其他层一般是利用点的属性信息来计算相似度 聚类扩展 Cluster Expansion 论文： Scalable community discovery on textual data with relations 基于关系（文章引用）与文本属性 针对的问题 大型文档语料 -- large cocument corpus 没有同时考虑 textual attribute 和 relations(文献里的引用？) 大数据集的可扩展性scalability 多数算法基于一堆要（人工）设定的参数 思路 非监督方法 快速地找到初始的核， 作为群落的种子 核进行扩展（或合并merge)， 扩展成群落，（提高scalability) cores dictate the formation and topics of communities 核 用来表示 社区的构造和主题 步骤 4 steps: core probing core merging, 根据主题相似度进行合并 affiliation, 利用关系信息，将core扩展成初始社区 classification， 主题不相关的成员从社区中移除 第一步 Core Probing 基本思想 co-occurrence analysis: multiple objects are linked simultaneously by others, they are more likely to be able to define a coherent topic scope prob 步骤 生成每个点的outgoing relations 用关联规则来计算频繁项集(Apriori) 与 Apriori 的不同点 不使用固定的过滤阈值， 根据项集的长度决定阈值 项集存在包含关系，如果项集 S1 和 S2存在 $ S1 \\in S2 $, 不保留S1 core merging 保证了合并后核的高度一致性， 不受过滤阈值的影响 证明过程略 步骤 输入参数: core probing 返回的核 迭代： 对S中任意一对核Ki, Kj， 如果重叠， 转2 计算p-min, p-max, p- 如果 Ki, Kj的交集不为空，且 pi- 或 pj- 属于 该交集， 转4 从S 中移除Ki, 和 Kj, 加入Ki,Kj的并集。 如果遍历完， S 没有变化， 则退出 计算 p-min, p-max, p- p-min, pmax: 在特征空间内， 为 core生成了边界框 p- : 中心 图示： Affiliation Propagation 完成cores probe后，剩余的点作为 affiliated members 初始化社区C = 找到的核K ， 迭代处理： 对K中的每个点d，把所有的、其他的、能连到d的点u 加到C中 设定迭代次数， 避免关系环 或迭代中 没有新的点加入 相关概念 好像没什么用 两个社区的公共成员则为 interdisciplinary member 点和社区间的相近度(closeness)用迭代时的次数代表 Intra-Community Classification 只根据relation找到的社区 很可能误判(false hits) 要根据属性分析， 将当前的C 划分成两个集合， C' 和 C- 步骤 核K 视作是 positive example正例， 即肯定属于这个社区 选择社区C的核K（正例） 和 其他社区的核（negative example) 将所有点转换成 特征向量（feature vector）来代表它们的topical position 使用 LDA（Latent Dirichlet Allocation）来降维 使用某种分类器（SVM），将负标签的点都移除 图示 主要贡献 用关联规则、频繁项集来初始化 统一距离 Unified Distance structural and attribute similarities using a unified distance measure SA-Cluster 步骤 建立统一距离度量， 新的图 用新的图 进行聚类， 类k-means unified distance measure 基于属性增广图(attribute-argmented graph), 使用Random Walk with Restart (RWR) 邻点随机游走距离 Neighborhood Random Walk Distance l as the length that a random walk can go c ∈ (0, 1) as the restart probability attribute-argmented graph 添加属性点（attribute vertices），代表属性的值。 原始的点连接到对应的属性点上 两点上共同的属性点越多， 两点相似度直觉上就越高。 聚类算法 利用unified distance measure， 进行 k-medoids clustering（类似 k-means） 选择每个聚簇(cluster)最中心的点 其余点分配给最近的中心点。 迭代， 调整边的权重 聚类中心初始化 思想： 从vi走 l 步能到的点越多， vi越可能是中心 计算点的密度函数： 降序排列， 选择前k点作为聚类中心 聚类过程 分配点到最近的中心，即有最大random walk distance的中心点 对每个cluster ,用随机游走距离 计算\"平均点\" 寻找新的中心点，距\"平均点\"最近 不停迭代， 直到 聚类目标函数 收敛 聚类目标函数 目标是最大化 问题转化 有以上的目标函数后， 可转化成三个子问题 聚类分配 中心更新 权重调整 权重自我调整 在每次迭代时， 进行权重调整 属性 ai 权重在第t+1次迭代的计算公式为： 投票机制 majority voting mechanism counts the number of vertices within clusters that share the same attribute values with the centroids on ai 主要贡献 一个统一的距离评估方式， 将结构和属性相似度结合 带权重的自调整方法， 调节结构属性相似度的重要度 基于模型方法 Model-Based Method model-based community detection approach based on both structural and attribute aspects of a graph 步骤关键点 概率模型的构建， 结合结构和属性信息， 不使用人工定义的距离 变分法(variational approach)解决模型 构建概率模型 聚类属性图定义： X: n x n 的邻接矩阵 Y: n x t 的属性矩阵 Z: n x 1 的聚类向量， 即每个点所属的聚类 目标： 求最优化： 其中 联合概率分布 alpha - 每个聚类的点分布（vertex distribution) theta - 属性分布(attribute distribution) phi - 类间 边出现概率(edge occurrence prob) 两大问题 Z 的N个变量最大化 计算量过大， 全局最优基本不可能 计算Z的后验概率分布时， 不存在 p(Z|X,Y)的closed-form expression 变分法 variational algorithm 使用variational distribution q(α, θ, φ, Z) 来逼近原分布 并且对 variational distribution 作限制 全局最优就转成求局部最优 两个新问题 如何定义the family of variational distributions 如何从中找出最优分布， 最接近p(α, θ, φ, Z|X, Y) Parametric Family Optimizing Variational Parameters measure the distance between a variational distribution q(α, θ, φ, Z) and the true posterior p(α, θ, φ, Z|X, Y) 等价于 最大化 关系式： 图合并 Graph Merging combine structural and attribute information using the graph merging process CODICIL 步骤 创建内容边 create content edges 边组合 combining edges 边采样 sampling edges with bias 聚类 clustering creating content edges 对每个点vi, 用cosine相似度， 计算k 内容最近邻 在vi 和 k近邻间 建立content edges combining edges 将新创建的content edges 和 初始的拓扑边集进行简单的联合(unified) sampling edges with bias 对每个点 vi， 从邻点选择要保留的边， 通过 cosine 相似度或Jaccard 相似度 clustering 因为图合并部分独立于community detection， 所以任意 community detection 都可以， 这块不是本文的重点 主要贡献 通过 用内容信息消除连接结构里的噪音， 来强化社区信号 矩阵分解 Matrix Factorization 论文： Community Detection with Edge Content in Social Media Networks Edge-Induced Matrix Factorization 主要idea 通过从多层图中抽取相同因子(common factors) 把不同信息进行结合 使用通用的聚类方法处理 方法 使用 低秩矩阵因子分解(low-rank matrix factorization) 来逼近目标矩阵O， P: n x n 的特征矩阵 lambda(大写的？): n x n 特征值矩阵 目标 对于多个目标矩阵O&#94;i, i = 1,-,l 要算出一个common factor matrix 求最小化： P: n x n的所有层 公因子矩阵 Λ&#94;i: n x n 矩阵， 第i层的特征 || ·|| is the Frobenius norm α: regularization 参数 全局转局部最优 迭代处理： 固定P , 优化 Λ&#94;i 固定Λ&#94;i , 优化 P 直到 收敛 模式挖掘 Pattern Mining Coherent Closed Quasi-Clique Discovery from Large Dense Graph Databases Cocain 方法 子图挖掘算法， 搜索多层图中频率高于某给定阈值的 quasi-cliques 基础定义 gamma(γ)-Quasi-clique cross-graph quasi-clique: a set of vertices belonging to a quasi-clique appears on all layers must be the maximal set Edge Cut, Edge Connectivity edge cut is a set of edges Ec such that G'=(V ,E-Ec) is disconnected A minimum cut is the smallest set among all edge cuts. The edge connectivity of G, denoted by κ(G), is the size of the minimum cut coherent subgraph: a subgraph that satisfies a minimum cut bound gamma(γ)-Isomorphism 同构 若两个图G1, G2是 gamma同构， 当且仅当： 都是 γgamma-quasi-cliques 点个数相同 存在 biject f:V1->V2, 对V1中的每个点v, 满足F1(v) = F2(f(v)) multiset 点的标签的集合(a bag of vertex labels) 忽略顺序 突出多样性 定义为 M(G)， G的multiset string of a graph Given a k-graph g, any sequence of all elements in M(g) 给定 k-graph g, M(g)的任意一种序列 canonical form of a graph the minimum string among all its strings and denoted by CF(G) 图的最小 string, 记作 CF(G) 有引理： 两个γ-quasi-cliques Q1 Q2 是γ同构， 当且仅当 CF(Q1) = CF(Q2) 步骤 将子图转成 canonical forms 枚举γ-quasi-cliques可行解(feasible candidate for γ-quasi-cliques), 用DFS策略进行剪枝 基于 闭包检查规划(closure-checking scheme)， 选择出闭包的 γ-quasi-cliques 枚举策略 枚举树 满足： 子代必须能归入祖先 关键： 对每个 quasi-clique Q, 处理完它的子代后， 进行闭包检查 主要贡献 find cross-graph quasi-cliques in a multi-layer graph that are frequent, coherent, and closed 另一篇模式挖掘 论文 论文： Mining Coherent Subgraphs in Multi-Layer Graphs with Edge Labels 本文贡献 提出了带边标签的多层图聚类的新范式 提出了MLCS, 避免了结果集的冗余 提出了最好优先搜索算法MiMAG 来求MLCS聚类的近似解 multi-layer coherent subgraph (MLCS) model clusters of vertices that are densely connected by edges with similar labels in a subset of the graph layers 找聚类， 满足条件：在某层的图中，不仅边的密度高，且具有相似的标签。 the edge labels represent characteristics of the relations quasi-clique One-dimensional MLCS cluster One-dimensional MLCS cluster 某一图（层）的点集满足以下条件： 形成一个 0.5-quasi-clique 点集的每条边的两个顶点： dist(l_i(x), l_i(y)) <= w, edge label 为连续值时需要w, 不然置为0. 多层 MLCS cluster 冗余关系 redundancey relation MiMAG 算法 Mining Multi-layered, Attributed Graphs 计算出最大化、 无冗余、 高质量（不是最优质量）的聚类 基于寻找quasi-cliques的快速算法。 总结 Apriori 频繁项集的， 特例或通用 随机游走 概率模型 矩阵分解 clique community 没有严格定义 未来研究方向 通用多层图的适应性 General multi-layer graph applicability 当前算法一般仅研究了 pillar(柱形)多层图 现实世界不保证不同层之间正好一一对应 所以现有算法的泛化， 对通用多层图的适应性非常有意义 多层图的不确定性 Uncertainty in multi-layer graphs 现有的研究都假定 图数据已经清理完毕， 缺少噪音、歧义的研究 constructing multi-layer graphs with entity resolution and/or trustworthy analysis certainly enhances the quality of the community detection process 可扩展性问题 所以可能要考虑并行及分布式之类的方法 或是对多层图的特征向量矩阵（feature-vector matrices）进行采样 Temporal analysis 图是随时间变化的。 目前存在一些对单层图的时间变化的研究， 但基本不可用于多层图 谢谢！","tags":"研究","title":"多层网络聚落检测综述"},{"url":"http://sndnyang.github.io/GaoCD.html","text":"导论 算法思路 通过 划分密度(partition density) 这个目标函数的最优化来寻找 连接群落link communities 通过 novel genotype representation method, 将 连接群落映射回 点群落。 群落数自动发现 算法描述 框架 目标函数 partition density D only considers the link density within the community, different from the common community definition that a community should be densely intra-connected and sparsely connected with the rest communities.# 划分密度D $$ D(c) = \\frac{m_c - (n_c - 1)}{\\frac{n_c(n_c-1)}{2} - (n_c-1)} $$ partition density D is the average of Dc over all communities $$ D = \\frac{2}{M}\\sum_c m_c\\frac{m_c - (n_c - 1)}{(n_c-2)(n_c-1)} $$ 3.3 基因表达 编码 基于连接的表示方法， 群体中的个体g 有 m 个基因， 下标i 代表边的序号 m 是边数——吓死人了。 gj 从连接的点中选一个。 当无向图中两边共点时， 两边相连 解码 把基因型转化成 分割（由连接群落组成）， gi 作为基因型， 值 j 可看作是边 i 和 边 j 有一个共同点， 并应该归入同一群落中。 桥接边： 连接两个 聚落的边。 Fine tuning: 调整 单一映射方法得到的点群落 的点附属关系 寻找有多附属关系的点 membership 计算这种点 对各群落是否有贡献——如果加入的话。 贡献计算方法： $$ AD(c) = 2 * \\frac{|E(c)|}/{|c|} $$ 添加点后， EC 上升， 则OK","tags":"数据挖掘","title":"GaoCD-总结"},{"url":"http://sndnyang.github.io/ema_comminities_dynamic_networks.html","text":"摘要： 目标： 最大化当前数据的聚类准确性 最小化阶段过渡时的聚类漂移 clustering drift 新概念： temporal smoothness 短时平滑性 snapshot quality , temporal quality 快照质量和短时质量 优点： provides a solution representing the best trade-off between the accuracy of the clustering obtained, and the deviation from one time step to the successive. 为聚类的准确性及阶段过渡时的变动提出了一个最优折衷的方案 ** 问题是这几个东西都不是这篇文章提的概念， 只是函数可能有变化 导言 进化聚类方法(evolutionary clustering) 利用 temporal smoothness 框架。 核心假设： abrupt changes of clustering in a short time period are not desirable （译： 短时间内聚类突变是不值得要的？不合适的？） it smooths each community over time 平滑性的实现 折衷： snapshot quality: 在当前阶段所拥有数据下， 聚类要尽可能精确。 temporal cost: 每个聚类在阶段过渡时， 不能发生剧烈变化。 本文方法 名字： DYNMOGA (DYNamic MultiObjective Genetic Algorithms) 目标 最大化 snapshot quality, 表明当前聚类效果（准确性）， 为此调整了 modularity 的概念 最小化 temporal cost， 表明两阶段间聚类差别， 为此去计算 归一化互信息(normalized mutual information) 优势 利用这两个方法的优势 选择性搜索解空间， 不需要提前知道 聚类个数。 本文主要贡献 将动态网络中群落结构的检测问题 建模成 多目标优化问题--以前肯定有人弄过了，也算贡献？ 本方法可以考虑成 通用框架，应用于进化聚类。 仅仅需要修改目标函数，测试不同的质量函数--别人的算法也可以，这篇就是利用别人的框架。 本方法不需要参数， 不需要为快照和短时成本设置权重， 也不用设定聚类个数--不知道他人工作情况。 相关工作 主要工作 Evolutionary Clustering by Chakrabarti et al. in [13] 认为changes of connections in short time periods could be caused by noise. 提出了 temporal smoothness 和 snapshot cost temporal cost 问题是： not allow that the number of communities varies over time FacetNet by Lin et al[5] particle-and-density based clustering method by Kim and Han [3] 这些方法的主要问题 聚类个数 不知道。 相对于要选择 参数 alpha 去应用于 temporal smoothness。 DYNMOGA算法 DYNMOGA has been adapted with a customized population type that suitably represents a partitioning of a network and endowed with two complementary objectives 他们使用了 matlab 实现的 NSGA-II 算法框架, DYNMOGA支持 定制的、可表示网络分割情况的群体类型, 并具有两种互补的目标（然而并没有说是哪两种）。 目标函数 定义 $$$ CR&#94;t = { C&#94;t_1, ... C&#94;t_k } 是图在 t 阶段的聚类结果 一个聚类中有 n_S 个结点 m_S 条边。 m_S(u) = {v | v \\in C_t } 是结点u 在聚类C&#94;t 的邻点个数 c_S = { (u, v) | u \\in C&#94;t, v \\notin C&#94;t} 是聚类C&#94;t边界的边数。 l_S 是 只连接 模块 C&#94;t_S 内部结点 的边总数。 d_S 是 C&#94;t_S 中点的度数之和 $$$ 多种分值定义 Q: the first term of each summand is the fraction of edges inside a community, while the second one is the expected value of the fraction of edges that would be in the network if edges fall at random without regard to the community structure. Values approaching 1 indicate strong community structure modularity 颗粒度 ： $$ Q = \\sum&#94;k_{s=1}[\\frac{l_s}{m} - (\\frac{d_s}{2m})&#94;2] $$ conductance 导率, the fraction of edges pointing outside the clustering： $$ CO = \\sum&#94;k_{S=1}\\frac{c_S}{2m_S+c_S} $$ Normalized Cut 归一化分割 the fraction of total edge connections to all the nodes in the graph: $$ NC = \\sum&#94;k_{S=1}\\frac{c_S}{2m_S+c_S} + \\frac{c_S}{2(m-m_S)+c_S} $$ Community Score 群落分值, measure the fraction of internal edges of each cluster per nodes： $$ CS = \\sum&#94;k_{s=1}(\\sum_{u \\in C&#94;t}(\\frac{m_S(v)}{n_S})&#94;2) * \\frac{2m_S}{n_S} $$ 基因表达 locus-based adjacency representation [34] 每个个体包含 n 个基因， n 指代 结点的个数 每个基因 取值范围 1-n， 即第i个基因与第j个基因之间有连接，该划分到同一群落 ** 注： 这种表达肯定不能用于 群落重叠问题——然而 现实是， 主流用法 就是这样，大同小异 好处： 由个体组成部分的个数，在解码步骤中自动得到 decoding step 解码 使用并查集 建立并查集 makeset 对每条边去查找, findset 查到后的进行合并 初始化 一个有若干个体的群体， 对每个点i, 在邻接点中随机选择一个作为值， 表示 存在边 (i,j) uniform crossover 均匀交叉 给定两个父辈个体， 创建一个随机二元mask, 进行选择， 当 mask 为0时， 取第一个父辈个体的基因（值）， 为1时， 取第二个父辈个体。 如此组成子代的基因 突变 与初始化类似， 对结点i 随机变更值成其他邻点。","tags":"数据挖掘","title":"进化多目标方法在动态网络聚落检测中的应用-总结"},{"url":"http://sndnyang.github.io/ea_based_communities_survey.html","text":"网络相关背景知识 定义 群落检测没有标准定义， 目前一般视为： 一组顶点，之间可能有着共同的属性或在图中起到相似的作用 分类 common model 共同？普通？模型 directed model 有向图模型 signed model 正负向模型 overlapping model 重叠模型 dynamic model 动态模型 进化算法与多目标优化 进化算法 相同性质： 省 框架： Algorithm 1 General framework of EAs 输入： 参数与问题实例 输出： 最优方案 Begin: population initialisation store optimal solutions for i = 1 to max_iteration do for each individual in the population do generate a new individual through stochastic components evaluate the fitness of the new individual endfor update optimal solutions endfor End 多目标优化 定义： 略 Pareto optimal solution: 待看书， 此处略 适应函数 单目标优化 4.1.1 基于颗粒度的模型 因为目标是多目标优化，先略， 如果有用再回来看 4.1.2 multi-resolution model 多分辨率？模型 同上 4.2 多目标优化 其他model 略 重叠模型 操作子 设计 个体表示 个体重现 个体局部搜索 结论 主要思想： 将群落检测建模成 单目标或多目标优化问题 设计元启发方法来解决 问题一： 根据 no free lunch 理论， 没有通用方法能解决全部类型的网络 不同网络的时-空特性不同 问题二： 由于数据集过大， 基于元启发方法的群落检测是LSGO问题（大规模全局优化）， 包含大量的决策变量，对现存优化技术是个挑战。 如何又快又好就值得思考 网络群落问题将超越纯粹结构分析，变成强调网络智能。","tags":"研究","title":"基于进化算法的群落检测问题综述总结"},{"url":"http://sndnyang.github.io/overlap_communities_survey.html","text":"算法 1. Clique Percolation 中文名？派系过滤 假设 群落由 完全连通子图的重叠集合组成——a community consists of overlapping sets of fully connected subgraphs 思路 detects communities by searching for adjacent cliques 扩展内容 派系(Cliques)。在一个无向网络图中，\"派系\"指的是至少包含3个点的最大完备子图。这个概念包含3层含义：①一个派系至少包含三个点。②派系是完备的，根据完备图的定义，派系中任何两点之间都存在直接联系。③派系是\"最大\"的，即向这个子图中增加任何一点，将改变其\"完备\"的性质。 n-派系(n-Cliques)。对于一个总图来说，如果其中的一个子图满足如下条件，就称之为n-派系：在该子图中，任何两点之间在总图中的距离(即捷径的长度)最大不超过n。从形式化角度说，令d(i,j)代表两点和n在总图中的距离，那么一个n-派系的形式化定义就是一个满足如下条件的拥有点集的子图，即：d(i,J)\\le n，对于所有的，n_i,n_j\\in N,来说，在总图中不存在与子图中的任何点的距离不超过n的点。 n-宗派(n—Clan)。所谓n-宗派(n—Clan)是指满足以下条件的n-派系，即其中任何两点之间的捷径的距离都不超过n。可见，所有的n-宗派都是n-派系。 k-丛(k-Plex)。一个k-丛就是满足下列条件的一个凝聚子群，即在这样一个子群中，每个点都至少与除了k个点之外的其他点直接相连。也就是说，当这个凝聚子群的规模为n时，其中每个点至少都与该凝聚子群中n-k个点有直接联系，即每个点的度数都至少为n—k。 某个的步骤之一 begins by identifying all cliques of size k in a network, a new graph is constructed such that each vertex represents one of these k-cliques 结论 more like pattern matching rather than finding communities since they aim to find specific, localized structure in a network. 2. Line Graph and Link Partitioning 中文名？连接划分 思路 partitioning links A node in the original graph is called overlapping if links connected to it are put in more than one cluster. 3. Local Expansion and Optimization 中文名？局部增广和优化 思路 based on growing a natural community or a partial community rely on a local benefit function that characterizes the quality of a densely connected group of nodes 4. Fuzzy Detection 中文名？模糊检测 思路 quantify the strength of association between all pairs of nodes and communities 例子 Non-negative Matrix Factorization 5. Agent-Based and Dynamical Algorithms 中文名？基于啥的动态算法 思路 label propagation algorithm by allowing a node to have multiple labels 6. others 中文名——无法分类 :) 评估方法 1. Normalized Mutual Information 中文名？标准互信息 2. Omega Index 结论","tags":"数据挖掘","title":"社区覆盖问题总结"},{"url":"http://sndnyang.github.io/findbugs_summary.html","text":"原文: Finding Bugs is Easy by David Hovemeyer, William Pugh 摘要 旧方法基于 formal methods 和 复杂程序分析， 难用， 无作用 bug patterns - detectors 简单的自动技术在遇到常规错误和难解特性时都有用 导论 conclusion 不存在这种bug, 过于明显，以至于在实际代码中没有找到例子——发现的bug中，有些十分明显， 让我们吃惊， 即使是在生产应用和库里 现代OO语言的高度复杂性， 对语言特性及API的滥用是屡见不鲜的。 自动bug检测可以在 程序正确性 上起到巨大作用","tags":"软件工程","title":"FindBugs总结"},{"url":"http://sndnyang.github.io/human_kernel_summary.html","text":"bayesian nonparametric models such as Gaussian processes function extrapolation problems kernel learning framework ** reverse the human-like and inductive biases of human across a set of behavioral experiments to gain psychological insights and to extrapolate in human model ability determined by its support(which solutions are a priori possible ) inductive biases (which solutions are a priori likely) controlled by a covariance kernel","tags":"研究","title":"human kernel 总结"},{"url":"http://sndnyang.github.io/FOCS_summary.html","text":"基本定义 图： G(V, E) 目标—— 找到一簇子图(全部并正确)， 每个子图都是一个团（community) 即 $ S = {S_i | S_i \\subset V } $ 团（community)： 子图中任意点在该子图中的连通性 高于 非团的子图 定义 包含点 $ v_j $ 的团的集合为： $$ S(v_j) = {S_i | v_j \\in S_i \\land S_i \\in S } $$ disjoint cluster: $$ |S(v_j)| \\le 1 $$ overlapped cluster: > 1 定义 $ N(v_j) $ 为 $v_j$ 的邻接点 定义 $ N_i(v_j) $ 为 $v_j$ 在团 $S_i$ 的邻接点， 即 $$ N_i(v_j) ={v_k | (v_j, v_k) \\in E \\land v_k \\in S_i } $$ 新概念定义 团连通性 community connectedness, 即点 $v_j$在团$S_i$邻点超阈值个数 除以 该团点数, 代表 点对应团的归属性。 $$ \\zeta&#94;i_j = \\frac{|N_i(v_j)|-K+1}{|S_i| - K} if |N_i(v_j)| > K, else, 0 $$ 邻接连通性 neighborhood connectedness， 即 点 $v_j$在团$S_i$邻点数 除以 $v_j$的总邻点数， 代表 点加入新团的可能性 $$ \\xi&#94;i_j = \\frac{|N_i(v_j)|}{|N(v_j)|} $$ 外围结点 peripheral node: $v_i$的团邻接点 $$ Added_i = {v_k|v_k \\in N(v_i) \\land v_k \\in S_i }, \\forall S_i \\in S&#94;l $$ 步骤 初始化 初始化全部有K个以上邻接点的点， 由它及其邻接点组成团 $S_i$ 定义该阶段的外围结点 脱离阶段 对前面所有团里的点 $ V_j$, 计算相应的 团连通性 $ \\zeta&#94;i_j $ 邻接连通性 $ \\xi&#94;i_j $ 将[0, 1] 区间划分为 $max(20, N(v_j))$ 块， 每块初始化为0. 根据团连通性分数， 统计各区间 点的个数。 标记 最右的非0元， 并开始向左遍历， 直到： 遍历完毕 或 遇到某区间，<=标记值(最右非零元)， 且<=左边区间值， 这个值选为 留存阈值 stay cut-of of \\zeta 外围结点 $v_k \\in Added_i $ 排除出 $S_i$ , 当团连通性分数 $ \\zeta&#94;i_k $ 比留存阈值低。 Removal of only peripheral nodes ensures that nodes that form the core of a community are never eliminated 扩充阶段 对上一阶段处理后的团中所有点$v_j$， 当以下条件满足： 1. 该点未入 $ S_i$团 2. 该点选入 $S_i$团的可能性高（即 邻接连通性 $\\xi&#94;i_j$ 高于选中阈值 join cut-off ） join cut-off 选中阈值的计算方式与 stay 相同 去重阶段 代码描述 参考链接 focs-code submit","tags":"数据挖掘","title":"FOCS总结"},{"url":"http://sndnyang.github.io/d3js_learn_1.html","text":"流程 一、添加 svg 画布， 得到一个 选择器selection var svg = d3.select(\"body\") //选择文档中的某一元素，根据CSS规范，如 \"body\" .append(\"svg\") //添加一个svg元素, 符合html操作 .attr(\"width\", width) //设定宽度 .attr(\"height\", height); //设定高度 二、布局（转换数据） 2.1 种类 布局种类 2.2 定义布局 var mylayout = d3.layout.tree() // 例： Tree布局 2.3 设置布局的基本属性（接上一项） .size ([ width , height ]) // tree 只想到这个属性 2.4 用布局转换数据 var nodes = mylayout.nodes(data)[.reverse()] // reverse 逆序， data格式有讲究， json且属性名字限定 var links = mylayout.links(nodes) // 树图要生成点和边 或者 var piedata = pie(dataset); // 饼状图就不需要边了 三、数据绑定到元素（默认新建元素） 3.1 从画布选择器中，再选择后代元素 svg.select(name) // 选择第一个匹配的元素 或者 svg.selectAll(name) // 选择全部匹配的元素 3.2 加载数据（接上一项） .data ( dataset ) // 元素和数据集一对一加载 , 此时得到的叫 update 部分 或者 .datum ( oneData ) // 一个数据绑定给全部元素 3.3 对应数据来创建元素（接上一项），如果元素是现成的不需要建 .enter () // 只能接上一项 数据加载 , 此时得到的叫 enter 部分 .append ( type ) // 创建需要类型的元素 3.4 数据属性设置（例） 根据上一项的 type 和 数据， 设置需要的属性 .attr ( \"x\" , 20 ) //屏幕上的起始横坐标 .attr ( \"y\" , function ( d , i ) { //屏幕上的起始纵坐标 return i * rectHeight ; }) .attr ( \"width\" , function ( d ) { // 元素宽度（有这个属性的话） return d ; }) .attr ( \"height\" , rectHeight-2 ) // 元素高度（有这个属性的话） .attr ( \"fill\" , \"steelblue\" ) ; // 颜色填充 3.5 事件设置（也是属性） .on ( \"mouseover\" , function ( d , i ) {} ) .on ( \"mouseout\" , function ( d , i ) {} ) .on ( \"click\" , function ( d , i ) {} ) 在该元素下继续添加其他子元素 前面3.1-3.5步骤结果保存在某变量中， 即可使用该变量继续append,设置属性和事件 四、动画效果 4.1 方法 transition() // 启动过渡效果, 其前后是图形变化前后的状态（形状、位置、颜色等等） duration() // 指定过渡的持续时间，单位为毫秒 ease() 指定过渡的方式，常用的有： linear：普通的线性变化 circle：慢慢地到达变换的最终状态 elastic：带有弹跳的到达最终状态 bounce：在最终状态处弹跳几次 delay() 指定延迟的时间，可用匿名函数function(d,i) 指定各个的延迟 4.2 创建动态效果 var transition = selection.transition() .duration(time) .attr()... // 定义变量是可能用于4.3 4.3 对子元素进行处理 如果元素中有子元素，需要一并处理。 transition.select(name).attr()","tags":"工具","title":"D3.js 学习心得一"},{"url":"http://sndnyang.github.io/memory-movies-week1.html","text":"记忆概念 分类 working memory episodic memory semantic memory procedural memory working memory workbench(工作台) -- 维持、操作思维和意识 特点：短暂","tags":"心理学","title":"记忆与电影-第一周"},{"url":"http://sndnyang.github.io/automata-hw1.html","text":"编程容易，笔算不易，且写且珍惜 开篇语 第二周的编程练习非常简单——前两轮开课时不知道是不是会难很多， 总之这次把代码框架都搭好了， 顺利的话， 10分钟就OK了。 不顺利的地方在哪儿——我使用的是python版本， 里面有个小bug，我看不懂输出。 在源文件里找了半天代码结构没明白， 最后打算从main开始吧， 一看， 代码里面是这样写的 def main(filepath): return Start('testRE.in') if __name__ == '__main__': main(sys.argv[1]) 好吧，难怪我的文件名参数没用——剩下的在看懂他输出是什么内容后很快就解决了。 无耻的分界线 你以为我就想说这个？ NO！ 最起码笔算不易还没说呢！！！ 本周第一个视频讲的是正则表达式基础，从定义到正则表达式转换成NFA CUT！！！ 概念没兴趣， RE转NFA 学过的内容，看到那张图就OK了， 主要是确实so easy. CONTINUE！！！ 然后就到DFA转RE了， 说实话吧， 过程不是很复杂，对于我这种人来说，就一个公式（虽然后面不停地修正自己对过程理解上的缺陷）， 套公式，谁不会呢？！ 转折 我还真不会了， 小测第一题就是DFA转RE, 4个状态之间的转换， 套用公式简单 $$ R&#94;k_{ij} =R&#94;{k-1}{ij} + R&#94;{k-1}(R&#94;{k-1}{kk})* R&#94;{k-1} $$ 问题有几个： 1. 状态的序号， 给状态按什么顺序分配序号最好呢，在这里可能没影响，不过我换了好几次。 2. 手算真是很绕， 整个下午，多半时间都花在推导上了，连午觉都想想后起来先推导了几次，实在绕晕了才午休。 在纸上推导被绕的过程中， 顺手写代码，模拟一下这个公式， 就是个递归的公式，用nnn 的数组来记录状态嘛。 初始版的数组下标就不对，不经过额外状态的路径（直接连结或无连结）被我忽略了， 另外也没有组织好正则表达式的表达形式，括号没用好。 输出的结果自然是无效的。 最后第二周的第一个小测是我经过7次连蒙带猜后，总算拿到5分。 第一题纸算太难，第二题花费很长时间来理解题意，后3题时不时拖后腿，几次前两题答对（后几次已经不是猜了），后三题不小心出错。 万万没想到（其实不至于） 到晚上， 做其他事耐心缺失之下， 又打开了代码， 决定调整好数组下标， 再组织好括号的输出。很快修改完毕， 随意运行之后，输出结果居然就这么达到预期 因 honor code， 只摘取与题目要求无关的输出部分 k i j regular expression 4 1 1 1(11+0(01+10))*(1+00) 4 1 2 1(11+0(01+10))*01 4 1 3 1(11+0(01+10))*0 4 1 4 1+1(11+0(01+10))*(11+0(01+10)) 4 2 1 0(11+0(01+10))*(1+00) 4 2 2 0(11+0(01+10))*01 4 2 3 0(11+0(01+10))*0 4 2 4 0+0(11+0(01+10))*(11+0(01+10)) 4 3 1 0+(01+10)(11+0(01+10))*(1+00) 4 3 2 1+(01+10)(11+0(01+10))*01 4 3 3 (01+10)(11+0(01+10))*0 虽然没有经过完全化简——也不确认全部正确，但形式上是没问题了。 测验第一题对应的结果也是正确的。 根据输出的结果， 回顾题目， 我去， 选项怎么这么明显啊， 我之前的推导都在为了什么？ 结论 个人经验， 手工推导DFA到RE 非常容易错乱，而老师题目中的几个选项稍认真分析下，可以直接判断正误， 我之前是有多不认真呢。","tags":"CS","title":"自动机作业1-正则表达式"},{"url":"http://sndnyang.github.io/algo_connectivity.html","text":"问题定义： 给定 N个物体（点） 存在两种操作： 连接： 连接两点 查询连通性： 两点间是否存在路径 问题建模： 对象建模： 简单就是个点， 然后用个数组下标0 —— N-1来表示 连通性建模： '连接到' 等价于以下数学表示： 反射： 自身是连通的。 对称： 如果p连接到q，则q也连接到p 传递： 如果p连接到q，q又连通r， 则p 也连通r. 连通分量(connected component) 相互连通的物体的最大集合 操作的实现: Find查询: 查询两点是否在同一分量 Union连接：","tags":"算法","title":"连通性问题"},{"url":"http://sndnyang.github.io/vi-vim-tips.html","text":"vi 小技巧 部分 vi +xx 文件名，可以直接跳到位置的， 和vim一样。xx代表行数 查找时， \\<xxx> 代表全词匹配——vim也一样，而且更先进的是，当光标在该单词上时，vim快捷键shift+8就是向下查找，shift+3就是向上查找。 vim 技巧 模式 ctrl-v 或 ctrl-q 进入可视块状态， 即 列编辑模式。 页面跳转 gg 文件首行 G 文件末尾 H M L 光标跳转页顶，页中，页底 zt zz zb 光标所在行置顶、置中、置底 排版缩进 按v进入visual状态，选择多行，用>或<缩进或缩出 等效于 x >>/<< x行缩进 通常根据语言特征使用自动缩进排版：在命令状态下对当前行用== （连按=两次）, 或对多行用n==（n是自然数）表示自动缩进从当前行起的下面n行。你可以试试把代码缩进任意打乱再用n==排版，相当于一般IDE里的code format。使用gg=G可对整篇代码进行排版 至于如何针对自定义语言制订缩进规则，还有点问题，成功率不高。 代码跳转 % 跳转到配对的括号去 [[ 跳转到代码块的开头去(但要求代码块中'{'必须单独占一行) gD 跳转到局部变量的定义处 '' 跳转到光标上次停靠的地方, 是两个'单引号, 而不是一个\" 双引号 书签 mx 设置书签,x只能是a-z的26个字母 `x 跳转到书签处(\"\"是1左边的键)","tags":"工具","title":"vi-vim小技巧"}]}