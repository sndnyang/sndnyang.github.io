<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="sndnyang" />
        <meta name="copyright" content="sndnyang" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="神经网络, 机器学习, 深度学习, 机器学习, " />

<meta property="og:title" content="神经网络反向传播 "/>
<meta property="og:url" content="http://sndnyang.github.io/backpropagation-nn.html" />
<meta property="og:description" content="神经网络后向传播算法简介，重在理清发明创造的思维、脉络" />
<meta property="og:site_name" content="懒龙谷" />
<meta property="og:article:author" content="sndnyang" />
<meta property="og:article:published_time" content="2016-11-24T11:38:30+08:00" />
<meta name="twitter:title" content="神经网络反向传播 ">
<meta name="twitter:description" content="神经网络后向传播算法简介，重在理清发明创造的思维、脉络">


        <title>神经网络反向传播  · 懒龙谷
</title>

        <style>
            body { padding-top: 50px; padding-bottom: 10px; }
        </style>

        <link href="//cdn.bootcss.com/bootstrap/2.3.2/css/bootstrap.min.css" rel="stylesheet">
        <link href="//cdn.bootcss.com/bootstrap/2.3.2/css/bootstrap-responsive.min.css" rel="stylesheet">
        <link href="//cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" rel="stylesheet">

        <script src="//cdn.bootcss.com/jquery/1.11.1/jquery.min.js"></script>

        <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" media="screen">

		<link rel="stylesheet" href="//bdimg.share.baidu.com/static/api/css/share_style1_24.css">
        <link rel="stylesheet" type="text/css" href="/theme/css/style.css" media="screen">
        
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-fixed-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="/"><span class=site-name>懒龙谷</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="/">Home</a></li>
                            <li ><a href="http://www.zhimind.com">知维图主页</a></li>
                            <li ><a href="/categories.html">Categories</a></li>
                            <li ><a href="/tags.html">Tags</a></li>
                            <li ><a href="/archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span12">

        <link rel="stylesheet" type="text/css" href="static/css/tutorial.css">
        <script src="/theme/js/index.js" type="text/javascript"></script>
        <script type="text/javascript" src="/theme/js/jQuery.md5.js"> </script>
        <script type="text/javascript">
		$(document).ready(function() {
		    backToTop();
		});
      
    	</script>
        
        <div class="hint">
            <ul class="flashes">
            </ul>
        </div>
        
        <article>
            <header class="page-header span12 text-center">
                <h1>
                    <a href="/backpropagation-nn.html">
                        神经网络反向传播  
                    </a>
                </h1>
            </header>

            <div class="row-fluid">
                <div class="span2 table-of-content">
                    <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix">
                    <h4>Contents</h4>
                        <div class="toc">
<ul>
<li><a href="#_1"></a></li>
<li><a href="#_2">目前遇到的问题</a><ul>
<li><a href="#_3">关键问题</a></li>
<li><a href="#_4">继续分析</a></li>
<li><a href="#_5">最后一项</a></li>
<li><a href="#_6">方法对吗？</a><ul>
<li><a href="#_7">哪里不行了？</a></li>
</ul>
</li>
<li><a href="#_8">详细分析</a></li>
<li><a href="#_9"></a></li>
<li><a href="#_10">那怎么办？</a><ul>
<li><a href="#_11">试试又不会掉块肉</a></li>
<li><a href="#_12">差什么？</a></li>
</ul>
</li>
<li><a href="#_13">否定后要提方案</a><ul>
<li><a href="#_14">对比一下</a></li>
<li><a href="#_15">所以呢</a></li>
<li><a href="#ok">OK？</a></li>
<li><a href="#_16">还要考虑系数</a></li>
<li><a href="#_17">系数有什么问题</a></li>
<li><a href="#_18">以常为镜，可以非常</a></li>
<li><a href="#_19">找呀找呀找系数</a></li>
<li><a href="#_20">我跪了</a></li>
<li><a href="#_21">接下来</a></li>
<li><a href="#_22">推广</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_23">反向传播算法步骤</a></li>
<li><a href="#_24">总结</a></li>
<li><a href="#_25">恭喜</a></li>
</ul>
</div>
                    </nav>
                </div>
                <div class="span10 article-content">

                        
                        
<h1 id="_1"></h1>
<p>本文来自以下内容：</p>
<ol>
<li>Hinton coursera《neural network for machine learning》<a href="https://www.coursera.org/learn/neural-networks">课程主页</a></li>
<li>Andrew Ng Coursera <a href="https://www.coursera.org/learn/machine-learning">机器学习</a></li>
<li>学堂在线 袁博 <a href="http://www.xuetangx.com/courses/course-v1:TsinghuaX+80240372X+2016_T2/about">数据挖掘：理论与算法</a></li>
</ol>
<div class="process"><span><p>原则上我会从什么开始？</p><input class="quiz" name="quiz" type="radio" value="问题">问题</input><br/><input class="quiz" name="quiz" type="radio" value="定义">定义</input><br/><input class="quiz" name="quiz" type="radio" value="故事">故事</input><br/><input class="quiz" name="quiz" type="radio" value="历史">历史</input><br/></span><br/><input class="answers" type="hidden" value="问题"/><input class="comments" type="hidden" value="故事:不是不想从故事和历史开始，孤陋寡闻了#历史:不是不想从故事和历史开始，孤陋寡闻了#定义:直接上定义和以前教科书有什么区别？"/><button onclick="checkQuiz(this, 0)">submit</button><br/></div>
<h1 id="_2">目前遇到的问题</h1>
<p>我们刚刚学习了 <a href="http://www.zhimind.com/tutorial/feed-forward-nn">神经网络前向传播</a>, </p>
<div class="process"><span><p>现在有什么问题？</p><input class="quiz" name="quiz" type="checkbox" value="权重参数向量如何更新">权重参数向量如何更新</input><br/><input class="quiz" name="quiz" type="checkbox" value="输出结果不对怎么办">输出结果不对怎么办</input><br/><input class="quiz" name="quiz" type="checkbox" value="如何初始化权重向量">如何初始化权重向量</input><br/></span><br/><input class="answers" type="hidden" value="权重参数向量如何更新@输出结果不对怎么办@如何初始化权重向量"/><input class="comments" type="hidden" value="不知道说什么，问题编不完"/><button onclick="checkQuiz(this, 1)">submit</button><br/></div>
<h3 id="_3">关键问题</h3>
<p>这三个问题中，你觉得关键问题是啥？ 不知道的话， 逐个分析一下。</p>
<div class="process"><span><p>如何初始化权重矩阵（向量）？<input class="quiz" type="text"/></p></span><br/><input class="answers" type="hidden" value="0|零"/><input class="comments" type="hidden" value=":{%text|举一反三，其他方法怎么初始化的？_@0|零#:{%radio|没看过其他方法包括感知机和逻辑回归的初始化吗？前面文章都没看过？&amp;是&amp;否@否#是:怎么就直接看到这了呢？#否:没编程实践过是吧？我还没制作好编程部分。%}%}"/><button onclick="checkQuiz(this, 2)">submit</button><br/></div>
<h3 id="_4">继续分析</h3>
<p>初始化权重一般是全部置0。</p>
<p>第二项 输出结果不对怎么办？ 这个很明显就是要去更新 权重参数向量！ 所以 这两个选项是同一个问题。</p>
<div class="process"><span><p>难道本质不是同一个问题吗？</p><input class="quiz" name="quiz" type="radio" value="是同一问题">是同一问题</input><br/><input class="quiz" name="quiz" type="radio" value="否">否</input><br/></span><br/><input class="answers" type="hidden" value="是同一问题"/><input class="comments" type="hidden" value="否:请联系我，说下您的看法？"/><button onclick="checkQuiz(this, 3)">submit</button><br/></div>
<h3 id="_5">最后一项</h3>
<div class="process"><span><p>要更新权重参数向量，您现在有想到什么方法，之前学过，也许能用？<input class="quiz" type="text"/></p></span><br/><input class="answers" type="hidden" value="梯度下降|牛顿"/><input class="comments" type="hidden" value="逻辑回归用的方法也许能用？"/><button onclick="checkQuiz(this, 4)">submit</button><br/></div>
<h2 id="_6">方法对吗？</h2>
<p>上面的空不知道读者会填成什么。</p>
<p>以前在<a href="http://www.zhimind.com/tutorial/logistic-regression-foundation">逻辑回归</a>使用的 <a href="http://www.zhimind.com/tutorial/gradient-descent">梯度下降</a> 或 <a href="http://www.zhimind.com/tutorial/newton-methods">牛顿迭代法</a>  是否能直接拿来用呢？</p>
<div class="process"><span><p>你觉得能直接拿来使用吗？</p><input class="quiz" name="quiz" type="radio" value="可以">可以</input><br/><input class="quiz" name="quiz" type="radio" value="不行">不行</input><br/></span><br/><input class="answers" type="hidden" value="不行"/><input class="comments" type="hidden" value="不行:{%text|为什么？_@隐|hidden#发现问题，奖励朵小红花%}#可以:下面就可以看到为什么不可以。其实应该猜一下。"/><button onclick="checkQuiz(this, 5)">submit</button><br/></div>
<h3 id="_7">哪里不行了？</h3>
<p>还记得 <a href="http://www.zhimind.com/tutorial/gradient-descent">梯度下降法</a> 或 <a href="http://www.zhimind.com/tutorial/newton-methods">牛顿迭代法</a> 的过程吗？</p>
<div class="process"><span><p>求什么运算<input class="quiz" type="text"/>进行迭代，迭代到什么时候<input class="quiz" type="text"/>结束。</p></span><br/><input class="answers" type="hidden" value="导|微@0|零|极|最|次"/><input class="comments" type="hidden" value="梯度下降和牛顿迭代都是进行什么运算？应该有印象？#什么时候结束也能答上来吧？#答不上来请找我投诉~~~"/><button onclick="checkQuiz(this, 6)">submit</button><br/></div>
<h2 id="_8">详细分析</h2>
<p>来看看式子， 逻辑回归的迭代式：
$$
w = w + \alpha \nabla_w l(w)
$$
其中：
$$
\nabla l(w) = y^T (y - h(x))\
l(w)= y^T log(h(Xw)) + (1-y^T)log(1-h(Xw))
$$</p>
<p>而牛顿迭代法求解一次导$J'(\theta) = 0$时得到的极值， 
$$
w_{n+1} = w_n - \frac{J'(w_n)}{J''(w_n)}
$$</p>
<p>$J(w) = l(w) 或 =(h(x) - y)^2$</p>
<p>可以看到， 式子里都有 y ， 也就是数据的label， </p>
<div class="process"><span><p>神经网络的哪里可以跟label比较？</p><input class="quiz" name="quiz" type="radio" value="输入层">输入层</input><br/><input class="quiz" name="quiz" type="radio" value="隐藏层">隐藏层</input><br/><input class="quiz" name="quiz" type="radio" value="输出层">输出层</input><br/></span><br/><input class="answers" type="hidden" value="输出层"/><input class="comments" type="hidden" value="很明显了对吧？"/><button onclick="checkQuiz(this, 7)">submit</button><br/></div>
<h2 id="_9"></h2>
<div class="process"><span><p>所以梯度下降和牛顿迭代直接用在神经网络上会有什么问题？<input class="quiz" type="text"/></p></span><br/><input class="answers" type="hidden" value="隐|hidden"/><input class="comments" type="hidden" value="输入层就是输入单元，不需要调整，输出层的可以调整，那么？#是不是有不知道怎么调整的神经元？"/><button onclick="checkQuiz(this, 8)">submit</button><br/></div>
<h2 id="_10">那怎么办？</h2>
<p>总不能只管调整最后输出层的 权重向量吧？梯度下降法或牛顿迭代法应该只能调整传给输出层的权重向量~~~</p>
<p>反正隐藏层没有label标签可以比较，不能直接用梯度下降法。</p>
<div class="process"><span><p>不能直接用，那是否应该借鉴梯度下降牛顿法的思想，调整一下后使用？</p><input class="quiz" name="quiz" type="radio" value="应该试试">应该试试</input><br/><input class="quiz" name="quiz" type="radio" value="直接想新办法">直接想新办法</input><br/></span><br/><input class="answers" type="hidden" value="应该试试"/><input class="comments" type="hidden" value="呃#应该试试:多借鉴、多模仿、多改进、多尝试"/><button onclick="checkQuiz(this, 9)">submit</button><br/></div>
<h3 id="_11">试试又不会掉块肉</h3>
<p>创新不是无根之萍，要善于从已有方法、 相似领域方法中进行微创新，多借鉴，多模仿，多改进，多尝试。即使爱因斯坦提出相对论也是基于前人很多工作，特别是数学上的~~~</p>
<p>我们来借鉴、分析下梯度下降法， 因为神经网络使用的是 sigmoid神经元，我们来看看对应的数学式。</p>
<p>模型：
$$
\mathcal{l}(w) = y^T log(h(w<em>x)) + (1-y^T)log(1-h(w</em>x)) \
$$</p>
<p>梯度下降法：
$$
w = w + \alpha \nabla_w l(w) \
\nabla l(w) = y^T (y - h(w*x))
$$</p>
<div class="process"><span><p>可以总结出几个特点，请选出你认为在神经网络可能有用的特点。</p><input class="quiz" name="quiz" type="checkbox" value="求梯度即求导或偏导">求梯度即求导或偏导</input><br/><input class="quiz" name="quiz" type="checkbox" value="迭代更新">迭代更新</input><br/><input class="quiz" name="quiz" type="checkbox" value="更新系数">更新系数</input><br/><input class="quiz" name="quiz" type="checkbox" value="矩阵相乘">矩阵相乘</input><br/><input class="quiz" name="quiz" type="checkbox" value="y-h(w*x)">y-h(w*x)</input><br/></span><br/><input class="answers" type="hidden" value="求梯度即求导或偏导@迭代更新@更新系数"/><input class="comments" type="hidden" value="矩阵相乘:这个不算特点吧？#y-h(w*x):正是隐藏层没有y"/><button onclick="checkQuiz(this, 10)">submit</button><br/></div>
<h3 id="_12">差什么？</h3>
<p>有梯度（导数）， 有系数，要对权重向量（矩阵）更新，那差什么？</p>
<p>首先，$\nabla l(w) = y^T (y - h(w<em>x))$ 里有y-h(w</em>x)， 所以，我们得找新的梯度、导数。</p>
<div class="process"><span><p>对了，能理解为什么要梯度、导数吧？</p><input class="quiz" name="quiz" type="radio" value="能">能</input><br/><input class="quiz" name="quiz" type="radio" value="否">否</input><br/></span><br/><input class="answers" type="hidden" value="能"/><input class="comments" type="hidden" value="否:请看[梯度下降法基础](http://www.zhimind.com/tutorial/gradient-descent)#能:{%text|为什么呢？_@#逗你玩%}"/><button onclick="checkQuiz(this, 11)">submit</button><br/></div>
<h2 id="_13">否定后要提方案</h2>
<p>OK， 首先肯定不能在隐藏层用损失函数了，那需要y标签数据，而隐藏层应该输出什么，没人知道。</p>
<p>那我们要先在已使用的函数里找一找有没有可用的函数、方案。 毕竟没有线索去创造一个新函数。</p>
<p><strong>注：</strong> 很明显我是根据BP反向传播算法的结论来猜测、反推这些理由，希望和真正创造这方法时的思维比较相近。 但实际上是否一定要梯度、 真的没有线索来发现新方法函数吗？这倒不好说，读者如果发现了新的想法那绝对是意外之喜。</p>
<p>又一个显然，除了输出层的损失函数， 还有神经元的模型、函数， 本节使用的是sigmoid神经元，即sigmoid函数。</p>
<div class="process"><span><p>sigmoid函数能影响到权重矩阵（向量）的所有值吗？</p><input class="quiz" name="quiz" type="radio" value="能">能</input><br/><input class="quiz" name="quiz" type="radio" value="否">否</input><br/></span><br/><input class="answers" type="hidden" value="能"/><input class="comments" type="hidden" value="能:{%radio|如果我不这么问，读者你能想到并自己根据sigmoid函数的这个特性推导出反向传播算法吗？%}#否:为什么？"/><button onclick="checkQuiz(this, 12)">submit</button><br/></div>
<h3 id="_14">对比一下</h3>
<p>假设第k层(k&gt;=1)有m个神经元，下一层k+1层有n个神经元</p>
<div class="process"><span><p>所以从k到k+1层的权重矩阵是<input class="quiz formula" onchange="Preview.Update(this)" type="text"/>
维。</p><p>预览:</p><div class="MathPreview"> </div></span><br/><input class="answers" type="hidden" value="m*n"/><button onclick="checkQuiz(this, 13)">submit</button><br/></div>
<p>乘号请用 * 。</p>
<h3 id="_15">所以呢</h3>
<p>第k层有m个神经元，下一层k+1层有n个神经元，即有n个sigmoid函数， 第k+1层第j个神经元记为： $$a_j^{k+1}=g( \sum_{i=1}^m w_{i,n}^k * a_i^k)$$</p>
<p>k+1层的每个神经元都有m个输入向量（偏置单元bias unit先不管），所以n个sigmoid都可以对m个输入向量求偏导， 所以可以看到每条权重边都跟sigmoid有关系， 那我们就可以试用sigmoid函数来代替原先梯度下降法里的函数。</p>
<p>sigmoid求偏导部分暂略。</p>
<h3 id="ok">OK？</h3>
<p>已求出 sigmoid的导数， </p>
<div class="process"><span><p>调整了梯度下降法的计算式，可以直接使用了吗？<input class="quiz" type="text"/></p></span><br/><input class="answers" type="hidden" value=""/><button onclick="checkQuiz(this, 14)">submit</button><br/></div>
<h3 id="_16">还要考虑系数</h3>
<p>还记得梯度下降用梯度和 学习速率（系数）来更新权重向量吧？</p>
<div class="process"><span><p>神经网络中，这个学习速率能用常数吗？</p><input class="quiz" name="quiz" type="radio" value="也许可以">也许可以</input><br/><input class="quiz" name="quiz" type="radio" value="不可以">不可以</input><br/></span><br/><input class="answers" type="hidden" value="也许可以"/><input class="comments" type="hidden" value="毕竟没说一定可以，难说绝对不可以"/><button onclick="checkQuiz(this, 15)">submit</button><br/></div>
<h3 id="_17">系数有什么问题</h3>
<p>我不知道是否可以使用常系数（有兴趣的读者可以一试），但一定不好。</p>
<div class="process"><span><p>似乎显而易见，但为什么不好？<input class="quiz" type="text"/></p></span><br/><input class="comments" type="hidden" value="权重只跟它对应的什么有关？只受什么影响？#所以不同的权重很可能连的什么的什么是什么样的，绕吧？"/><button onclick="checkQuiz(this, 16)">submit</button><br/></div>
<h3 id="_18">以常为镜，可以非常</h3>
<p>虽然不能直接使用常系数，但系数、学习速率背后的思想还是得借鉴一下，不能直接扔了吧？</p>
<p>如<a href="http://www.zhimind.com/tutorial/gradient-descent">梯度下降法</a>里所说</p>
<div class="process"><span><p>这个系数、学习速率应该代表了什么？<input class="quiz" type="text"/></p></span><br/><input class="answers" type="hidden" value=""/><input class="comments" type="hidden" value=":暂时还没写完梯度下降法部分"/><button onclick="checkQuiz(this, 17)">submit</button><br/></div>
<h3 id="_19">找呀找呀找系数</h3>
<p>如图所示（暂时没有）：</p>
<p>梯度可以看作是概率， 学习速率或系数其实是 $\Delta x$，所以二者相乘 = $\Delta y$。意味着 x的变化率对y的影响。</p>
<p>我们已经找到了梯度， 但还差 $\Delta x$， 不同层不同神经元不同边有不同的$\Delta x$。</p>
<div class="process"><span><p>读者觉得它应该是从何而来？<input class="quiz" type="text"/></p></span><br/><input class="answers" type="hidden" value=""/><button onclick="checkQuiz(this, 18)">submit</button><br/></div>
<h3 id="_20">我跪了</h3>
<p>扯不下去了， 赶紧结束这篇。$\Delta x$ 可以理解成差异、 变化量等等，但终究是一个减法， $\Delta$ 这符号一般就用来表示“差”。</p>
<p>目前， 唯一的已知的“差”值 就是 数据实际的标签label 和 前向传播输出的差， 实际值和预测值的差别 $y - a^L$， 就叫 <strong>误差</strong>。</p>
<p>所以要从输出层的这个$\Delta x$开始，往回一层层地更新权重矩阵。</p>
<div class="process"><span><p>反向传播为什么叫反向，不需要我这段解释，很简单，对吗？</p><input class="quiz" name="quiz" type="radio" value="对">对</input><br/><input class="quiz" name="quiz" type="radio" value="否">否</input><br/></span><br/><input class="answers" type="hidden" value=""/><button onclick="checkQuiz(this, 19)">submit</button><br/></div>
<h3 id="_21">接下来</h3>
<p>视频和文字各有不好，视频不好改，要重录， 文字不好做动画——特别是我不会做动画~~~</p>
<p>从输出层怎么反向到上一层呢？输出层的神经元通过突触（边）影响到上一隐藏层的神经元， 所以有两个数据了。</p>
<ol>
<li>输出层L层神经元的误差 $y - a^L$</li>
<li>上一层l层到输出层L层的边 的权重 $w^l$ </li>
</ol>
<p>那系数就是这两货的乘积了，那梯度、求导再结合进来， 先明显应该使用输出L层的g(x)求导——事实上从这里看不出来， 用l层的g(x)求导，然后除一下，可能更符合逻辑关系——前一层的变化通过导数、斜率影响下一层。为什么在下一段。</p>
<h3 id="_22">推广</h3>
<p>接上面的，为什么不用l层神经元的函数，很简单，输入层到第一个隐藏层这里怎么办？ 输入层可没有这个函数~~~</p>
<p>如何推广到其他层？ 其他层没有输出，不知道具体误差， 所以要利用下一层的误差， 下一层的误差则是来自下下一层，直到输出层的误差。</p>
<p>所以， 下面给出总结：</p>
<h1 id="_23">反向传播算法步骤</h1>
<ol>
<li>先计算输出层L层误差 $y - a^L$</li>
<li>第l层l(w)使用sigmoid神经元的sigmoid函数 g(x)。</li>
<li>系数$\alpha$ 则是上一层的<strong>误差</strong>，反向乘以边的权重</li>
<li>第k层的误差 上面第2步的系数 乘上第1步函数的导数。</li>
<li>权重向量更新是 输出$a_j^l * \delta_i^{l+1}$ -- 有点乱</li>
</ol>
<p>如图：</p>
<p><img alt="1" src="http://7xt8es.com1.z0.glb.clouddn.com/zhimind/ml/nn_error.png"/></p>
<p><img alt="2" src="http://7xt8es.com1.z0.glb.clouddn.com/zhimind/ml/bp_algorithm.png"/></p>
<h1 id="_24">总结</h1>
<p>在前一篇 <a href="http://www.zhimind.com/tutorial/feed-forward-nn">神经网络前向传播</a> 之后， 我们研究了如何更新神经网络里的权重（矩阵）。</p>
<p>借鉴 <a href="http://www.zhimind.com/tutorial/gradient-descent">梯度下降法</a> 的思想和数学式$
w = w + \alpha \nabla_w l(w)
$进行改进， 进而得到 <strong>反向传播算法</strong>。</p>
<p>实验品， 
<div class="process"><input class="comments" type="hidden" value="反向传播之前先进行什么传播？
#(前向|前馈)&amp;传播:反向传播第一步肯定是要先跟标签label比较，所以是计算什么？
#输出&amp;(误差|偏差):{%radio|从大目标上看，输出层求出来了，是不是反向了，反向到哪里？_@(前|上)&amp;一层#一层:往上或前的那层了，所以大目标是不是求那层的错误呢？具体步骤再说#反向到哪层？%}
#sigmoid&amp;(梯度|导|微):接下来的相乘一步到位，不要分两步走，就算计算出来上一层的误差了
#(梯度|导|微)&amp;乘&amp;(前|上)&amp;一层&amp;(误差|偏差)&amp;权重:{%formula|按算法来看就可以更新了,但用的是l层的_和l+1层的_来更新(名称)？@输出@误差#我暂时晕了%}
#更新&amp;权重&amp;l层输出&amp;l+1层误差:
"/><button onclick="checkQuiz(this, 20)">submit</button><br/></div></p>
<h1 id="_25">恭喜</h1>
<p>不好意思， 本篇因为有点复杂， 捋顺整个逻辑比较难， 拖的时间太长， 又心情不好， 写不下去了~~中间关键步骤敷衍了事，草草结尾。</p>
<p>神经网络反向传播算法就是以上内容。 浅层神经网络一般讲完反向传播算法 就结束。 所以， 有兴趣的读者可以准备看 深度学习 了。</p>
<p><a href="http://blog.zhimind.com/deep-learning-catalog.html">知维图-深度学习目录</a></p>
                        
                        <div class="bdsharebuttonbox bdshare-button-style1-24" data-bd-bind="1444449175032">
                    <a href="#" class="bds_more" data-cmd="more"></a>
                    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
                    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
                    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
                    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
                    <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
                </div>

                    <section>
<p id="comment-message">this man is lazy, nothing left </p>
<div class="accordion" id="accordion2">
    <div class="accordion-group">
        <div class="accordion-heading">
            <a class="accordion-toggle disqus-comment-count" data-toggle="collapse" data-parent="#accordion2"
                href="http://sndnyang.github.io/backpropagation-nn.html#disqus_thread">
                Comments
            </a>
        </div>
        <div id="disqus_thread" class="accordion-body collapse">
            <div class="accordion-inner">
                <div class="comments">
                    <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'sndnyang';
        var disqus_identifier = 'backpropagation-nn.html';
    var disqus_url = 'http://sndnyang.github.io/backpropagation-nn.html';

    (function() {
         var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
         dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
         (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

                </div>
            </div>
        </div>
    </div>
</div>
</section>

                    <hr/>
                </div>
                <div class="span2" style="float:right;font-size:0.9em;">
                    <h4>Published</h4>
                    <time pubdate="pubdate" datetime="2016-11-24T11:38:30+08:00">十一月 24, 2016</time>
                    <h4>Category</h4>
                    <a class="category-link" href="/categories.html#ji-qi-xue-xi-ref">机器学习</a>
                    <h4>Tags</h4>
                    <ul class="list-of-tags tags-in-article">
                        <li><a href="/tags.html#ji-qi-xue-xi-ref">机器学习
                            <span>15</span>
</a></li>
                        <li><a href="/tags.html#shen-du-xue-xi-ref">深度学习
                            <span>6</span>
</a></li>
                        <li><a href="/tags.html#shen-jing-wang-luo-ref">神经网络
                            <span>2</span>
</a></li>
                    </ul>
                    <h4>Stay in Touch</h4>
                    <a href="http://weibo.com/u/2405149384" title="My weibo Profile" class="sidebar-social-links" target="_blank">
                        <i class="fa fa-weibo sidebar-social-links"></i></a>
                    <a href="http://github.com/sndnyang" title="My Github Profile" class="sidebar-social-links" target="_blank">
                        <i class="fa fa-github sidebar-social-links"></i></a>
                    <a href="http://blog.csdn.net/sndnyangd" title="My csdn blog" class="sidebar-social-links" target="_blank">
                        <i class="fa fa-github sidebar-social-links"></i></a>    
                    <a href="mailto:sndnyangd@gmail.com" title="My Email Address" class="sidebar-social-links" target="_blank">
                        <i class="fa fa-envelope sidebar-social-links"></i></a>
                </div>
            </div>
        </article>
        <script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
                </div>
            </div>
        </div>
        <div id="top" data-toggle="tooltip" data-placement="left" title="回到顶部">
        <a href="javascript:;">
        <div class="arrow"></div>
        <div class="stick"></div>
      </a>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

            <script type="text/javascript">
var disqus_shortname = 'sndnyang';
(function () {
    var s = document.createElement('script'); s.async = true;
    s.type = 'text/javascript';
    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>
<script  language="javascript" type="text/javascript">
function uncollapse() {
    if (window.location.hash.match(/^#comment-\d+$/)) {
        $('#disqus_thread').collapse('show');
    }
}
</script>
<script type="text/javascript" language="JavaScript">
uncollapse();
window.onhashchange=function(){
    if (window.location.hash.match(/^#comment-\d+$/))
        window.location.reload(true);
}
</script>
<script>
$('#disqus_thread').on('shown', function () {
    var link = document.getElementsByClassName('accordion-toggle');
    var old_innerHTML = link[0].innerHTML;
    $(link[0]).fadeOut(500, function() {
        $(this).text('Click here to hide comments').fadeIn(500);
    });
    $('#disqus_thread').on('hidden', function () {
        $(link[0]).fadeOut(500, function() {
            $(this).text(old_innerHTML).fadeIn(500);
        });
    })
})
</script>


        <script src="//cdn.bootcss.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-74494551-1', 'auto');
            ga('send', 'pageview');

        </script>
        <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>
    </body>
</html>